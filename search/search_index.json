{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang Di Halaman Tugas Penambangan Data (Data Mining) \u00b6 Profile \u00b6 Name : Wahyu Zainur Putra NIM : 180411100128 Kelas : Penambangan Data 5-D Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab, S.Si., M.Kom Alamat : Perum Griya Abadi AL 10 Socah Bangkalan Terima Kasih telah mengunjungi Halaman ini \u00b6 Kumpulan Tugas Tugas Penambangan Data","title":"Halaman Profile"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data-data-mining","text":"","title":"Selamat Datang Di Halaman Tugas Penambangan Data (Data Mining)"},{"location":"#profile","text":"Name : Wahyu Zainur Putra NIM : 180411100128 Kelas : Penambangan Data 5-D Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab, S.Si., M.Kom Alamat : Perum Griya Abadi AL 10 Socah Bangkalan","title":"Profile"},{"location":"#terima-kasih-telah-mengunjungi-halaman-ini","text":"Kumpulan Tugas Tugas Penambangan Data","title":"Terima Kasih telah mengunjungi Halaman ini"},{"location":"Clustering/","text":"CLUSTERING \u00b6 Pengertian \u00b6 Clustering adalah metode pengelompokan data. Clustering bisa disebut sebagai sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Clustering merupakan proses partisi satu set objek data ke dalam himpunan bagian yang disebut cluster. Objek yang di dalam cluster tersebut memiliki kemiripan karakteristik. Clustering biasa disebut sebagai proses mempartisi sekumpulan objek data menjadi subset yang disebut cluster. Objek dalam cluster memiliki karakteristik yang sama antara satu sama lain dan berbeda dari cluster lainnya. Partisi tidak dilakukan secara manual tetapi dengan algoritma clustering. Oleh karena itu, pengelompokan sangat berguna dan dapat menemukan grup atau grup yang tidak dikenal dalam data. Clustering banyak digunakan dalam berbagai aplikasi seperti intelijen bisnis, pengenalan pola gambar, pencarian web, bidang biologi, dan untuk keamanan. Dalam intelijen bisnis, pengelompokan dapat mengatur banyak pelanggan menjadi beberapa kelompok. Misalnya mengelompokkan pelanggan menjadi beberapa cluster dengan karakteristik kesamaan yang kuat. Clustering juga dikenal sebagai segmentasi data karena clustering partisi banyak set data menjadi beberapa kelompok berdasarkan kesamaan mereka. Selain itu, clustering juga bisa menjadi deteksi outlier. Konsep dasar Clustering \u00b6 Hasil Clustering yang baik akan menghasilkan data kelas tinggkat yang memuaskan / dapat kita percaya karena bergantung pada metode yang digunakan, kesamaan yang tinggi dalam satu kelas dan tingkat kesamaan yang rendah antar kelas. Kesamaan yang dimaksud merupakan pengukuran secaranumeric terhadap dua buah objek. Nilai kesamaan antar kedua objek akan semakin tinggi jika kedua objek yang dibandingkan memiliki kemiripan yang tinggi. Begitu juga dengan sebaliknya. Kualitas hasil clustering sangat bergantung pada metode yang dipakai. Metode K-Means Clustering \u00b6 K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Menentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung sebelumnya Alokasikan kembali semua data ke cluster tersebut Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 Secara sederhana algoritma K-Means dimulai dari tahap berikut : Pilih K buah titik centroid. Menghitung jarak data dengan centroid. Update nilai titik centroid. Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Algoritma KMeans mengelompokkan data dengan mencoba memisahkan sampel dalam n kelompok yang memiliki varian yang sama, meminimalkan kriteria yang dikenal sebagai inersia atau jumlah-kuadrat dalam-kluster (lihat di bawah). Algoritma ini membutuhkan jumlah cluster yang harus ditentukan. Ini berskala baik untuk sejumlah besar sampel dan telah digunakan di berbagai bidang aplikasi di berbagai bidang. Rumus K-Means \u00b6 $$ d(x,y)=|x-y|= \\sqrt{\\sum _ { i = 1 } ^ { n } (x _ { i }-y_{i})^2} $$ Implementasikan K-Means Menggunakan Python \u00b6 Contoh Data DIgits \u00b6 from time import time import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.cluster import KMeans from sklearn.datasets import load_digits from sklearn.decomposition import PCA from sklearn.preprocessing import scale np . random . seed ( 42 ) digits = load_digits () data = scale ( digits . data ) n_samples , n_features = data . shape n_digits = len ( np . unique ( digits . target )) labels = digits . target sample_size = 300 print ( \"n_digits: %d , \\t n_samples %d , \\t n_features %d \" % ( n_digits , n_samples , n_features )) print ( 82 * '_' ) print ( 'init \\t\\t time \\t inertia \\t homo \\t compl \\t v-meas \\t ARI \\t AMI \\t silhouette' ) def bench_k_means ( estimator , name , data ): t0 = time () estimator . fit ( data ) print ( ' %-9s \\t %.2f s \\t %i \\t %.3f \\t %.3f \\t %.3f \\t %.3f \\t %.3f \\t %.3f ' % ( name , ( time () - t0 ), estimator . inertia_ , metrics . homogeneity_score ( labels , estimator . labels_ ), metrics . completeness_score ( labels , estimator . labels_ ), metrics . v_measure_score ( labels , estimator . labels_ ), metrics . adjusted_rand_score ( labels , estimator . labels_ ), metrics . adjusted_mutual_info_score ( labels , estimator . labels_ , average_method = 'arithmetic' ), metrics . silhouette_score ( data , estimator . labels_ , metric = 'euclidean' , sample_size = sample_size ))) bench_k_means ( KMeans ( init = 'k-means++' , n_clusters = n_digits , n_init = 10 ), name = \"k-means++\" , data = data ) bench_k_means ( KMeans ( init = 'random' , n_clusters = n_digits , n_init = 10 ), name = \"random\" , data = data ) # in this case the seeding of the centers is deterministic, hence we run the # kmeans algorithm only once with n_init=1 pca = PCA ( n_components = n_digits ) . fit ( data ) bench_k_means ( KMeans ( init = pca . components_ , n_clusters = n_digits , n_init = 1 ), name = \"PCA-based\" , data = data ) print ( 82 * '_' ) # ############################################################################# # Visualize the results on PCA-reduced data reduced_data = PCA ( n_components = 2 ) . fit_transform ( data ) kmeans = KMeans ( init = 'k-means++' , n_clusters = n_digits , n_init = 10 ) kmeans . fit ( reduced_data ) # Step size of the mesh. Decrease to increase the quality of the VQ. h = . 02 # point in the mesh [x_min, x_max]x[y_min, y_max]. # Plot the decision boundary. For that, we will assign a color to each x_min , x_max = reduced_data [:, 0 ] . min () - 1 , reduced_data [:, 0 ] . max () + 1 y_min , y_max = reduced_data [:, 1 ] . min () - 1 , reduced_data [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) # Obtain labels for each point in mesh. Use last trained model. Z = kmeans . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) # Put the result into a color plot Z = Z . reshape ( xx . shape ) plt . figure ( 1 ) plt . clf () plt . imshow ( Z , interpolation = 'nearest' , extent = ( xx . min (), xx . max (), yy . min (), yy . max ()), cmap = plt . cm . Paired , aspect = 'auto' , origin = 'lower' ) plt . plot ( reduced_data [:, 0 ], reduced_data [:, 1 ], 'k.' , markersize = 2 ) # Plot the centroids as a white X centroids = kmeans . cluster_centers_ plt . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = 'x' , s = 169 , linewidths = 3 , color = 'w' , zorder = 10 ) plt . title ( 'K-means clustering on the digits dataset (PCA-reduced data) \\n ' 'Centroids are marked with white cross' ) plt . xlim ( x_min , x_max ) plt . ylim ( y_min , y_max ) plt . xticks (()) plt . yticks (()) plt . show () n_digits: 10, n_samples 1797, n_features 64 ------ init time inertia homo compl v-meas ARI AMI silhouette k-means++ 0.19s 69432 0.602 0.650 0.625 0.465 0.621 0.146 random 0.19s 69694 0.669 0.710 0.689 0.553 0.686 0.147 PCA-based 0.04s 70804 0.671 0.698 0.684 0.561 0.681 0.118 K-Means menggunakan Tiga Cluster \u00b6 Kemudian ditunjukkan apa efek inisialisasi yang buruk pada proses klasifikasi: Dengan menetapkan n_init menjadi hanya 1 (standarnya adalah 10), jumlah waktu algoritma akan dijalankan dengan biji centroid yang berbeda berkurang. Plot selanjutnya menampilkan apa yang akan digunakan oleh delapan kluster dan akhirnya kebenaran dasar. import numpy as np import matplotlib.pyplot as plt # Though the following import is not directly being used, it is required # for 3D projection to work from mpl_toolkits.mplot3d import Axes3D from sklearn.cluster import KMeans from sklearn import datasets np . random . seed ( 5 ) iris = datasets . load_iris () X = iris . data y = iris . target estimators = [( 'k_means_iris_8' , KMeans ( n_clusters = 8 )), ( 'k_means_iris_3' , KMeans ( n_clusters = 3 )), ( 'k_means_iris_bad_init' , KMeans ( n_clusters = 3 , n_init = 1 , init = 'random' ))] fignum = 1 titles = [ '8 clusters' , '3 clusters' , '3 clusters, bad initialization' ] for name , est in estimators : fig = plt . figure ( fignum , figsize = ( 4 , 3 )) ax = Axes3D ( fig , rect = [ 0 , 0 , . 95 , 1 ], elev = 48 , azim = 134 ) est . fit ( X ) labels = est . labels_ ax . scatter ( X [:, 3 ], X [:, 0 ], X [:, 2 ], c = labels . astype ( np . float ), edgecolor = 'k' ) ax . w_xaxis . set_ticklabels ([]) ax . w_yaxis . set_ticklabels ([]) ax . w_zaxis . set_ticklabels ([]) ax . set_xlabel ( 'Petal width' ) ax . set_ylabel ( 'Sepal length' ) ax . set_zlabel ( 'Petal length' ) ax . set_title ( titles [ fignum - 1 ]) ax . dist = 12 fignum = fignum + 1 # Plot the ground truth fig = plt . figure ( fignum , figsize = ( 4 , 3 )) ax = Axes3D ( fig , rect = [ 0 , 0 , . 95 , 1 ], elev = 48 , azim = 134 ) for name , label in [( 'Setosa' , 0 ), ( 'Versicolour' , 1 ), ( 'Virginica' , 2 )]: ax . text3D ( X [ y == label , 3 ] . mean (), X [ y == label , 0 ] . mean (), X [ y == label , 2 ] . mean () + 2 , name , horizontalalignment = 'center' , bbox = dict ( alpha =. 2 , edgecolor = 'w' , facecolor = 'w' )) # Reorder the labels to have colors matching the cluster results y = np . choose ( y , [ 1 , 2 , 0 ]) . astype ( np . float ) ax . scatter ( X [:, 3 ], X [:, 0 ], X [:, 2 ], c = y , edgecolor = 'k' ) ax . w_xaxis . set_ticklabels ([]) ax . w_yaxis . set_ticklabels ([]) ax . w_zaxis . set_ticklabels ([]) ax . set_xlabel ( 'Petal width' ) ax . set_ylabel ( 'Sepal length' ) ax . set_zlabel ( 'Petal length' ) ax . set_title ( 'Ground Truth' ) ax . dist = 12 fig . show () Metode K-Modes \u00b6 K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Implementasi K-Modes \u00b6 Ketersediaan informasi tentang tingkat kekritisan lahan yang akurat memiliki arti khusus dalam program rehabilitasi hutan dan lahan sehingga prioritas DAS yang akan direhabilitasi dapat diketahui. Dari masalah di atas perlu cara untuk menentukan DAS prioritas yang akan direhabilitasi. Metode yang digunakan dalam penelitian ini adalah K-Mode Clustering. K-Mode Clustering memberikan model dataset ke dalam cluster di mana data pada sebuah cluster yang memiliki karakteristik yang sama dan memiliki karakteristik yang berbeda dari cluster lain berdasarkan parameter tingkat permintaan lahan. Dari penelitian ini diperoleh kelompok DAS dengan skor rendah di kawasan hutan lindung. Ditemukan pada klaster 2 yang memiliki kriteria kritikan dan dalam bentuk tutupan lahan sedang, kemiringan lereng, petak erosi parah dan pengelolaan yang buruk. Implementasi K-Modes Dengan Python menggunakan Random Kategorikal Data \u00b6 import numpy as np from kmodes.kmodes import KModes # random categorical data data = np . random . choice ( 20 , ( 100 , 10 )) km = KModes ( n_clusters = 4 , init = 'Huang' , n_init = 5 , verbose = 1 ) clusters = km . fit_predict ( data ) # Print the cluster centroids print ( km . cluster_centroids_ ) Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 28, cost: 793.0 Run 1, iteration: 2/100, moves: 1, cost: 793.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 2, iteration: 1/100, moves: 28, cost: 791.0 Run 2, iteration: 2/100, moves: 4, cost: 789.0 Run 2, iteration: 3/100, moves: 3, cost: 789.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 3, iteration: 1/100, moves: 20, cost: 797.0 Run 3, iteration: 2/100, moves: 7, cost: 792.0 Run 3, iteration: 3/100, moves: 3, cost: 792.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 4, iteration: 1/100, moves: 21, cost: 799.0 Run 4, iteration: 2/100, moves: 6, cost: 798.0 Run 4, iteration: 3/100, moves: 0, cost: 798.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 5, iteration: 1/100, moves: 18, cost: 795.0 Run 5, iteration: 2/100, moves: 6, cost: 795.0 Best run was number 2 [[14 8 0 18 3 7 0 1 16 3] [ 7 1 12 4 18 16 5 17 6 2] [ 9 17 3 2 11 5 11 0 11 1] [ 8 13 8 3 9 0 2 12 6 9]] Metode K-Prototype \u00b6 Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Algoritma K-Prototype \u00b6 Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek Tahap 1: Tentukan K dengan inisial kluster z1, z2, ...,zk secara acak dari n buah titik {x1, x2,...,xn} Tahap 2 Hitung jarak seluruh data point pada datas et terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memilik i jarak prototype terdekat dengan object yang diukur. Tahap 3 Hitung titik pusat cluster yang baru setela h semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru Tahap 4 jika titik pusat cluster tidak berubah ata u sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih be rubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. Rumus K- Prototype \u00b6 K- Prototype ini adalah Gabungan data yang ada numerik (data digit) seperti k-Means dan ada data kategorikal dari k-Modes Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Implementasi Algoritma K-Prototype \u00b6 Implementasi algoritma Berikut adalah 5 langkah sederhana dalam mengimplementasikan algoritma K-Prototype 1. Baca parameter Prototipe awal Alokasi awal Realokasi Output program Berikut Penjelasan lebih Lanjut mengenai 5 Langkah di atas Baca parameter Di sini baca berbagai parameter dari database yang diberikan. Seperti Total nomor catatan n Jumlah kluster maksimum k No. Kategori untuk setiap atribut kategori Nama dan tipe setiap atribut Urutan atribut dalam database Pemilihan prototipe awal Di sini pilih objek k sebagai prototipe awal untuk cluster k secara acak. Misalnya, jika X [i] menunjukkan objek i X [i, j] nilai atribut jth untuk objek i Prototype_N [i] - Apakah elemen numerik prototipe untuk klaster i Prototype_C [i] - Apakah elemen kategori prototipe untuk cluster i Alokasi awal Setiap objek dari kumpulan data x ditugaskan ke sebuah cluster yang memiliki perbedaan minimum dengan prototipe dengan metode sebelumnya, ukuran ketidaksamaan. Setelah prototipe kluster diperbarui sesuai setelah setiap tugas. Beberapa fungsi yang tersedia dalam algoritma adalah sebagai berikut. Distance () - Square Euclidean berfungsi untuk atribut numerik Sigma () - berfungsi dengan perbedaan minimum antara atribut kategori dan prototipe-nya Clustership [] - Keanggotaan cluster objek Clustercount [] - No. objek dalam cluster [i] SumInCluster [i] - Merangkum atribut numerik objek dalam cluster [i] dan digunakan untuk memperbarui nilai atribut numerik dari prototipe cluster FrequencyInCluster [i] - Merekam frekuensi dari berbagai nilai atribut kategori HighestFreq () - Digunakan untuk mendapatkan nilai kategori mana yang memiliki frekuensi tertinggi dan digunakan untuk memperbarui nilai atribut kategori prototipe Realokasi Di sini prototipe untuk kelompok objek sebelumnya dan saat ini harus diperbarui. Ketika kami menjalankan konsol algoritma menunjukkan variabel \"bergerak\" yang mencatat jumlah objek yang telah mengubah cluster dalam proses. Jika bergerak = 0, itu menunjukkan bahwa algoritma telah memperoleh hasil terbaik. Di bawah ini diberikan adalah kategorisasi set data di atas dengan menggunakan algoritma k-prototype import numpy as np from kmodes.kprototypes import KPrototypes import matplotlib.pyplot as plt from matplotlib import style style . use ( \"ggplot\" ) colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] #Data points with their publisher name,category score, category name, place name syms = np . genfromtxt ( 'travel.csv' , dtype = str , delimiter = ',' )[:, 1 ] X = np . genfromtxt ( 'travel.csv' , dtype = object , delimiter = ',' )[:, 2 :] X [:, 0 ] = X [:, 0 ] . astype ( float ) kproto = KPrototypes ( n_clusters = 15 , init = 'Cao' , verbose = 2 ) clusters = kproto . fit_predict ( X , categorical = [ 1 , 2 ]) # Print cluster centroids of the trained model. print ( kproto . cluster_centroids_ ) # Print training statistics print ( kproto . cost_ ) print ( kproto . n_iter_ ) for s , c in zip ( syms , clusters ): print ( \"Result: {}, cluster:{}\" . format ( s , c )) # Plot the results for i in set ( kproto . labels_ ): index = kproto . labels_ == i plt . plot ( X [ index , 0 ], X [ index , 1 ], 'o' ) plt . suptitle ( 'Data points categorized with category score' , fontsize = 18 ) plt . xlabel ( 'Category Score' , fontsize = 16 ) plt . ylabel ( 'Category Type' , fontsize = 16 ) plt . show () # Clustered result fig1 , ax3 = plt . subplots () scatter = ax3 . scatter ( syms , clusters , c = clusters , s = 50 ) ax3 . set_xlabel ( 'Data points' ) ax3 . set_ylabel ( 'Cluster' ) plt . colorbar ( scatter ) ax3 . set_title ( 'Data points classifed according to known centers' ) plt . show () result = zip ( syms , kproto . labels_ ) sortedR = sorted ( result , key = lambda x : x [ 1 ]) print ( sortedR ) Implementasi python sederhana dari pengelompokan prototipe K adalah sebagai berikut. ** Di sini saya telah menggunakan kumpulan data sederhana yang telah diekstraksi dari Facebook menggunakan grafik API. Rincian mengenai implementasi yang dilakukan di sana akan dibahas secara terpisah. Berikut ini adalah snapshot dari kumpulan data yang berisi atribut kategorikal dan numerik. Nilai yang dipisahkan koma termasuk nama penerbit, skor kategori, jenis kategori, dan nama tempat secara terpisah. 240,Ransika Fernando,0.59375,plant,No Data 240,Ransika Fernando,0.04296875,outdoor_,No Data 240,Ransika Fernando,0.26953125,outdoor_road,No Data 241,Sachini Jagodaarachchi,0.98046875,outdoor_mountain,Manigala Mountain 242,Chathuri Senanayake,0.96484375,outdoor_mountain,Adara Kanda 242,Chathuri Senanayake,0.1953125,building_,No Data 242,Chathuri Senanayake,0.00390625,outdoor_,No Data 242,Chathuri Senanayake,0.23046875,building_,Kuwait 242,Chathuri Senanayake,0.2578125,building_street,Kuwait 242,Chathuri Senanayake,0.015625,outdoor_,Kuwait 243,Nilantha Premakumara,0.9453125,sky_sun,No Data 243,Nilantha Premakumara,0.75,outdoor_mountain,No Data 244,Chathuri Senanayake,0.00390625,outdoor_,Trincomalee 244,Chathuri Senanayake,0.6328125,outdoor_oceanbeach,Trincomalee 245,Surangani Bandara,0.7734375,plant_tree,No Data 246,Hasitha Lakmal,0.4140625,people_many,No Data 246,Hasitha Lakmal,0.0078125,outdoor_,No Data 247,Pradeep Kalansooriya,0.40234375,building_,No Data 247,Pradeep Kalansooriya,0.0078125,outdoor_,No Data 248,Dilini Wijesinghe,0.07421875,outdoor_,Victoria Dam 248,Dilini Wijesinghe,0.0078125,others_,Victoria Dam 249,Chiranthi Vinghghani,0.015625,outdoor_,No Data 249,Chiranthi Vinghghani,0.6484375,outdoor_waterside,No Data 250,Janindu Praneeth Weerawarnakula,0.671875,outdoor_oceanbeach,Galle Fort 251,Chathurangi Shyalika,0.00390625,outdoor_,No Data 252,Chathurangi Shyalika,0.9296875,trans_trainstation,No Data 253,Surangani Bandara,0.625,outdoor_field,No Data 253,Surangani Bandara,0.01171875,outdoor_,No Data 254,Surangani Bandara,0.99609375,sky_object,No Data 255,Chathurangi Shyalika,0.00390625,outdoor_,No Data 256,Chathurangi Shyalika,0.33984375,outdoor_field,No Data MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Clustering"},{"location":"Clustering/#clustering","text":"","title":"CLUSTERING"},{"location":"Clustering/#pengertian","text":"Clustering adalah metode pengelompokan data. Clustering bisa disebut sebagai sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Clustering merupakan proses partisi satu set objek data ke dalam himpunan bagian yang disebut cluster. Objek yang di dalam cluster tersebut memiliki kemiripan karakteristik. Clustering biasa disebut sebagai proses mempartisi sekumpulan objek data menjadi subset yang disebut cluster. Objek dalam cluster memiliki karakteristik yang sama antara satu sama lain dan berbeda dari cluster lainnya. Partisi tidak dilakukan secara manual tetapi dengan algoritma clustering. Oleh karena itu, pengelompokan sangat berguna dan dapat menemukan grup atau grup yang tidak dikenal dalam data. Clustering banyak digunakan dalam berbagai aplikasi seperti intelijen bisnis, pengenalan pola gambar, pencarian web, bidang biologi, dan untuk keamanan. Dalam intelijen bisnis, pengelompokan dapat mengatur banyak pelanggan menjadi beberapa kelompok. Misalnya mengelompokkan pelanggan menjadi beberapa cluster dengan karakteristik kesamaan yang kuat. Clustering juga dikenal sebagai segmentasi data karena clustering partisi banyak set data menjadi beberapa kelompok berdasarkan kesamaan mereka. Selain itu, clustering juga bisa menjadi deteksi outlier.","title":"Pengertian"},{"location":"Clustering/#konsep-dasar-clustering","text":"Hasil Clustering yang baik akan menghasilkan data kelas tinggkat yang memuaskan / dapat kita percaya karena bergantung pada metode yang digunakan, kesamaan yang tinggi dalam satu kelas dan tingkat kesamaan yang rendah antar kelas. Kesamaan yang dimaksud merupakan pengukuran secaranumeric terhadap dua buah objek. Nilai kesamaan antar kedua objek akan semakin tinggi jika kedua objek yang dibandingkan memiliki kemiripan yang tinggi. Begitu juga dengan sebaliknya. Kualitas hasil clustering sangat bergantung pada metode yang dipakai.","title":"Konsep dasar Clustering"},{"location":"Clustering/#metode-k-means-clustering","text":"K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: Menentukan jumlah cluster Alokasikan data secara random ke cluster yang ada Hitung rata-rata setiap cluster dari data yang tergabung sebelumnya Alokasikan kembali semua data ke cluster tersebut Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Metode K-Means Clustering"},{"location":"Clustering/#algoritma-k-means","text":"Secara sederhana algoritma K-Means dimulai dari tahap berikut : Pilih K buah titik centroid. Menghitung jarak data dengan centroid. Update nilai titik centroid. Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Algoritma KMeans mengelompokkan data dengan mencoba memisahkan sampel dalam n kelompok yang memiliki varian yang sama, meminimalkan kriteria yang dikenal sebagai inersia atau jumlah-kuadrat dalam-kluster (lihat di bawah). Algoritma ini membutuhkan jumlah cluster yang harus ditentukan. Ini berskala baik untuk sejumlah besar sampel dan telah digunakan di berbagai bidang aplikasi di berbagai bidang.","title":"Algoritma K-Means"},{"location":"Clustering/#rumus-k-means","text":"$$ d(x,y)=|x-y|= \\sqrt{\\sum _ { i = 1 } ^ { n } (x _ { i }-y_{i})^2} $$","title":"Rumus K-Means"},{"location":"Clustering/#implementasikan-k-means-menggunakan-python","text":"","title":"Implementasikan K-Means Menggunakan Python"},{"location":"Clustering/#contoh-data-digits","text":"from time import time import numpy as np import matplotlib.pyplot as plt from sklearn import metrics from sklearn.cluster import KMeans from sklearn.datasets import load_digits from sklearn.decomposition import PCA from sklearn.preprocessing import scale np . random . seed ( 42 ) digits = load_digits () data = scale ( digits . data ) n_samples , n_features = data . shape n_digits = len ( np . unique ( digits . target )) labels = digits . target sample_size = 300 print ( \"n_digits: %d , \\t n_samples %d , \\t n_features %d \" % ( n_digits , n_samples , n_features )) print ( 82 * '_' ) print ( 'init \\t\\t time \\t inertia \\t homo \\t compl \\t v-meas \\t ARI \\t AMI \\t silhouette' ) def bench_k_means ( estimator , name , data ): t0 = time () estimator . fit ( data ) print ( ' %-9s \\t %.2f s \\t %i \\t %.3f \\t %.3f \\t %.3f \\t %.3f \\t %.3f \\t %.3f ' % ( name , ( time () - t0 ), estimator . inertia_ , metrics . homogeneity_score ( labels , estimator . labels_ ), metrics . completeness_score ( labels , estimator . labels_ ), metrics . v_measure_score ( labels , estimator . labels_ ), metrics . adjusted_rand_score ( labels , estimator . labels_ ), metrics . adjusted_mutual_info_score ( labels , estimator . labels_ , average_method = 'arithmetic' ), metrics . silhouette_score ( data , estimator . labels_ , metric = 'euclidean' , sample_size = sample_size ))) bench_k_means ( KMeans ( init = 'k-means++' , n_clusters = n_digits , n_init = 10 ), name = \"k-means++\" , data = data ) bench_k_means ( KMeans ( init = 'random' , n_clusters = n_digits , n_init = 10 ), name = \"random\" , data = data ) # in this case the seeding of the centers is deterministic, hence we run the # kmeans algorithm only once with n_init=1 pca = PCA ( n_components = n_digits ) . fit ( data ) bench_k_means ( KMeans ( init = pca . components_ , n_clusters = n_digits , n_init = 1 ), name = \"PCA-based\" , data = data ) print ( 82 * '_' ) # ############################################################################# # Visualize the results on PCA-reduced data reduced_data = PCA ( n_components = 2 ) . fit_transform ( data ) kmeans = KMeans ( init = 'k-means++' , n_clusters = n_digits , n_init = 10 ) kmeans . fit ( reduced_data ) # Step size of the mesh. Decrease to increase the quality of the VQ. h = . 02 # point in the mesh [x_min, x_max]x[y_min, y_max]. # Plot the decision boundary. For that, we will assign a color to each x_min , x_max = reduced_data [:, 0 ] . min () - 1 , reduced_data [:, 0 ] . max () + 1 y_min , y_max = reduced_data [:, 1 ] . min () - 1 , reduced_data [:, 1 ] . max () + 1 xx , yy = np . meshgrid ( np . arange ( x_min , x_max , h ), np . arange ( y_min , y_max , h )) # Obtain labels for each point in mesh. Use last trained model. Z = kmeans . predict ( np . c_ [ xx . ravel (), yy . ravel ()]) # Put the result into a color plot Z = Z . reshape ( xx . shape ) plt . figure ( 1 ) plt . clf () plt . imshow ( Z , interpolation = 'nearest' , extent = ( xx . min (), xx . max (), yy . min (), yy . max ()), cmap = plt . cm . Paired , aspect = 'auto' , origin = 'lower' ) plt . plot ( reduced_data [:, 0 ], reduced_data [:, 1 ], 'k.' , markersize = 2 ) # Plot the centroids as a white X centroids = kmeans . cluster_centers_ plt . scatter ( centroids [:, 0 ], centroids [:, 1 ], marker = 'x' , s = 169 , linewidths = 3 , color = 'w' , zorder = 10 ) plt . title ( 'K-means clustering on the digits dataset (PCA-reduced data) \\n ' 'Centroids are marked with white cross' ) plt . xlim ( x_min , x_max ) plt . ylim ( y_min , y_max ) plt . xticks (()) plt . yticks (()) plt . show () n_digits: 10, n_samples 1797, n_features 64 ------ init time inertia homo compl v-meas ARI AMI silhouette k-means++ 0.19s 69432 0.602 0.650 0.625 0.465 0.621 0.146 random 0.19s 69694 0.669 0.710 0.689 0.553 0.686 0.147 PCA-based 0.04s 70804 0.671 0.698 0.684 0.561 0.681 0.118","title":"Contoh Data DIgits"},{"location":"Clustering/#k-means-menggunakan-tiga-cluster","text":"Kemudian ditunjukkan apa efek inisialisasi yang buruk pada proses klasifikasi: Dengan menetapkan n_init menjadi hanya 1 (standarnya adalah 10), jumlah waktu algoritma akan dijalankan dengan biji centroid yang berbeda berkurang. Plot selanjutnya menampilkan apa yang akan digunakan oleh delapan kluster dan akhirnya kebenaran dasar. import numpy as np import matplotlib.pyplot as plt # Though the following import is not directly being used, it is required # for 3D projection to work from mpl_toolkits.mplot3d import Axes3D from sklearn.cluster import KMeans from sklearn import datasets np . random . seed ( 5 ) iris = datasets . load_iris () X = iris . data y = iris . target estimators = [( 'k_means_iris_8' , KMeans ( n_clusters = 8 )), ( 'k_means_iris_3' , KMeans ( n_clusters = 3 )), ( 'k_means_iris_bad_init' , KMeans ( n_clusters = 3 , n_init = 1 , init = 'random' ))] fignum = 1 titles = [ '8 clusters' , '3 clusters' , '3 clusters, bad initialization' ] for name , est in estimators : fig = plt . figure ( fignum , figsize = ( 4 , 3 )) ax = Axes3D ( fig , rect = [ 0 , 0 , . 95 , 1 ], elev = 48 , azim = 134 ) est . fit ( X ) labels = est . labels_ ax . scatter ( X [:, 3 ], X [:, 0 ], X [:, 2 ], c = labels . astype ( np . float ), edgecolor = 'k' ) ax . w_xaxis . set_ticklabels ([]) ax . w_yaxis . set_ticklabels ([]) ax . w_zaxis . set_ticklabels ([]) ax . set_xlabel ( 'Petal width' ) ax . set_ylabel ( 'Sepal length' ) ax . set_zlabel ( 'Petal length' ) ax . set_title ( titles [ fignum - 1 ]) ax . dist = 12 fignum = fignum + 1 # Plot the ground truth fig = plt . figure ( fignum , figsize = ( 4 , 3 )) ax = Axes3D ( fig , rect = [ 0 , 0 , . 95 , 1 ], elev = 48 , azim = 134 ) for name , label in [( 'Setosa' , 0 ), ( 'Versicolour' , 1 ), ( 'Virginica' , 2 )]: ax . text3D ( X [ y == label , 3 ] . mean (), X [ y == label , 0 ] . mean (), X [ y == label , 2 ] . mean () + 2 , name , horizontalalignment = 'center' , bbox = dict ( alpha =. 2 , edgecolor = 'w' , facecolor = 'w' )) # Reorder the labels to have colors matching the cluster results y = np . choose ( y , [ 1 , 2 , 0 ]) . astype ( np . float ) ax . scatter ( X [:, 3 ], X [:, 0 ], X [:, 2 ], c = y , edgecolor = 'k' ) ax . w_xaxis . set_ticklabels ([]) ax . w_yaxis . set_ticklabels ([]) ax . w_zaxis . set_ticklabels ([]) ax . set_xlabel ( 'Petal width' ) ax . set_ylabel ( 'Sepal length' ) ax . set_zlabel ( 'Petal length' ) ax . set_title ( 'Ground Truth' ) ax . dist = 12 fig . show ()","title":"K-Means menggunakan Tiga Cluster"},{"location":"Clustering/#metode-k-modes","text":"K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"Clustering/#implementasi-k-modes","text":"Ketersediaan informasi tentang tingkat kekritisan lahan yang akurat memiliki arti khusus dalam program rehabilitasi hutan dan lahan sehingga prioritas DAS yang akan direhabilitasi dapat diketahui. Dari masalah di atas perlu cara untuk menentukan DAS prioritas yang akan direhabilitasi. Metode yang digunakan dalam penelitian ini adalah K-Mode Clustering. K-Mode Clustering memberikan model dataset ke dalam cluster di mana data pada sebuah cluster yang memiliki karakteristik yang sama dan memiliki karakteristik yang berbeda dari cluster lain berdasarkan parameter tingkat permintaan lahan. Dari penelitian ini diperoleh kelompok DAS dengan skor rendah di kawasan hutan lindung. Ditemukan pada klaster 2 yang memiliki kriteria kritikan dan dalam bentuk tutupan lahan sedang, kemiringan lereng, petak erosi parah dan pengelolaan yang buruk.","title":"Implementasi K-Modes"},{"location":"Clustering/#implementasi-k-modes-dengan-python-menggunakan-random-kategorikal-data","text":"import numpy as np from kmodes.kmodes import KModes # random categorical data data = np . random . choice ( 20 , ( 100 , 10 )) km = KModes ( n_clusters = 4 , init = 'Huang' , n_init = 5 , verbose = 1 ) clusters = km . fit_predict ( data ) # Print the cluster centroids print ( km . cluster_centroids_ ) Init: initializing centroids Init: initializing clusters Starting iterations... Run 1, iteration: 1/100, moves: 28, cost: 793.0 Run 1, iteration: 2/100, moves: 1, cost: 793.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 2, iteration: 1/100, moves: 28, cost: 791.0 Run 2, iteration: 2/100, moves: 4, cost: 789.0 Run 2, iteration: 3/100, moves: 3, cost: 789.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 3, iteration: 1/100, moves: 20, cost: 797.0 Run 3, iteration: 2/100, moves: 7, cost: 792.0 Run 3, iteration: 3/100, moves: 3, cost: 792.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 4, iteration: 1/100, moves: 21, cost: 799.0 Run 4, iteration: 2/100, moves: 6, cost: 798.0 Run 4, iteration: 3/100, moves: 0, cost: 798.0 Init: initializing centroids Init: initializing clusters Starting iterations... Run 5, iteration: 1/100, moves: 18, cost: 795.0 Run 5, iteration: 2/100, moves: 6, cost: 795.0 Best run was number 2 [[14 8 0 18 3 7 0 1 16 3] [ 7 1 12 4 18 16 5 17 6 2] [ 9 17 3 2 11 5 11 0 11 1] [ 8 13 8 3 9 0 2 12 6 9]]","title":"Implementasi K-Modes Dengan Python menggunakan Random Kategorikal Data"},{"location":"Clustering/#metode-k-prototype","text":"Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"Clustering/#algoritma-k-prototype","text":"Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek Tahap 1: Tentukan K dengan inisial kluster z1, z2, ...,zk secara acak dari n buah titik {x1, x2,...,xn} Tahap 2 Hitung jarak seluruh data point pada datas et terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memilik i jarak prototype terdekat dengan object yang diukur. Tahap 3 Hitung titik pusat cluster yang baru setela h semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru Tahap 4 jika titik pusat cluster tidak berubah ata u sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih be rubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek.","title":"Algoritma K-Prototype"},{"location":"Clustering/#rumus-k-prototype","text":"K- Prototype ini adalah Gabungan data yang ada numerik (data digit) seperti k-Means dan ada data kategorikal dari k-Modes Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya.","title":"Rumus K- Prototype"},{"location":"Clustering/#implementasi-algoritma-k-prototype","text":"Implementasi algoritma Berikut adalah 5 langkah sederhana dalam mengimplementasikan algoritma K-Prototype 1. Baca parameter Prototipe awal Alokasi awal Realokasi Output program Berikut Penjelasan lebih Lanjut mengenai 5 Langkah di atas Baca parameter Di sini baca berbagai parameter dari database yang diberikan. Seperti Total nomor catatan n Jumlah kluster maksimum k No. Kategori untuk setiap atribut kategori Nama dan tipe setiap atribut Urutan atribut dalam database Pemilihan prototipe awal Di sini pilih objek k sebagai prototipe awal untuk cluster k secara acak. Misalnya, jika X [i] menunjukkan objek i X [i, j] nilai atribut jth untuk objek i Prototype_N [i] - Apakah elemen numerik prototipe untuk klaster i Prototype_C [i] - Apakah elemen kategori prototipe untuk cluster i Alokasi awal Setiap objek dari kumpulan data x ditugaskan ke sebuah cluster yang memiliki perbedaan minimum dengan prototipe dengan metode sebelumnya, ukuran ketidaksamaan. Setelah prototipe kluster diperbarui sesuai setelah setiap tugas. Beberapa fungsi yang tersedia dalam algoritma adalah sebagai berikut. Distance () - Square Euclidean berfungsi untuk atribut numerik Sigma () - berfungsi dengan perbedaan minimum antara atribut kategori dan prototipe-nya Clustership [] - Keanggotaan cluster objek Clustercount [] - No. objek dalam cluster [i] SumInCluster [i] - Merangkum atribut numerik objek dalam cluster [i] dan digunakan untuk memperbarui nilai atribut numerik dari prototipe cluster FrequencyInCluster [i] - Merekam frekuensi dari berbagai nilai atribut kategori HighestFreq () - Digunakan untuk mendapatkan nilai kategori mana yang memiliki frekuensi tertinggi dan digunakan untuk memperbarui nilai atribut kategori prototipe Realokasi Di sini prototipe untuk kelompok objek sebelumnya dan saat ini harus diperbarui. Ketika kami menjalankan konsol algoritma menunjukkan variabel \"bergerak\" yang mencatat jumlah objek yang telah mengubah cluster dalam proses. Jika bergerak = 0, itu menunjukkan bahwa algoritma telah memperoleh hasil terbaik. Di bawah ini diberikan adalah kategorisasi set data di atas dengan menggunakan algoritma k-prototype import numpy as np from kmodes.kprototypes import KPrototypes import matplotlib.pyplot as plt from matplotlib import style style . use ( \"ggplot\" ) colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] #Data points with their publisher name,category score, category name, place name syms = np . genfromtxt ( 'travel.csv' , dtype = str , delimiter = ',' )[:, 1 ] X = np . genfromtxt ( 'travel.csv' , dtype = object , delimiter = ',' )[:, 2 :] X [:, 0 ] = X [:, 0 ] . astype ( float ) kproto = KPrototypes ( n_clusters = 15 , init = 'Cao' , verbose = 2 ) clusters = kproto . fit_predict ( X , categorical = [ 1 , 2 ]) # Print cluster centroids of the trained model. print ( kproto . cluster_centroids_ ) # Print training statistics print ( kproto . cost_ ) print ( kproto . n_iter_ ) for s , c in zip ( syms , clusters ): print ( \"Result: {}, cluster:{}\" . format ( s , c )) # Plot the results for i in set ( kproto . labels_ ): index = kproto . labels_ == i plt . plot ( X [ index , 0 ], X [ index , 1 ], 'o' ) plt . suptitle ( 'Data points categorized with category score' , fontsize = 18 ) plt . xlabel ( 'Category Score' , fontsize = 16 ) plt . ylabel ( 'Category Type' , fontsize = 16 ) plt . show () # Clustered result fig1 , ax3 = plt . subplots () scatter = ax3 . scatter ( syms , clusters , c = clusters , s = 50 ) ax3 . set_xlabel ( 'Data points' ) ax3 . set_ylabel ( 'Cluster' ) plt . colorbar ( scatter ) ax3 . set_title ( 'Data points classifed according to known centers' ) plt . show () result = zip ( syms , kproto . labels_ ) sortedR = sorted ( result , key = lambda x : x [ 1 ]) print ( sortedR ) Implementasi python sederhana dari pengelompokan prototipe K adalah sebagai berikut. ** Di sini saya telah menggunakan kumpulan data sederhana yang telah diekstraksi dari Facebook menggunakan grafik API. Rincian mengenai implementasi yang dilakukan di sana akan dibahas secara terpisah. Berikut ini adalah snapshot dari kumpulan data yang berisi atribut kategorikal dan numerik. Nilai yang dipisahkan koma termasuk nama penerbit, skor kategori, jenis kategori, dan nama tempat secara terpisah. 240,Ransika Fernando,0.59375,plant,No Data 240,Ransika Fernando,0.04296875,outdoor_,No Data 240,Ransika Fernando,0.26953125,outdoor_road,No Data 241,Sachini Jagodaarachchi,0.98046875,outdoor_mountain,Manigala Mountain 242,Chathuri Senanayake,0.96484375,outdoor_mountain,Adara Kanda 242,Chathuri Senanayake,0.1953125,building_,No Data 242,Chathuri Senanayake,0.00390625,outdoor_,No Data 242,Chathuri Senanayake,0.23046875,building_,Kuwait 242,Chathuri Senanayake,0.2578125,building_street,Kuwait 242,Chathuri Senanayake,0.015625,outdoor_,Kuwait 243,Nilantha Premakumara,0.9453125,sky_sun,No Data 243,Nilantha Premakumara,0.75,outdoor_mountain,No Data 244,Chathuri Senanayake,0.00390625,outdoor_,Trincomalee 244,Chathuri Senanayake,0.6328125,outdoor_oceanbeach,Trincomalee 245,Surangani Bandara,0.7734375,plant_tree,No Data 246,Hasitha Lakmal,0.4140625,people_many,No Data 246,Hasitha Lakmal,0.0078125,outdoor_,No Data 247,Pradeep Kalansooriya,0.40234375,building_,No Data 247,Pradeep Kalansooriya,0.0078125,outdoor_,No Data 248,Dilini Wijesinghe,0.07421875,outdoor_,Victoria Dam 248,Dilini Wijesinghe,0.0078125,others_,Victoria Dam 249,Chiranthi Vinghghani,0.015625,outdoor_,No Data 249,Chiranthi Vinghghani,0.6484375,outdoor_waterside,No Data 250,Janindu Praneeth Weerawarnakula,0.671875,outdoor_oceanbeach,Galle Fort 251,Chathurangi Shyalika,0.00390625,outdoor_,No Data 252,Chathurangi Shyalika,0.9296875,trans_trainstation,No Data 253,Surangani Bandara,0.625,outdoor_field,No Data 253,Surangani Bandara,0.01171875,outdoor_,No Data 254,Surangani Bandara,0.99609375,sky_object,No Data 255,Chathurangi Shyalika,0.00390625,outdoor_,No Data 256,Chathurangi Shyalika,0.33984375,outdoor_field,No Data MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Implementasi Algoritma K-Prototype"},{"location":"Decision Tree/","text":"Decision Tree (Pohon Keputusan) \u00b6 Pengertian \u00b6 Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem-break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik. Entropy \u00b6 Definisi: \u00b6 Entropi adalah ukuran ketidakmurnian, gangguan, atau ketidakpastian dalam banyak contoh. Apa yang Entropi lakukan? \u00b6 Entropy mengontrol bagaimana Decision Tree memutuskan untuk membagi data. Ini sebenarnya mempengaruhi bagaimana Pohon Keputusan menarik batas-batasnya Mencari Entropy \u00b6 Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S),sebagai berikut The Equation of Entropy(Rumus Entropy): \u00b6 $$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P Information Gain \u00b6 Information gain (IG) mengukur seberapa banyak \"informasi\" yang diberikan fitur kepada kita tentang kelas. \u2013 Fitur yang mempartisi dengan sempurna harus memberikan informasi yang maksimal. \u2013 Fitur-fitur yang tidak terkait tidak boleh memberikan informasi. \u2022 Ini mengukur pengurangan entropi. Entropi: (im) kemurnian dalam koleksi contoh yang sewenang-wenang. Apakah yang dimaksud dengan Informasi dan mengapa itu penting di Decision Tree? \u00b6 Definisi: Perolehan informasi (IG) mengukur seberapa banyak \"informasi\" yang diberikan fitur kepada kita tentang kelas. Kenapa itu penting? Perolehan informasi adalah kunci utama yang digunakan oleh Algoritma Decision Tree untuk membangun Decision Tree. Algoritma Decision Trees akan selalu berusaha memaksimalkan perolehan Informasi. Atribut dengan perolehan Informasi tertinggi akan diuji / dipisah terlebih dahulu. The Equation of Information gain(Rumus Information Gain): \u00b6 $$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan A = sebagai atribut data n = jumlah partisi Si = Probability dari Si terhadap S Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitungan manual : \u00b6 Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Pertama \u00b6 Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus $Entropy(S)$ $P_i = C0 = , P(C0)=P_1 = \\frac{10}{20}$ $P_i = C0 = , P(C1)=P_2 = \\frac{10}{20}$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$ Kedua \u00b6 Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value (Male,Female) Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value (Family,Sports,Luxury) Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value(Small,Medium,Large,ExtraLarge) Ketiga \u00b6 Menghitung Gain dan Entropy ,sebagai berikut $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Gain(S,Gender) = 1-(\\frac{10}{20}X({-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}})+\\frac{10}{20}X({-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}) = 0,029049 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Dapat Diperoleh bahwa Gain yang paling terbesar adalah $Gain(S,CarType)$,maka dapat disimpulkan bahwa Gain CarType adalah faktor terpenting dalam membuat decision tree Berikut Hasil dari Decision Tree yang telah dibuat: https://raw.githubusercontent.com/WahyuZ98/Wahyu_Zainur.github.io/master/node33.jpg Contoh Data yang telah didapatkan untuk menghitung entropy,gain dengan program,sebagai berikut: \u00b6 Pertama \u00b6 Kita harus mengimport/memasukkan data dengan meng-coding program tersebut dengan menggunakan bahasa pemrograman python (jupyter notebook) import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Kedua \u00b6 Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Berikut Tampilan yang telah di inisialisasi menjadi angka Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 1 1 Ketiga \u00b6 Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ] Keempat \u00b6 Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) Text(167.4, 195.696, 'node #0\\nGender <= 1.5\\nentropy = 1.0\\nsamples = 100.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(125.55000000000001, 152.208, 'node #1\\nCar Type <= 0.5\\nentropy = 0.65\\nsamples = 60.0%\\nvalue = [0.167, 0.833]\\nclass = C1'), Text(83.7, 108.72, 'node #2\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(167.4, 108.72, 'node #3\\nGender <= 0.5\\nentropy = 0.722\\nsamples = 50.0%\\nvalue = [0.2, 0.8]\\nclass = C1'), Text(83.7, 65.232, 'node #4\\nCar Type <= 2.5\\nentropy = 0.918\\nsamples = 15.0%\\nvalue = [0.333, 0.667]\\nclass = C1'), Text(41.85, 21.744, 'node #5\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(125.55000000000001, 21.744, 'node #6\\nentropy = 0.0\\nsamples = 5.0%\\nvalue = [1.0, 0.0]\\nclass = C0'), Text(251.10000000000002, 65.232, 'node #7\\nCar Type <= 1.5\\nentropy = 0.592\\nsamples = 35.0%\\nvalue = [0.143, 0.857]\\nclass = C1'), Text(209.25, 21.744, 'node #8\\nentropy = 1.0\\nsamples = 10.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(292.95, 21.744, 'node #9\\nentropy = 0.0\\nsamples = 25.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(209.25, 152.208, 'node #10\\nentropy = 0.0\\nsamples = 40.0%\\nvalue = [1.0, 0.0]\\nclass = C0')] Berikut Hasil Dari Decision Tree,Program dari Code sebagai berikut : https://raw.githubusercontent.com/WahyuZ98/Wahyu_Zainur.github.io/master/decisiontree.png MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Decision Tree"},{"location":"Decision Tree/#decision-tree-pohon-keputusan","text":"","title":"Decision Tree (Pohon Keputusan)"},{"location":"Decision Tree/#pengertian","text":"Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem-break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Decision tree membangun model klasifikasi atau regresi dalam bentuk struktur pohon. Ini memecah dataset menjadi himpunan bagian yang lebih kecil dan lebih kecil sementara pada saat yang sama pohon keputusan terkait dikembangkan secara bertahap. Hasil akhirnya adalah pohon dengan simpul keputusan dan simpul daun. Node keputusan (mis., Outlook) memiliki dua atau lebih cabang (mis., Sunny, Overcast, dan Rainy). Node daun (mis., Play) mewakili klasifikasi atau keputusan. Node keputusan teratas dalam pohon yang sesuai dengan prediktor terbaik disebut simpul akar. Pohon keputusan dapat menangani data kategorikal dan numerik.","title":"Pengertian"},{"location":"Decision Tree/#entropy","text":"","title":"Entropy"},{"location":"Decision Tree/#definisi","text":"Entropi adalah ukuran ketidakmurnian, gangguan, atau ketidakpastian dalam banyak contoh.","title":"Definisi:"},{"location":"Decision Tree/#apa-yang-entropi-lakukan","text":"Entropy mengontrol bagaimana Decision Tree memutuskan untuk membagi data. Ini sebenarnya mempengaruhi bagaimana Pohon Keputusan menarik batas-batasnya","title":"Apa yang Entropi lakukan?"},{"location":"Decision Tree/#mencari-entropy","text":"Untuk menghitung Information gain perlu dihitung dahulu nilai informasi dalam suatu bits dari suatu kumpulan obyek. Cara penghitungan dilakukan dengan menggunakan konsep entropi. Entropi menyatakan impurity suatu kumpulan obyek . Berikut merupakan definisi dari entropi suatu ruang sampel data (S),sebagai berikut","title":"Mencari Entropy"},{"location":"Decision Tree/#the-equation-of-entropyrumus-entropy","text":"$$ Entropy(S) = \\sum_{i=1}^n {-P_i\\log_2{P_i}} $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan n = jumlah partisi Pi = Probability dari Pi terhadap P","title":"The Equation of Entropy(Rumus Entropy):"},{"location":"Decision Tree/#information-gain","text":"Information gain (IG) mengukur seberapa banyak \"informasi\" yang diberikan fitur kepada kita tentang kelas. \u2013 Fitur yang mempartisi dengan sempurna harus memberikan informasi yang maksimal. \u2013 Fitur-fitur yang tidak terkait tidak boleh memberikan informasi. \u2022 Ini mengukur pengurangan entropi. Entropi: (im) kemurnian dalam koleksi contoh yang sewenang-wenang.","title":"Information Gain"},{"location":"Decision Tree/#apakah-yang-dimaksud-dengan-informasi-dan-mengapa-itu-penting-di-decision-tree","text":"Definisi: Perolehan informasi (IG) mengukur seberapa banyak \"informasi\" yang diberikan fitur kepada kita tentang kelas. Kenapa itu penting? Perolehan informasi adalah kunci utama yang digunakan oleh Algoritma Decision Tree untuk membangun Decision Tree. Algoritma Decision Trees akan selalu berusaha memaksimalkan perolehan Informasi. Atribut dengan perolehan Informasi tertinggi akan diuji / dipisah terlebih dahulu.","title":"Apakah yang dimaksud dengan Informasi dan mengapa itu penting di Decision Tree?"},{"location":"Decision Tree/#the-equation-of-information-gainrumus-information-gain","text":"$$ Gain(S,A) = entropy(S)-\\sum_{i=1}^n \\frac{|s_i|}{|s|}\\quad x \\quad entropy(S_i) $$ Keterangan : S = ruang sampel data yang di gunakaan untuk data pelatihan A = sebagai atribut data n = jumlah partisi Si = Probability dari Si terhadap S","title":"The Equation of Information gain(Rumus Information Gain):"},{"location":"Decision Tree/#data-berikut-untuk-menghitung-entropygain-yang-ada-di-tiap-data-tersebut-menggunakan-hitungan-manual","text":"Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1","title":"Data Berikut untuk menghitung Entropy,Gain yang ada di tiap data tersebut menggunakan hitungan manual :"},{"location":"Decision Tree/#pertama","text":"Kita harus menghitung Entropy terlebih dahulu dengan menggunakan rumus $Entropy(S)$ $P_i = C0 = , P(C0)=P_1 = \\frac{10}{20}$ $P_i = C0 = , P(C1)=P_2 = \\frac{10}{20}$ $$ Entropy(S) = {-\\frac{10}{20}\\log_2{\\frac{10}{20}}}{-\\frac{10}{20}\\log_2{\\frac{10}{20}}}= 1 $$","title":"Pertama"},{"location":"Decision Tree/#kedua","text":"Sudah dicari Entropynya = 1 maka selanjutnya mencari tiap Gainnya Gain dari Gender dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value (Male,Female) Gain dari CarType dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value (Family,Sports,Luxury) Gain dari Shirt Size dari tiap Value,berikut Rumus untuk menghitung entropy dari tiap value(Small,Medium,Large,ExtraLarge)","title":"Kedua"},{"location":"Decision Tree/#ketiga","text":"Menghitung Gain dan Entropy ,sebagai berikut $$ Entropy(Male) = {-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}}=0,97051 $$ $$ Entropy(Female) = {-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}=0,97051 $$ $$ Gain(S,Gender) = 1-(\\frac{10}{20}X({-\\frac{6}{10}\\log_2{\\frac{6}{10}}}{-\\frac{4}{10}\\log_2{\\frac{4}{10}}})+\\frac{10}{20}X({-\\frac{4}{10}\\log_2{\\frac{4}{10}}}{-\\frac{6}{10}\\log_2{\\frac{6}{10}}}) = 0,029049 $$ $$ Entropy(Family) = {-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}}=0,81128 $$ $$ Entropy(Sports) = {-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}=0 $$ $$ Entropy(Luxury) = {-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}=0,54356 $$ $$ Gain(S,CarType) = 1-(\\frac{4}{20}X({-\\frac{1}{4}\\log_2{\\frac{1}{4}}}{-\\frac{3}{4}\\log_2{\\frac{3}{4}}})+\\frac{8}{20}X({-\\frac{8}{8}\\log_2{\\frac{8}{8}}}{-\\frac{0}{8}\\log_2{\\frac{0}{8}}}+\\frac{8}{20}X({-\\frac{1}{8}\\log_2{\\frac{1}{8}}}{-\\frac{7}{8}\\log_2{\\frac{7}{8}}}) = 0,62032 $$ $$ Entropy(Small) = {-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}}=0,97095 $$ $$ Entropy(Medium) = {-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}=0,98523 $$ $$ Entropy(Large) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Entropy(ExtraLarge) = {-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}=1 $$ $$ Gain(S,ShirtSize) = 1-(\\frac{5}{20}X({-\\frac{3}{5}\\log_2{\\frac{3}{5}}}{-\\frac{2}{5}\\log_2{\\frac{2}{5}}})+\\frac{7}{20}X({-\\frac{3}{7}\\log_2{\\frac{3}{7}}}{-\\frac{4}{7}\\log_2{\\frac{4}{7}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}+\\frac{4}{20}X({-\\frac{2}{4}\\log_2{\\frac{2}{4}}}{-\\frac{2}{4}\\log_2{\\frac{2}{4}}}) = 0,012433 $$ Dapat Diperoleh bahwa Gain yang paling terbesar adalah $Gain(S,CarType)$,maka dapat disimpulkan bahwa Gain CarType adalah faktor terpenting dalam membuat decision tree Berikut Hasil dari Decision Tree yang telah dibuat: https://raw.githubusercontent.com/WahyuZ98/Wahyu_Zainur.github.io/master/node33.jpg","title":"Ketiga"},{"location":"Decision Tree/#contoh-data-yang-telah-didapatkan-untuk-menghitung-entropygain-dengan-programsebagai-berikut","text":"","title":"Contoh Data yang telah didapatkan untuk menghitung entropy,gain dengan program,sebagai berikut:"},{"location":"Decision Tree/#pertama_1","text":"Kita harus mengimport/memasukkan data dengan meng-coding program tersebut dengan menggunakan bahasa pemrograman python (jupyter notebook) import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier from sklearn import tree data = pd . read_excel ( \"data_informationgain.xlsx\" ) df = pd . DataFrame ( data ) df . style . hide_index () Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1","title":"Pertama"},{"location":"Decision Tree/#kedua_1","text":"Merubah label / kolom yang ada di data diatas untuk bisa dihitung dengan menggunakan code python,yaitu untuk memudahkan pengguna agar bisa menghitung code yang nanti akan digunakan . Pada gender hanya menggunakan biner (0,1){F,M}. Pada cartype menggunakan inisialisasi angka urutan (0,1,2){Familiy,Luxury,Sports} . Pada Shirt Size menggunakan inisialisasi angka urutan (0,1,2,3){Small,Medium,Large,ExtraLarge}.Berikut codenya lab = LabelEncoder () df [ \"gender_n\" ] = lab . fit_transform ( df [ \"Gender\" ]) df [ \"car_type_n\" ] = lab . fit_transform ( df [ \"Car Type\" ]) df [ \"shirt_size_n\" ] = lab . fit_transform ( df [ \"Shirt Size\" ]) df [ \"class_n\" ] = lab . fit_transform ( df [ \"Class\" ]) df . style . hide_index () Berikut Tampilan yang telah di inisialisasi menjadi angka Customer ID Gender Car Type Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 1 1","title":"Kedua"},{"location":"Decision Tree/#ketiga_1","text":"Setelah merubah label menjadi inisialisasi angka (numerik) melanjutkan ke code berikutnya untuk bisa dihitung dan code berikut untuk penghapusan fitur /label yang tidak diperlukan inputs = df . drop ([ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" , \"Class\" , \"class_n\" ], axis = \"columns\" ) target = df [ \"class_n\" ]","title":"Ketiga"},{"location":"Decision Tree/#keempat","text":"Membuat Klasifikasi fitur untuk bisa dibuat Pohon Keputusan (Decision Tree)untuk bisa di classifier . model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=100, splitter='best') from matplotlib import pyplot as plt tree . plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ \"Customer ID\" , \"Gender\" , \"Car Type\" , \"Shirt Size\" ], class_names = [ \"C0\" , \"C1\" ], label = \"all\" , filled = True , impurity = True , node_ids = True , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) Text(167.4, 195.696, 'node #0\\nGender <= 1.5\\nentropy = 1.0\\nsamples = 100.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(125.55000000000001, 152.208, 'node #1\\nCar Type <= 0.5\\nentropy = 0.65\\nsamples = 60.0%\\nvalue = [0.167, 0.833]\\nclass = C1'), Text(83.7, 108.72, 'node #2\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(167.4, 108.72, 'node #3\\nGender <= 0.5\\nentropy = 0.722\\nsamples = 50.0%\\nvalue = [0.2, 0.8]\\nclass = C1'), Text(83.7, 65.232, 'node #4\\nCar Type <= 2.5\\nentropy = 0.918\\nsamples = 15.0%\\nvalue = [0.333, 0.667]\\nclass = C1'), Text(41.85, 21.744, 'node #5\\nentropy = 0.0\\nsamples = 10.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(125.55000000000001, 21.744, 'node #6\\nentropy = 0.0\\nsamples = 5.0%\\nvalue = [1.0, 0.0]\\nclass = C0'), Text(251.10000000000002, 65.232, 'node #7\\nCar Type <= 1.5\\nentropy = 0.592\\nsamples = 35.0%\\nvalue = [0.143, 0.857]\\nclass = C1'), Text(209.25, 21.744, 'node #8\\nentropy = 1.0\\nsamples = 10.0%\\nvalue = [0.5, 0.5]\\nclass = C0'), Text(292.95, 21.744, 'node #9\\nentropy = 0.0\\nsamples = 25.0%\\nvalue = [0.0, 1.0]\\nclass = C1'), Text(209.25, 152.208, 'node #10\\nentropy = 0.0\\nsamples = 40.0%\\nvalue = [1.0, 0.0]\\nclass = C0')] Berikut Hasil Dari Decision Tree,Program dari Code sebagai berikut : https://raw.githubusercontent.com/WahyuZ98/Wahyu_Zainur.github.io/master/decisiontree.png MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Keempat"},{"location":"Error di Dalam Komputasi/","text":"Menghitung Nilai $e^x$ Dengan Deret Maclaurin \u00b6 Deret Maclaurin \u00b6 Suatu fungsi $f(x)$ yang memiliki turunan $f'(x),f''(x),f'''(x)$ dan seterusnya yang kontinyu dalam interval $I$ dengan $a,x$ semua anggota $I$ di sekitar $a$ yaitu $|x-a|<R, f(x)$ dapat diekspansi kedalam deret Taylor Deret Maclaurin adalah bila pada deret Taylor tersebut berpusat pada titik nol. Jadi bisa disimpulkan bahwasanya deret Maclaurin adalah bagian deret Taylor, dengan kata lain, deret Taylor yang berpusat di nol disebut dengan deret Maclaurin. Deret Taylor dari sebuah fungsi riil atau fungsi kompleks f(x) yang terdiferensialkan takhingga dalam sebuah pemetaan sebuah bilangan riil atau kompleks a adalah deret pangkat. Dapat di Definisikan sebagai berikut $$ f(x)=f(a)+{(x-a) \\over 1!}f'(a)+{(x-a)^2 \\over 2!}f''(a)+....+{(x-a)^n \\over n!}f^n(a)+...+R_n(x) $$ Definisi Deret Maclaurin \u00b6 Deret Maclaurin adalah perluasan dari deret Taylor dari fungsi sekitar 0 Berikut Dapat mendefinisikan Deret Maclaurin $$ f(x)\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} $$ Deret Taylor atau Deret MacLaurin ini sangat bermanfaat dalam metode numerik untuk menghitung atau menghampiri nilai-nilai fungsi yang susah dihitung secara manual Perhitungan Nilai $e^x$ \u00b6 Dalam perhitungan mendekati fungsi - fungsi diatas berikut fungsi bahwa turunan dari deret maclaurin ini bisa kita berikan formula bahwa $f(x) = e^x$ $$ e^x\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} $$ Dapat kita ketahui bahwa $f(x) = e^x$ akan selalu melalukan Turunan sampai ke-n atau sampai ke batas error yang terkecil , bisa disebut hampir mendekati yang sebenarnya. Berikut contoh dibawah melakukan turunan sampai turunan ke-4 $$ f(x) = e^x\\\\ f'(x) = e^x\\\\ f^2(x) = e^x \\\\ f^3(x) = e^x \\\\ f^4(x) = e^x \\\\ $$ Deret Maclaurin $e^x$ \u00b6 $$ e^x\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} \\\\ e^x= 1+x+{x^2 \\over2!} + {x^3 \\over 3!} + {x^4 \\over 4!}+ ... $$ Dimana $e^x$ untuk menghitung hasil dari $e^x$ menggunakan $x = 1$ dengan 4 suku ( 4 Turunan) $$ e^x= 1+1+{1 \\over2} + {1 \\over 6} + {1 \\over 24}+ ...\\\\ e^x = 2 + 0.7083333333333333 \\\\ e^x = 2.708333333333333 $$ Approximate Error ($E_a$) \u00b6 Menghitung approximate error (Kesalahan Perkiraan) dimana, Kesalahan perkiraan didefinisikan sebagai beda antara nilai perkiraan sekarang (ke-n) beda antara nilai perkiraan sekarang (ke-n) dengan nilai perkiraan sebelumnya (ke-(n-1)). Berikut dapat kita rumuskan sebagai Approximate Error ($E_a$) = Approximate Value $ke(n)$ \u2013 Approximate Value ke$ (n \u2013 1)$ Untuk bisa menghtiung nilai perkiraan sekarang dengan nilai perkiraan sebelumnya (n-1) maka dari yang sebelumnya hanya dengan 4 suku , perkiraan yang sekarang menggunakan 5 suku. Jadi menghitung kembali $e^x$ dengan 5 suku, sebagai berikut $$ e^x= 1+x+{x^2 \\over2!} + {x^3 \\over 3!} + {x^4 \\over 4!}+ {x^5\\over 5!}+...\\\\ e^x= 1+1+{1 \\over2} + {1 \\over 6} + {1 \\over 24}+ {1 \\over 120}+ ...\\\\ e^x = 2 + 0.7166666666666666 \\\\ e^x = 2.716666666666667 $$ Dapat kita ketahui perkiraan yang sekarang dengan 5 suku, dimana Approximate Value $ke (n) = 2.716666666666667$, dan Approximate Value $ke (n-1) = 2.708333333333333$ Approximate Error ($E_a$) = Approximate Value $ke(n)$ \u2013 Approximate Value ke$ (n \u2013 1)$ $$ E_a =2.716666666666667 - 2.708333333333333 \\\\ E_a = 0.008333333333333748 $$ Nilai $E_a$(Nilai kesalahan perkiraan ) adalah $E_a = 0.008333333333333748$. Dan untuk bisa mengetahui True error (Kesalahan yang sebenarnya) dapat kita definisikan sebagai beda antara nilai eksak dalam penghitungan dan pendekatan menggunakan metode numerik. Berikut rumus dari True Error True Error$(E_t)$ = True Value \u2013 Approximate Value dimana, True Value dari $e^x = $ $2.718281828459045$ dan Approximate Value = $2.716666666666667$ maka : $$ E_t = 2.718281828459045 - 2.716666666666667 \\\\ E_t = 0.0016151617923783057 $$ Relative Approximate Error($|\u03f5a|$) \u00b6 Didefinisikan sebagai rasio antara kesalahan perkiraan dan nilai perkiraan ke-n. Untuk menghitung Kesalahan perkiraan dibagi dengan nilai perkiraan ke -n, yang dapat didefinisikan sebagai berikut : $$ \u03f5a = {Kesalahan Perkiraan \\over Nilai PerkiraanKe-n} $$ Bisa juga dengan $$ \u03f5a = {Nilai \\quad Perkiraan \\quad Sekarang - Nilai \\quad Perkiraan \\quad Sebelumnya \\over Nilai \\quad Perkiraan \\quad Sekarang} $$ Untuk menghitung Persentasi dari Relative Approximate Error hanya dikalikan dengan 100 $$ |\u03f5a| = {Nilai \\quad Perkiraan \\quad Sekarang - Nilai \\quad Perkiraan \\quad Sebelumnya \\over Nilai \\quad Perkiraan \\quad Sekarang} X 100 % $$ Implementasi Deret Maclaurin dengan Bahasa Pemrograman Python \u00b6 Pada implentansi deret Maclaurin ini, menggunakan rumus $e^x$ $$ e^x\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} $$ dimana $e^x$ menggunakan $e^{3x}$. Maka Turunannya : $$ f(x) = e^{3x}\\\\ f'(x) = 3e^{3x}\\\\ f^2(x) = 9e^{3x} \\\\ f^3(x) = 27e^{3x} \\\\ f^4(x) = 81e^{3x} \\\\ . \\\\ . \\\\ . \\\\ $$ Untuk menghitung $e^{3x}$, dimana x =1 dan ambang batasnya (batas error) = $0,001$. Untuk bisa mengetahui batas errornya yaitu dengan Appoximate Error (Perkiraan Kesalahan). Dengan Selisih dari Nilai sekarang pada suku sekarang dikurangi dengan suku sebelumnya / nilai sebelumnya Berikut Implementasi Code Pythonnya. Agar lebih mudah memahami code membuat variabel kosong terlebih dahulu (dengan memberikan nilai 0) import math eror = 0.001 sblm = 0 skr = 0 z = True suku = 0 turunan = 1 x = 1 def percent ( nomer ): return str ( round ( nomer * 100 , 4 )) + '%' while z == True : sblm = skr skr += (( turunan * ( x ** suku )) / math . factorial ( suku )) print ( \"f ke-\" , suku , '=' , turunan , \"(e^3x=\" , skr , ') (Ea=' , skr - sblm , ')(|\u03f5a|%=' ,( skr - sblm ) / skr , '|' , percent (( skr - sblm ) / skr ), ')' ) turunan = turunan * 3 suku += 1 if skr - sblm < eror : print ( \"Aproximate Error = \" , skr - sblm ) z = False Pada Program diatas telah ditetapkan bahwa batas error = $0,001$ untuk bisa memberikan kondisi batas untuk memberhentikan program ketika batas errornya $< 0,001$ Dengan cara men-selisihkan suku yang sekarang dikurangi dengan suku yang sebelumnya dan ketika $< 0,001$ maka telah memenuhi syarat batas dan program akan berhenti.Pada cara men-selisihkan tersebut yang bisa juga disebut sebagai Approximate Error $E_a$ if skr - sblm < eror : print ( \"Approximate Error = \" , skr - sblm ) z = False Untuk menghitung Relative Approximate Error dapat diprogram sebagai berikut dengan fungsi def percent def percent ( nomer ): return str ( round ( nomer * 100 , 4 )) + '%' dimana |\u03f5a|%= (Nilai Suku sekarang - Nilai suku sebelum)/Nilai Suku sekarang X 100% '(|\u03f5a|%=' ,( skr - sblm ) / skr , '|' , percent (( skr - sblm ) / skr ), ')' ) Hasil Program ketika dijalankan : f ke - 0 = 1 ( e ^ 3 x = 1.0 ) ( Ea = 1.0 )( | \u03f5 a |%= 1.0 | 100.0 % ) f ke - 1 = 3 ( e ^ 3 x = 4.0 ) ( Ea = 3.0 )( | \u03f5 a |%= 0.75 | 75.0 % ) f ke - 2 = 9 ( e ^ 3 x = 8.5 ) ( Ea = 4.5 )( | \u03f5 a |%= 0.5294117647058824 | 52.9412 % ) f ke - 3 = 27 ( e ^ 3 x = 13.0 ) ( Ea = 4.5 )( | \u03f5 a |%= 0.34615384615384615 | 34.6154 % ) f ke - 4 = 81 ( e ^ 3 x = 16.375 ) ( Ea = 3.375 )( | \u03f5 a |%= 0.20610687022900764 | 20.6107 % ) f ke - 5 = 243 ( e ^ 3 x = 18.4 ) ( Ea = 2.0249999999999986 )( | \u03f5 a |%= 0.11005434782608689 | 11.0054 % ) f ke - 6 = 729 ( e ^ 3 x = 19.412499999999998 ) ( Ea = 1.0124999999999993 )( | \u03f5 a |%= 0.05215711526078554 | 5.2157 % ) f ke - 7 = 2187 ( e ^ 3 x = 19.846428571428568 ) ( Ea = 0.4339285714285701 )( | \u03f5 a |%= 0.02186431527802765 | 2.1864 % ) f ke - 8 = 6561 ( e ^ 3 x = 20.009151785714284 ) ( Ea = 0.162723214285716 )( | \u03f5 a |%= 0.008132439397150944 | 0.8132 % ) f ke - 9 = 19683 ( e ^ 3 x = 20.063392857142855 ) ( Ea = 0.05424107142857082 )( | \u03f5 a |%= 0.0027034844911218605 | 0.2703 % ) f ke - 10 = 59049 ( e ^ 3 x = 20.079665178571425 ) ( Ea = 0.016272321428569825 )( | \u03f5 a |%= 0.0008103880858499218 | 0.081 % ) f ke - 11 = 177147 ( e ^ 3 x = 20.08410308441558 ) ( Ea = 0.004437905844156376 )( | \u03f5 a |%= 0.00022096609569784593 | 0.0221 % ) f ke - 12 = 531441 ( e ^ 3 x = 20.08521256087662 ) ( Ea = 0.001109476461039094 )( | \u03f5 a |%= 5.523847246706314e-05 | 0.0055 % ) f ke - 13 = 1594323 ( e ^ 3 x = 20.08546859390609 ) ( Ea = 0.0002560330294691937 )( | \u03f5 a |%= 1.2747177307422833e-05 | 0.0013 % ) Approximate Error = 0.0002560330294691937 Dapat diketahui Approximate Error dari Suku yang sekarang dikurangi dengan suku sebelumnya yang hasilnya harus $< 0,001$. Yang telah diperoleh Approximate Error $= 0.0002560330294691937$ $<0,001$ Dan Approximate Value ($E_a$ )yang diperoleh $= 20.08546859390609$ Setelah memperoleh Approximate Error y= ang $< 0,001$ ,mencari True Error (Nilai Error Yang Sebenarnya). Dengan Cara : True Error$(E_t)$ = True Value \u2013 Approximate Value Cara memperoleh True Value dengan cara import math lalu math.e**3 >>> import math >>> math . e ** 3 20.085536923187664 Lalu menghitung True Errornya: $$ E_t = TrueValue-ApproximateValue \\\\ E_t = 20.085536923187664 - 20.08546859390609 \\\\ E_t = 6.832928157507467e-05 $$ Diperoleh Nilai True Errornya $= 6.832928157507467e-05 $. Maka dapat kita ketahui bahwa semakin kecil batas error yang kita gunakan maka akan semakin dekat dengan Nilai True Valuenya (Nilai yang Sebenarnya). Berikut hasil yang mudah dipahami n $e^{3x}$ $E_a$Approximate Error |\u03f5a|%Relative Percentage Error 0 1.0 1.0 100.0% 1 4.0 3.0 75.0% 2 8.5 4.5 52.9412% 3 13.0 4.5 34.6154% 4 16.375 3.375 20.6107% 5 18.4 2.0249999999999986 11.0054% 6 19.412499999999998 1.0124999999999993 5.2157% 7 19.846428571428568 0.4339285714285701 2.1864% 8 20.009151785714284 0.162723214285716 0.8132% 9 20.063392857142855 0.05424107142857082 0.2703% 10 20.079665178571425 0.016272321428569825 0.081% 11 20.08410308441558 0.004437905844156376 0.0221% 12 20.08521256087662 0.001109476461039094 0.0055% 13 20.08546859390609 0.0002560330294691937 0.0013% Keterangan : n = Iterasi / Suku $e^{3x}$ = Nilai $e^{3x}$ yang dicari untuk bisa mendekati Nilai $e^{3x}$ yang sebenarnya $E_a$ = Nilai Approximate Error (Nilai Perkiraan Error) $|\u03f5a|%$ = Nilai Relative Persentase Error yang dihasilkan Semakin Kecil Nilai Relative Persentasi Error yang dihasilkan maka akan semakin mendekati Nilai True (Nilai Yang Sebenarnya) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Tugas 1"},{"location":"Error di Dalam Komputasi/#menghitung-nilai-ex-dengan-deret-maclaurin","text":"","title":"Menghitung Nilai $e^x$ Dengan Deret Maclaurin"},{"location":"Error di Dalam Komputasi/#deret-maclaurin","text":"Suatu fungsi $f(x)$ yang memiliki turunan $f'(x),f''(x),f'''(x)$ dan seterusnya yang kontinyu dalam interval $I$ dengan $a,x$ semua anggota $I$ di sekitar $a$ yaitu $|x-a|<R, f(x)$ dapat diekspansi kedalam deret Taylor Deret Maclaurin adalah bila pada deret Taylor tersebut berpusat pada titik nol. Jadi bisa disimpulkan bahwasanya deret Maclaurin adalah bagian deret Taylor, dengan kata lain, deret Taylor yang berpusat di nol disebut dengan deret Maclaurin. Deret Taylor dari sebuah fungsi riil atau fungsi kompleks f(x) yang terdiferensialkan takhingga dalam sebuah pemetaan sebuah bilangan riil atau kompleks a adalah deret pangkat. Dapat di Definisikan sebagai berikut $$ f(x)=f(a)+{(x-a) \\over 1!}f'(a)+{(x-a)^2 \\over 2!}f''(a)+....+{(x-a)^n \\over n!}f^n(a)+...+R_n(x) $$","title":"Deret Maclaurin"},{"location":"Error di Dalam Komputasi/#definisi-deret-maclaurin","text":"Deret Maclaurin adalah perluasan dari deret Taylor dari fungsi sekitar 0 Berikut Dapat mendefinisikan Deret Maclaurin $$ f(x)\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} $$ Deret Taylor atau Deret MacLaurin ini sangat bermanfaat dalam metode numerik untuk menghitung atau menghampiri nilai-nilai fungsi yang susah dihitung secara manual","title":"Definisi Deret Maclaurin"},{"location":"Error di Dalam Komputasi/#perhitungan-nilai-ex","text":"Dalam perhitungan mendekati fungsi - fungsi diatas berikut fungsi bahwa turunan dari deret maclaurin ini bisa kita berikan formula bahwa $f(x) = e^x$ $$ e^x\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} $$ Dapat kita ketahui bahwa $f(x) = e^x$ akan selalu melalukan Turunan sampai ke-n atau sampai ke batas error yang terkecil , bisa disebut hampir mendekati yang sebenarnya. Berikut contoh dibawah melakukan turunan sampai turunan ke-4 $$ f(x) = e^x\\\\ f'(x) = e^x\\\\ f^2(x) = e^x \\\\ f^3(x) = e^x \\\\ f^4(x) = e^x \\\\ $$","title":"Perhitungan Nilai $e^x$"},{"location":"Error di Dalam Komputasi/#deret-maclaurin-ex","text":"$$ e^x\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} \\\\ e^x= 1+x+{x^2 \\over2!} + {x^3 \\over 3!} + {x^4 \\over 4!}+ ... $$ Dimana $e^x$ untuk menghitung hasil dari $e^x$ menggunakan $x = 1$ dengan 4 suku ( 4 Turunan) $$ e^x= 1+1+{1 \\over2} + {1 \\over 6} + {1 \\over 24}+ ...\\\\ e^x = 2 + 0.7083333333333333 \\\\ e^x = 2.708333333333333 $$","title":"Deret Maclaurin $e^x$"},{"location":"Error di Dalam Komputasi/#approximate-error-e_a","text":"Menghitung approximate error (Kesalahan Perkiraan) dimana, Kesalahan perkiraan didefinisikan sebagai beda antara nilai perkiraan sekarang (ke-n) beda antara nilai perkiraan sekarang (ke-n) dengan nilai perkiraan sebelumnya (ke-(n-1)). Berikut dapat kita rumuskan sebagai Approximate Error ($E_a$) = Approximate Value $ke(n)$ \u2013 Approximate Value ke$ (n \u2013 1)$ Untuk bisa menghtiung nilai perkiraan sekarang dengan nilai perkiraan sebelumnya (n-1) maka dari yang sebelumnya hanya dengan 4 suku , perkiraan yang sekarang menggunakan 5 suku. Jadi menghitung kembali $e^x$ dengan 5 suku, sebagai berikut $$ e^x= 1+x+{x^2 \\over2!} + {x^3 \\over 3!} + {x^4 \\over 4!}+ {x^5\\over 5!}+...\\\\ e^x= 1+1+{1 \\over2} + {1 \\over 6} + {1 \\over 24}+ {1 \\over 120}+ ...\\\\ e^x = 2 + 0.7166666666666666 \\\\ e^x = 2.716666666666667 $$ Dapat kita ketahui perkiraan yang sekarang dengan 5 suku, dimana Approximate Value $ke (n) = 2.716666666666667$, dan Approximate Value $ke (n-1) = 2.708333333333333$ Approximate Error ($E_a$) = Approximate Value $ke(n)$ \u2013 Approximate Value ke$ (n \u2013 1)$ $$ E_a =2.716666666666667 - 2.708333333333333 \\\\ E_a = 0.008333333333333748 $$ Nilai $E_a$(Nilai kesalahan perkiraan ) adalah $E_a = 0.008333333333333748$. Dan untuk bisa mengetahui True error (Kesalahan yang sebenarnya) dapat kita definisikan sebagai beda antara nilai eksak dalam penghitungan dan pendekatan menggunakan metode numerik. Berikut rumus dari True Error True Error$(E_t)$ = True Value \u2013 Approximate Value dimana, True Value dari $e^x = $ $2.718281828459045$ dan Approximate Value = $2.716666666666667$ maka : $$ E_t = 2.718281828459045 - 2.716666666666667 \\\\ E_t = 0.0016151617923783057 $$","title":"Approximate Error ($E_a$)"},{"location":"Error di Dalam Komputasi/#relative-approximate-errora","text":"Didefinisikan sebagai rasio antara kesalahan perkiraan dan nilai perkiraan ke-n. Untuk menghitung Kesalahan perkiraan dibagi dengan nilai perkiraan ke -n, yang dapat didefinisikan sebagai berikut : $$ \u03f5a = {Kesalahan Perkiraan \\over Nilai PerkiraanKe-n} $$ Bisa juga dengan $$ \u03f5a = {Nilai \\quad Perkiraan \\quad Sekarang - Nilai \\quad Perkiraan \\quad Sebelumnya \\over Nilai \\quad Perkiraan \\quad Sekarang} $$ Untuk menghitung Persentasi dari Relative Approximate Error hanya dikalikan dengan 100 $$ |\u03f5a| = {Nilai \\quad Perkiraan \\quad Sekarang - Nilai \\quad Perkiraan \\quad Sebelumnya \\over Nilai \\quad Perkiraan \\quad Sekarang} X 100 % $$","title":"Relative Approximate Error($|\u03f5a|$)"},{"location":"Error di Dalam Komputasi/#implementasi-deret-maclaurin-dengan-bahasa-pemrograman-python","text":"Pada implentansi deret Maclaurin ini, menggunakan rumus $e^x$ $$ e^x\u2248f(0)+f'(0)x+ {f^2(0)x^2 \\over 2!}+ {f^3(0)x^3 \\over 3!}+....+{f^n(0)x^n \\over n!} $$ dimana $e^x$ menggunakan $e^{3x}$. Maka Turunannya : $$ f(x) = e^{3x}\\\\ f'(x) = 3e^{3x}\\\\ f^2(x) = 9e^{3x} \\\\ f^3(x) = 27e^{3x} \\\\ f^4(x) = 81e^{3x} \\\\ . \\\\ . \\\\ . \\\\ $$ Untuk menghitung $e^{3x}$, dimana x =1 dan ambang batasnya (batas error) = $0,001$. Untuk bisa mengetahui batas errornya yaitu dengan Appoximate Error (Perkiraan Kesalahan). Dengan Selisih dari Nilai sekarang pada suku sekarang dikurangi dengan suku sebelumnya / nilai sebelumnya Berikut Implementasi Code Pythonnya. Agar lebih mudah memahami code membuat variabel kosong terlebih dahulu (dengan memberikan nilai 0) import math eror = 0.001 sblm = 0 skr = 0 z = True suku = 0 turunan = 1 x = 1 def percent ( nomer ): return str ( round ( nomer * 100 , 4 )) + '%' while z == True : sblm = skr skr += (( turunan * ( x ** suku )) / math . factorial ( suku )) print ( \"f ke-\" , suku , '=' , turunan , \"(e^3x=\" , skr , ') (Ea=' , skr - sblm , ')(|\u03f5a|%=' ,( skr - sblm ) / skr , '|' , percent (( skr - sblm ) / skr ), ')' ) turunan = turunan * 3 suku += 1 if skr - sblm < eror : print ( \"Aproximate Error = \" , skr - sblm ) z = False Pada Program diatas telah ditetapkan bahwa batas error = $0,001$ untuk bisa memberikan kondisi batas untuk memberhentikan program ketika batas errornya $< 0,001$ Dengan cara men-selisihkan suku yang sekarang dikurangi dengan suku yang sebelumnya dan ketika $< 0,001$ maka telah memenuhi syarat batas dan program akan berhenti.Pada cara men-selisihkan tersebut yang bisa juga disebut sebagai Approximate Error $E_a$ if skr - sblm < eror : print ( \"Approximate Error = \" , skr - sblm ) z = False Untuk menghitung Relative Approximate Error dapat diprogram sebagai berikut dengan fungsi def percent def percent ( nomer ): return str ( round ( nomer * 100 , 4 )) + '%' dimana |\u03f5a|%= (Nilai Suku sekarang - Nilai suku sebelum)/Nilai Suku sekarang X 100% '(|\u03f5a|%=' ,( skr - sblm ) / skr , '|' , percent (( skr - sblm ) / skr ), ')' ) Hasil Program ketika dijalankan : f ke - 0 = 1 ( e ^ 3 x = 1.0 ) ( Ea = 1.0 )( | \u03f5 a |%= 1.0 | 100.0 % ) f ke - 1 = 3 ( e ^ 3 x = 4.0 ) ( Ea = 3.0 )( | \u03f5 a |%= 0.75 | 75.0 % ) f ke - 2 = 9 ( e ^ 3 x = 8.5 ) ( Ea = 4.5 )( | \u03f5 a |%= 0.5294117647058824 | 52.9412 % ) f ke - 3 = 27 ( e ^ 3 x = 13.0 ) ( Ea = 4.5 )( | \u03f5 a |%= 0.34615384615384615 | 34.6154 % ) f ke - 4 = 81 ( e ^ 3 x = 16.375 ) ( Ea = 3.375 )( | \u03f5 a |%= 0.20610687022900764 | 20.6107 % ) f ke - 5 = 243 ( e ^ 3 x = 18.4 ) ( Ea = 2.0249999999999986 )( | \u03f5 a |%= 0.11005434782608689 | 11.0054 % ) f ke - 6 = 729 ( e ^ 3 x = 19.412499999999998 ) ( Ea = 1.0124999999999993 )( | \u03f5 a |%= 0.05215711526078554 | 5.2157 % ) f ke - 7 = 2187 ( e ^ 3 x = 19.846428571428568 ) ( Ea = 0.4339285714285701 )( | \u03f5 a |%= 0.02186431527802765 | 2.1864 % ) f ke - 8 = 6561 ( e ^ 3 x = 20.009151785714284 ) ( Ea = 0.162723214285716 )( | \u03f5 a |%= 0.008132439397150944 | 0.8132 % ) f ke - 9 = 19683 ( e ^ 3 x = 20.063392857142855 ) ( Ea = 0.05424107142857082 )( | \u03f5 a |%= 0.0027034844911218605 | 0.2703 % ) f ke - 10 = 59049 ( e ^ 3 x = 20.079665178571425 ) ( Ea = 0.016272321428569825 )( | \u03f5 a |%= 0.0008103880858499218 | 0.081 % ) f ke - 11 = 177147 ( e ^ 3 x = 20.08410308441558 ) ( Ea = 0.004437905844156376 )( | \u03f5 a |%= 0.00022096609569784593 | 0.0221 % ) f ke - 12 = 531441 ( e ^ 3 x = 20.08521256087662 ) ( Ea = 0.001109476461039094 )( | \u03f5 a |%= 5.523847246706314e-05 | 0.0055 % ) f ke - 13 = 1594323 ( e ^ 3 x = 20.08546859390609 ) ( Ea = 0.0002560330294691937 )( | \u03f5 a |%= 1.2747177307422833e-05 | 0.0013 % ) Approximate Error = 0.0002560330294691937 Dapat diketahui Approximate Error dari Suku yang sekarang dikurangi dengan suku sebelumnya yang hasilnya harus $< 0,001$. Yang telah diperoleh Approximate Error $= 0.0002560330294691937$ $<0,001$ Dan Approximate Value ($E_a$ )yang diperoleh $= 20.08546859390609$ Setelah memperoleh Approximate Error y= ang $< 0,001$ ,mencari True Error (Nilai Error Yang Sebenarnya). Dengan Cara : True Error$(E_t)$ = True Value \u2013 Approximate Value Cara memperoleh True Value dengan cara import math lalu math.e**3 >>> import math >>> math . e ** 3 20.085536923187664 Lalu menghitung True Errornya: $$ E_t = TrueValue-ApproximateValue \\\\ E_t = 20.085536923187664 - 20.08546859390609 \\\\ E_t = 6.832928157507467e-05 $$ Diperoleh Nilai True Errornya $= 6.832928157507467e-05 $. Maka dapat kita ketahui bahwa semakin kecil batas error yang kita gunakan maka akan semakin dekat dengan Nilai True Valuenya (Nilai yang Sebenarnya). Berikut hasil yang mudah dipahami n $e^{3x}$ $E_a$Approximate Error |\u03f5a|%Relative Percentage Error 0 1.0 1.0 100.0% 1 4.0 3.0 75.0% 2 8.5 4.5 52.9412% 3 13.0 4.5 34.6154% 4 16.375 3.375 20.6107% 5 18.4 2.0249999999999986 11.0054% 6 19.412499999999998 1.0124999999999993 5.2157% 7 19.846428571428568 0.4339285714285701 2.1864% 8 20.009151785714284 0.162723214285716 0.8132% 9 20.063392857142855 0.05424107142857082 0.2703% 10 20.079665178571425 0.016272321428569825 0.081% 11 20.08410308441558 0.004437905844156376 0.0221% 12 20.08521256087662 0.001109476461039094 0.0055% 13 20.08546859390609 0.0002560330294691937 0.0013% Keterangan : n = Iterasi / Suku $e^{3x}$ = Nilai $e^{3x}$ yang dicari untuk bisa mendekati Nilai $e^{3x}$ yang sebenarnya $E_a$ = Nilai Approximate Error (Nilai Perkiraan Error) $|\u03f5a|%$ = Nilai Relative Persentase Error yang dihasilkan Semakin Kecil Nilai Relative Persentasi Error yang dihasilkan maka akan semakin mendekati Nilai True (Nilai Yang Sebenarnya) MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Implementasi Deret Maclaurin dengan Bahasa Pemrograman Python"},{"location":"Fuzzy C-Means Clustering/","text":"Fuzzy C-Means Clustering \u00b6 Pengertian \u00b6 Fuzzy Clustering adalah proses menentukan derajat keanggotaan, dan kemudian menggunakannya dengan memasukkannya kedalam elemen satu kelompok cluster atau lebih. Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Teori Fuzzy C-Means \u00b6 Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT) Hal ini akan memberikan informasi kesamaan dari setiap objek. Satu dari sekian banyaknya algoritma fuzzy clustering yang digunakan adalah algoritma fuzzy clustering c means. Vektor dari fuzzy clustering, V ={ v1, v2, v3,\u2026, vc }, merupakan sebuah fungsi objektif yang di defenisikan dengan derajat keanggotaan dari data Xj dan pusat cluster Vj . Algoritma fuzzy clustering c means membagi data yang tersedia dari setiap elemen data berhingga lalu memasukkannya kedalam bagian dari koleksi cluster yang dipengaruhi oleh beberapa kriteria yang diberikan. Berikan satu kumpulan data berhingga. X= {x1,\u2026, xn } dan pusat data. Di mana $\u03bc_{ij}$ adalah tingkat keanggotaan $X_j$ dan pusat dari cluster adalah bagian dari matriks keanggotaan $[\u03bc_{ij}]. d^2$ adalah akar dari jarak Euclidean dan m adalah parameter fuzzy yang rata-rata tingkat keburaman dari setiap data keanggotaan derajat tidak lebih besar dari 1,0. Output dari Fuzzy C-Means adalah deretan cluster pusat dan beberapa derajat keanggotaan untuk setiap titik data. Informasi ini dapat digunakan untuk membangun sistem inferensi fuzzy. Algoritma Fuzzy Clustering Means (FCM) \u00b6 Fuzzy c-means (FCM) adalah metode pengelompokan yang memungkinkan satu bagian data menjadi milik dua atau lebih kelompok. Metode ini (dikembangkan oleh Dunn pada tahun 1973 dan ditingkatkan oleh Bezdek pada tahun 1981) sering digunakan dalam pengenalan pola. Ini didasarkan pada minimalisasi fungsi tujuan berikut: $$ \\sum\\limits_{j=1}^k \\sum\\limits_{x_i \\in C_j} u_{ij}^m (x_i - \\mu_j)^2 $$ Dimana : $u_{i}j$ adalah sejauh mana observasi xi milik sebuah cluster $c_j$ $\u03bc_j$ adalah pusat dari cluster j $u_{ij}$ adalah sejauh mana observasi xi milik sebuah cluster $c_j$ m adalah fuzzifier. Algoritma Fuzzy Clustering Means (FCM) Algoritma Fuzzy C-Means adalah sebagai berikut: Implementasi Fuzzy C-Means Clustering Dengan Excel \u00b6 Dengan menggunakan Excel 1) Dengan menentukan Jumlah Cluster serta pembobot (sebagai m) yang digunakan . Pada Contoh di bawah ini menggunakan 3 cluster dan 2 pembobot 2) Pada Keanggotaan Cluster secara random. 3 Cluster tersebut melakukan normalisasi dan ketika dijumlah ketiga cluster akan berjumlah = 1. 3) Menentukan MiuKuadrat, yaitu dengan meng-kuadrat kan semua clustering,tergantung pada bobot yang digunakan. Berikut pada contoh di bawah ini. Pada Miu Kuadrat ialah Meng pangkatkan 2 Cluster pada Cluster 1 fitur ke 1,Cluster 2 fitur ke 2 dan Cluster 3 fitur ke 3. Dan didapatkanlah semua total Miu Kuadrat 4) Menentukan Miu Kudrat X1,X2, dan X3 dengan cara. Dengan menng-kalikan Fitur Parameter Hotel Bintang dan Kamar dengan Miu Kuadrat pada Fitur ke 1. pada Fitur ke 1 0,09 dikali dengan Fitur Parameter dengan baris Huruf A 0,09x1=0,09 dan 0,09x3=0,27. Terus berlanjut ke Miu Kuadrat selanjutnya 5) Menentukan Pusat cluser dengan cara membagi Total Miu Kuadrat X yang di dapat Pada Total Miu Kuadrat X1 dibagi dengan Total Miu Kuadrat pada fitur ke 1. Berikut contohnya Berikut File Excel yang bisa di download untuk metode Fuzzy C-Means secara Manual di Excel https://docs.google.com/spreadsheets/d/e/2PACX-1vRNAimNvR40bCtQV9CmfF38oO-gb_JyJEBlygo_02GFR-ZZu_X6svd5QXlhM_CIPQ/pub?gid=469314514&single=true&output=csv Implementasi Fuzzy C-Means dengan Code Python \u00b6 Berikut Contoh Code python dengan metode Fuzzy C-means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] # Mendefinisikan 3 cluster center centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) Hasil Running : Di atas adalah data uji kami. Kami melihat tiga gumpalan yang berbeda. Namun, apa yang akan terjadi jika kita tidak tahu berapa banyak cluster yang harus kita harapkan? Mungkin jika data tidak begitu jelas mengelompok? Mari kita coba mengelompokkan data kami beberapa kali, dengan antara 2 dan 9 cluster. fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () Hasil Running : FPC didefinisikan pada rentang dari 0 hingga 1, dengan 1 yang terbaik. Ini adalah metrik yang memberi tahu kita seberapa bersih data kita dijelaskan oleh model tertentu. Selanjutnya kita akan mengelompokkan kumpulan data - yang kita tahu memiliki tiga kelompok - beberapa kali, dengan antara 2 dan 9 kelompok. Kami kemudian akan menunjukkan hasil pengelompokan, dan plot koefisien partisi fuzzy. Ketika FPC dimaksimalkan, data kami dideskripsikan dengan sangat baik. fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) Seperti yang dapat kita lihat, jumlah pusat yang ideal adalah 3. Ini bukan berita untuk contoh buatan kami, tetapi menyediakan FPC bisa sangat berguna ketika struktur data Anda tidak jelas. Perhatikan bahwa kami mulai dengan dua pusat, bukan satu; mengelompokkan data dengan hanya satu pusat cluster adalah solusi sepele dan akan dengan definisi mengembalikan FPC == 1. Mengklasifikasikan Data Baru \u00b6 Sekarang kita dapat mengelompokkan data, langkah selanjutnya sering memasukkan poin baru ke dalam model yang sudah ada. Ini dikenal sebagai prediksi. Ini membutuhkan model yang ada dan data baru untuk diklasifikasikan. Membangun model \u00b6 Kami tahu model terbaik kami memiliki tiga pusat cluster. Kami akan membangun kembali model 3-klaster untuk digunakan dalam prediksi, menghasilkan data seragam baru, dan memprediksi klaster mana yang dimiliki masing-masing titik data baru. cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () Hasil Running: Akhirnya, kami menghasilkan data sampel yang seragam di bidang ini dan mengklasifikasikannya melalui prediksi kmeans, memasukkannya ke dalam model yang sudah ada sebelumnya. newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show () Hasil Running : MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Fuzzy C-Means Clustering"},{"location":"Fuzzy C-Means Clustering/#fuzzy-c-means-clustering","text":"","title":"Fuzzy C-Means Clustering"},{"location":"Fuzzy C-Means Clustering/#pengertian","text":"Fuzzy Clustering adalah proses menentukan derajat keanggotaan, dan kemudian menggunakannya dengan memasukkannya kedalam elemen satu kelompok cluster atau lebih. Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster.","title":"Pengertian"},{"location":"Fuzzy C-Means Clustering/#teori-fuzzy-c-means","text":"Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT) Hal ini akan memberikan informasi kesamaan dari setiap objek. Satu dari sekian banyaknya algoritma fuzzy clustering yang digunakan adalah algoritma fuzzy clustering c means. Vektor dari fuzzy clustering, V ={ v1, v2, v3,\u2026, vc }, merupakan sebuah fungsi objektif yang di defenisikan dengan derajat keanggotaan dari data Xj dan pusat cluster Vj . Algoritma fuzzy clustering c means membagi data yang tersedia dari setiap elemen data berhingga lalu memasukkannya kedalam bagian dari koleksi cluster yang dipengaruhi oleh beberapa kriteria yang diberikan. Berikan satu kumpulan data berhingga. X= {x1,\u2026, xn } dan pusat data. Di mana $\u03bc_{ij}$ adalah tingkat keanggotaan $X_j$ dan pusat dari cluster adalah bagian dari matriks keanggotaan $[\u03bc_{ij}]. d^2$ adalah akar dari jarak Euclidean dan m adalah parameter fuzzy yang rata-rata tingkat keburaman dari setiap data keanggotaan derajat tidak lebih besar dari 1,0. Output dari Fuzzy C-Means adalah deretan cluster pusat dan beberapa derajat keanggotaan untuk setiap titik data. Informasi ini dapat digunakan untuk membangun sistem inferensi fuzzy.","title":"Teori Fuzzy C-Means"},{"location":"Fuzzy C-Means Clustering/#algoritma-fuzzy-clustering-means-fcm","text":"Fuzzy c-means (FCM) adalah metode pengelompokan yang memungkinkan satu bagian data menjadi milik dua atau lebih kelompok. Metode ini (dikembangkan oleh Dunn pada tahun 1973 dan ditingkatkan oleh Bezdek pada tahun 1981) sering digunakan dalam pengenalan pola. Ini didasarkan pada minimalisasi fungsi tujuan berikut: $$ \\sum\\limits_{j=1}^k \\sum\\limits_{x_i \\in C_j} u_{ij}^m (x_i - \\mu_j)^2 $$ Dimana : $u_{i}j$ adalah sejauh mana observasi xi milik sebuah cluster $c_j$ $\u03bc_j$ adalah pusat dari cluster j $u_{ij}$ adalah sejauh mana observasi xi milik sebuah cluster $c_j$ m adalah fuzzifier. Algoritma Fuzzy Clustering Means (FCM) Algoritma Fuzzy C-Means adalah sebagai berikut:","title":"Algoritma Fuzzy Clustering Means (FCM)"},{"location":"Fuzzy C-Means Clustering/#implementasi-fuzzy-c-means-clustering-dengan-excel","text":"Dengan menggunakan Excel 1) Dengan menentukan Jumlah Cluster serta pembobot (sebagai m) yang digunakan . Pada Contoh di bawah ini menggunakan 3 cluster dan 2 pembobot 2) Pada Keanggotaan Cluster secara random. 3 Cluster tersebut melakukan normalisasi dan ketika dijumlah ketiga cluster akan berjumlah = 1. 3) Menentukan MiuKuadrat, yaitu dengan meng-kuadrat kan semua clustering,tergantung pada bobot yang digunakan. Berikut pada contoh di bawah ini. Pada Miu Kuadrat ialah Meng pangkatkan 2 Cluster pada Cluster 1 fitur ke 1,Cluster 2 fitur ke 2 dan Cluster 3 fitur ke 3. Dan didapatkanlah semua total Miu Kuadrat 4) Menentukan Miu Kudrat X1,X2, dan X3 dengan cara. Dengan menng-kalikan Fitur Parameter Hotel Bintang dan Kamar dengan Miu Kuadrat pada Fitur ke 1. pada Fitur ke 1 0,09 dikali dengan Fitur Parameter dengan baris Huruf A 0,09x1=0,09 dan 0,09x3=0,27. Terus berlanjut ke Miu Kuadrat selanjutnya 5) Menentukan Pusat cluser dengan cara membagi Total Miu Kuadrat X yang di dapat Pada Total Miu Kuadrat X1 dibagi dengan Total Miu Kuadrat pada fitur ke 1. Berikut contohnya Berikut File Excel yang bisa di download untuk metode Fuzzy C-Means secara Manual di Excel https://docs.google.com/spreadsheets/d/e/2PACX-1vRNAimNvR40bCtQV9CmfF38oO-gb_JyJEBlygo_02GFR-ZZu_X6svd5QXlhM_CIPQ/pub?gid=469314514&single=true&output=csv","title":"Implementasi Fuzzy C-Means Clustering Dengan Excel"},{"location":"Fuzzy C-Means Clustering/#implementasi-fuzzy-c-means-dengan-code-python","text":"Berikut Contoh Code python dengan metode Fuzzy C-means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] # Mendefinisikan 3 cluster center centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) Hasil Running : Di atas adalah data uji kami. Kami melihat tiga gumpalan yang berbeda. Namun, apa yang akan terjadi jika kita tidak tahu berapa banyak cluster yang harus kita harapkan? Mungkin jika data tidak begitu jelas mengelompok? Mari kita coba mengelompokkan data kami beberapa kali, dengan antara 2 dan 9 cluster. fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () Hasil Running : FPC didefinisikan pada rentang dari 0 hingga 1, dengan 1 yang terbaik. Ini adalah metrik yang memberi tahu kita seberapa bersih data kita dijelaskan oleh model tertentu. Selanjutnya kita akan mengelompokkan kumpulan data - yang kita tahu memiliki tiga kelompok - beberapa kali, dengan antara 2 dan 9 kelompok. Kami kemudian akan menunjukkan hasil pengelompokan, dan plot koefisien partisi fuzzy. Ketika FPC dimaksimalkan, data kami dideskripsikan dengan sangat baik. fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) Seperti yang dapat kita lihat, jumlah pusat yang ideal adalah 3. Ini bukan berita untuk contoh buatan kami, tetapi menyediakan FPC bisa sangat berguna ketika struktur data Anda tidak jelas. Perhatikan bahwa kami mulai dengan dua pusat, bukan satu; mengelompokkan data dengan hanya satu pusat cluster adalah solusi sepele dan akan dengan definisi mengembalikan FPC == 1.","title":"Implementasi Fuzzy C-Means dengan Code Python"},{"location":"Fuzzy C-Means Clustering/#mengklasifikasikan-data-baru","text":"Sekarang kita dapat mengelompokkan data, langkah selanjutnya sering memasukkan poin baru ke dalam model yang sudah ada. Ini dikenal sebagai prediksi. Ini membutuhkan model yang ada dan data baru untuk diklasifikasikan.","title":"Mengklasifikasikan Data Baru"},{"location":"Fuzzy C-Means Clustering/#membangun-model","text":"Kami tahu model terbaik kami memiliki tiga pusat cluster. Kami akan membangun kembali model 3-klaster untuk digunakan dalam prediksi, menghasilkan data seragam baru, dan memprediksi klaster mana yang dimiliki masing-masing titik data baru. cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () Hasil Running: Akhirnya, kami menghasilkan data sampel yang seragam di bidang ini dan mengklasifikasikannya melalui prediksi kmeans, memasukkannya ke dalam model yang sudah ada sebelumnya. newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show () Hasil Running : MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Membangun model"},{"location":"Mengukur Jarak Data/","text":"Mengukur Jarak Data \u00b6 Mengukur Tipe Jarak Numerik \u00b6 Data Numerik adalah data yang merupakan hasil dari pengukuran . Maka hasil dari dari numerical data itu adalah data murni yang dihasilkan dari penelitian tiap orang untuk mengukur data-data yang nanti dihasilkan melalui data yang telah diperoleh dari pengukuran data keseluruhan. Dalam era modern ini kita sudah mengenali data base (data mining ) yang memiliki banyak tipe data yang digunakan untuk bisa mengukur jarak tiap data data tersebut . Cara Menghitung setiap ukuran jarak dengan Tipe Numerik yang dibedakan dalam berbagai macam,yaitu diantaranya : \u00b6 Minkowski Distance \u00b6 Minkowski Distance adalah matriks dalam Ruang vektor normed yang dapat dianggap sebagai generalisasi dari Ecludian Distance dan Manhattan Distance. Dapat dirumuskan sebagai berikut : $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Dimana m adalah bilangan real positif Xi dan Yi adalah dua vektor yang dalam ruang dimensi n yang diemplementasikan untuk mengukur jarak Minkowski Distance pada model clustering data pada atribut yang telah diketahui untuk bisa melakukan normalisasi dan menghindari dominasi dari atribut yang memiliki data skala besar. Manhattan Distance \u00b6 Manhattan Distance adalah kasus khusus dari jarak Minkowsi distance pada m = 1 . Seperti Minkowski Distance , Manhattan Distance sensitif terhadap outlier. Tetapi jika digunakan dalam algoritma clustering, bentuk cluster adalah hyper-rectangular. Ukuran inilah yang didefinisikan sebagai rumus berikut : $$ d_{\\operatorname{man}}= \\sum_{i=1}^{n} \\left | x_{i} - y {i}\\right| $$ Euclidean Distance \u00b6 Jarak yang sering digunakan dan dikenal untuk data tipe numerik adalah menggunakan jarak Euclidean ini. Jarak euclidean ini adalah kasus khusus dari Jarak Minkowski distance ketika m = 2. Euclidean distance berkinerja baik ketika digunakan untuk bisa mengumpulkan data cluster kompak atau terisolasi. Meskipun jarak Eucldian ini sangat umum dalam pengolompokan, ia memiliki kelemahan,yaitu : jka dua vektor data tidak memiliki nilai atribut yang sama, kemungkinan memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain jika menggunakan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatas kelemahan ini. Average Distance \u00b6 Dengan jarak averange ini memiliki kekurangan dari Jarak Euclidean diatas, rata-rata jarak adalah versi modifikasi dari Jarak Euclidean untuk memperbaiki hasil. Untuk dua titik x, y dalam ruang dimensi n , rata-rata jarak didefinisikan sebagai berikut : $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Weighted Euclidean \u00b6 Ini adalah modifikasi lain dari Jarak Euclidean Distance yang memiliki tingkatan penting dari masing-masing atribut yang telah ditentukan. Dapat definisikan sebagai berikut dengan rumus : $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Dimana Wi tersebut adalah bobot yang diberikan pada atribut ke i. Chord Distance \u00b6 Chord Distance ini adalah suatu jarak yang telah dimodifikasi dari jarak Euclidean distance untuk bisa mengatasi kekurangan yang ada pada Euclidean distance. InI dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Serta jarak ini juga dapat dihitung dari data yang tidak dinormalisasi. Chord Distance ini dapat didefinisikan sebagai berikut : $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ Dimana : $$ Dimana \\quad ||x||_2 \\quad adalah \\quad L^2-norm||x||_2 =\\sqrt{ \\sum _ { i = 1 } ^ { n } x _ { i }^2} $$ Mahalanobis Distance \u00b6 Mahalanobis Distance berdasarkan data berbeda dengan kedua jarak yaitu Euclidean dan Manhattan distances, yang bebas antara data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal cluster. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi liner antara fitur dengan menerapkan suatu transformasi pemutihan ke data atau dengan menggunakan kudrat Jarak Mahalanobis. Mahalanobis dapat didefinisikan sebagai berikut : $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Dimana S sebagai matriks covariance data. Cosine Measure \u00b6 Ukururan Cosine banyak kesamaan yang digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$ Pearson Correlation \u00b6 Pearson correlation banyak digunakan dalam data ekspresi gen. Ukuran kesamaan ini menghitung similaritas antara dua bentuk pola ekspresi gen . Pearson correlation didefinisikan sebagai berikut : $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ Pada Jarak Pearson correlation ini memiliki kelamahan adalah sensitif terhadap outlier. Mengukur Jarak Tipe Data Binary \u00b6 Similirity dan desimilirty pada objek Biner ini dibedakan dalam 2 macam , yaitu atribut biner yang simetris dan asimetris. Atribut biner hanya memiliki 2 status 0 dan 1 contoh menggambar seseorang memiliki jenis kelamin dapat kita bedakan dengan pemisalan bahwa laki-laki menunjukkan 1 dan perempuan 0 karena dalam jenis kelamin hanya memiliki 2 status ,hal ini yang bisa kita sebut bahwa data ini disebut sebagai biner karena dengan membedakannya menggunakan 2 status tanpa adanya metode khusus untuk melakukan atribut biner sebagai atribut numerik. Oleh karena itu data biner dapat dibedakan dalam komputasi. Hal tersebut disebut simetris karena tanpa menghitung kembali ketidaksamaan berbeda dengan asimetris. Bila asimetris kita perlu untuk menghitung ketidaksamaan antara dua atribut biner contoh dalam penghitungan data kehamilan ataupun data terkena penyakit HIV itu perlu kita hitung kembali dikarenakan meskipun itu data biner ,dalam data pastinya ada perhitungan misal dalam terkena penyakit HIV itu data hari pertama menunjukkan positif di hari selanjutnya bisa berbeda ,begitu pula dengan data kehamilan pada data pertama menunjukkan negatif dan data berikutnya menunjukkan positif maka kita bisa anggap bahwa asimetris ini perlunya melibatkan perhitungan matriks antara ketidaksamaan dari data biner yang diberikan. Jika semua atribut iner dianggap memilki bobot yang sama, kita memiliki tabel kontingensi 2 x 2 dimana q adalah jumlah atribut yang = 1 untuk kedua objek i dan j,r adalah jumlah atribut yang sama dengan 1 untuk objek I tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p = q + r +s +t Atribut biner simetris, masing-masing memiliki nilai bobot yang sama. Dapat kita nyatakan sebagai Dissimilarity pada atribut yang disebut symmetric binary dissimilarity. Jika objek i dan j disebut sebagai atribut biner simetris pada rumus berikut : $$ d(i,j)=\\frac {r+s}{q+r+s+t} $$ Untuk atribut biner asimetris, dua atribut biner asimetris perlu dilakukan pencocokan agar keduanya dapat diketahui bahwa 1 (kecocokan positif) dari pada 0(kecocokan negatif). Ketidaksamaan berdasarkan atribut ini disebut asimetris biner dissimilarity. Dimana dapat kita hitung dengan rumus berikut : $$ d(i,j)=\\frac {r+s}{q+r+s} $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity pada rumus berikut : $$ sim(i,j)=\\frac {q}{q+r+s}=1-d(i,j) $$ Persamaan similarity diatas disebut dengan Jaccard Coefficient Mengukur Jarak Tipe Data Kategorical \u00b6 Overlay Metric \u00b6 Saat semuat atribut adalah tipe data nominal, ukuran jarak yang paling sederhana dengan menggunakan Overlay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana n adalah banyaknya atribut, ai (x) dan ai(y) adalah atribut ke i yaitu Ai dari masing masing objek x dan y, Delta (ai(x)),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Ada kegagalan memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi Value Difference Metric (VDM) \u00b6 Versi sederhana dari VDM tanpa skema pembobotan dapat didefinisikan sebagai berikut : $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$ Minimum Risk Metric (MRM) \u00b6 Ukuran ini dipresentasikan oleh Blanzieri dan Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassfication (kegagalan dalam kesalahan klasifikasi) didefinisikan sebagai berikut : $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Dimana C adalah banyaknya kelas VDM mengamsusikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi yang sama. Pendekatan lain berbasis probabilitias adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles and Hand didefinisikan sebagai berikut : $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$ Mengukur Jarak Tipe Ordinal \u00b6 Nilai-nilai atribut ordinal dapat kita ketahui dari urutan atau peringkat, namum besarnya antara nilai-nilai berturut turut tidak diketahui. Contoh tingkatan tingkatan bawah, sedang, tinggi untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu rentang atribut numerik dapat dipetakan ke atribut ordinal f yang Memiliki Mf state. M adalah jumlah keadaan yang dapat dilakukan oleh ordinal yang dimiliki atau bisa disebut sebagai banyaknya ordinal. rif sebagai ordinal yang akan dihitung. zif sebagai menormalisasikan data dengan menggantinya peringkatnya dengan rif,yaitu degan rumus berikut : $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ setalah menghitung rumus diatas dan mendapat nilai dari zif itu ,menghitung jaraknya dengan tipe numerik yang akan digunakan. Mengukur Jarak Tipe Campuran \u00b6 Cara mengukurnya dengan menghitung ketidaksamaan antara objek dengan atribut tipe data campuran yang berupa nominal,biner simetris,biner asimetris,kategorical atau ordinal yang ada pada kebanyakan database dapat dinyatakan dalam semua tipe dengan melakukan proses normalisasi dengan tipe atribut secara bersamaan. Salah satunya dengan menggabungkan atribut yang berbeda dengan matriks ketidaksamaan tunggal dan menyatakannya dalam skala interval antar [0,0,1.0]. Misalkan data berisi atribut p tipe campuran. Disimilarity antara objek i dan j dinyatakan dengan : $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$ Cara Menghitung Jarak Dengan Tipe Data Campuran Menggunakan Python \u00b6 Langkah Pertama \u00b6 Dengan menginstall pandas terlebih dahulu di cmd dengan pip install pandas. Bila telah di install masukkan kedalam code untuk bisa dibaca didalam program tersebut import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder Langkah Kedua \u00b6 Memasukkan data csv. yang telah dibuat dalam tipe campuran dan dimasukkkan kedalam code untuk bisa dibaca didalam programnya data = pd . read_csv ( \"DataJrk1.csv\" , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () Maka akan tampil tabel yang ada didalam csv tersebut,sebagai berikut Nama Jenis Kelamin IPK Penghasilan Orangtua Alamat Prestasi Dani L 3.5 3000000 Sumenep Internasional Risa P 3.3 5000000 Sampang Regional Imam L 3.4 4000000 Bangkalan Nasional Langkah Ketiga \u00b6 Langkah ini, menerapkan dari formula atau rumus yang telah dihitung untuk bisa menghitung jarak diatas yang menggunakan tipe data campuran, kita menghitungnya dalam bentuk fungsi python. Fungsi berikut digunakan untuk dinormalisasikan terbelebih dahulu untuk menjadi tipe data numerik. def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) Fungsi berikut untuk menghitung jarak tipe numerik dengan menerapkan rumus dari Euclidean Distance untuk menghitung jarak tersebut #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) Fungsi berikut untuk menghitung jarak pada tipe data binary simetris #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t )) Fungsi untuk menghitung jarak Tipe data Kategorikal #Menghitung Jarak tipe categorikal def distanceNom ( x , y ): p = len ( x ) or len ( y ) m = 0 for i in range ( len ( x )): if x [ i ][ 0 ] == y [ i ][ 0 ]: m += 1 return ( p - m ) / p Fungsi Selanjutnya dengan melakukan normalisasi pada tipe data ordinal #Menghitung Jarak tipe ordinal #inisialisasi x = { 'Internasional' : 3 , 'Nasional' : 2 , 'Regional' : 1 } def normalizedOrd ( y ): i_max = 0 for i in x : if x [ i ] > i_max : i_max = x [ i ] if y [ 0 ] == i : i_val = x [ i ] return ( i_val - 1 ) / ( i_max - 1 ) Langkah Keempat \u00b6 Pada langkah ini kita membuat inisialisasi didictionary dengan disimilarity matrix: d_x = { 0 : [ '' , 'Dani' , 'Risa' , 'Imam' ], 1 : [ 'Dani' , 0 , '' , '' ], 2 : [ 'Risa' , '' , 0 , '' ], 3 : [ 'Imam' , '' , '' , 0 ] } Langkah Kelima \u00b6 Untuk mempermudah menghitung jarak dari tipe data biner, kita bisa melakukan konversi nilai dari fitur tersebut dalam bentuk numerik atau angka 0 atau 1. Dalam pemprosesan konversi tersebut kita dapat menggunakan fungsi LabelEncode yang merupakan bawaan dari library sklearn X = data . iloc [:,:] . values labelEncode_X = LabelEncoder () X [:, 1 ] = labelEncode_X . fit_transform ( X [:, 1 ]) Langkah Keenam \u00b6 Pada Langkah ini kita menghitung jarak dari masing-masing tipe menggunaka fungsi yang telah dibuat sebelumnya Menghitung Jarak Tipe Numerik \u00b6 Berikut proses menghitung jarak dengan tipe numerik . Pada proses berikut kita mengambil fitur-fitur numerik dari masing-masing objek ,yaitu Dani,Risa dan Imam. Dari data numerik tersebut kemudian dinormalisasi dan menghitungnya dengan menggunakan fungsi Euclidean Distance yang hasilnya ditampung pada dictionary disimilarity matrix #ambil data numerikal aliNum = df . iloc [ 0 , 2 : 4 ] . values aniNum = df . iloc [ 1 , 2 : 4 ] . values abiNum = df . iloc [ 2 , 2 : 4 ] . values #normalisasi data numerikal aliNum = [ normalisasi ( aliNum [ 0 ], data . columns [ 2 ]), normalisasi ( aliNum [ 1 ], data . columns [ 3 ])] aniNum = [ normalisasi ( aniNum [ 0 ], data . columns [ 2 ]), normalisasi ( aniNum [ 1 ], data . columns [ 3 ])] abiNum = [ normalisasi ( abiNum [ 0 ], data . columns [ 2 ]), normalisasi ( abiNum [ 1 ], data . columns [ 3 ])] d_x [ 1 ][ 2 ] = euclidianDistance ( aniNum , aliNum ) d_x [ 1 ][ 3 ] = euclidianDistance ( abiNum , aliNum ) d_x [ 2 ][ 3 ] = euclidianDistance ( abiNum , aniNum ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas, akan menampilkan jarak dalam bentuk disimilarity matrix. Apabila disimilarity matrix mendekati0 maka kedua objek tersebut makin sama. 0 1 2 3 Dani Risa Imam Dani 0 Risa 2.83 0 Imam 1.41 1.41 0 Menghitung Jarak Tipe Kategorikal \u00b6 Proses berikut kita menghitung jarak dengan dengan tipe kategorikal/ nominal. Pada proses tersebut kita akan mengambil nilai dari fitur kategorikal dari masing-masing objek. Dalam kasus diatas kategorikal di atas adalah alamat yang beratas namakan tempat (Kabupaten). Selanjutnya nilai masing masing diambil dan dihitung menggunakan fungsi distanceNom(obj1,obj2) yang telah dibuat sebelumnya yaitu #ambil data kategorical aliKat = [ df . iloc [ 0 , 4 : 5 ] . values ] aniKat = [ df . iloc [ 1 , 4 : 5 ] . values ] abiKat = [ df . iloc [ 2 , 4 : 5 ] . values ] d_x [ 1 ][ 2 ] = distanceNom ( aniKat , aliKat ) d_x [ 1 ][ 3 ] = distanceNom ( abiKat , aliKat ) d_x [ 2 ][ 3 ] = distanceNom ( abiKat , aniKat ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 Dani Risa Imam Dani 0 Risa 1 0 Imam 1 1 0 Menghitung Jarak Tipe Binary \u00b6 Proses berikut menghitung jarak tipe biner dengan mengambil nilai dari masing masing objek .Dalam kasus tadi pada fitur binary adalah Jenis Kelamin. Pada fungsi ini kita menggunakan rumus di dalam fungsi distanceSimetris(obj1,obj2). Dari hasil perhitungan bisa kita tampung di dictionary disimilarity matrix #ambil data binary aliBin = X [ 0 , 1 : 2 ] aniBin = X [ 1 , 1 : 2 ] abiBin = X [ 2 , 1 : 2 ] d_x [ 1 ][ 2 ] = distanceSimetris ( aniBin , aliBin ) d_x [ 1 ][ 3 ] = distanceSimetris ( abiBin , aliBin ) d_x [ 2 ][ 3 ] = distanceSimetris ( abiBin , aniBin ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas bisa kita tampilkan hasil running dari jarak yang telah dihitung 0 1 2 3 Dani Risa Imam Dani 0 Risa 1 0 Imam 0 1 0 Menghitung Jarak Tipe Ordinal \u00b6 Berikut kita menghitung jarak tipe ordinal dengan mengambil nilai dari masing masing objek diatas dari fitur Prestasi tingkatan objeknya. Nilai dari masing masing objek di normalisasi menggunakan fungsi normalizedOrd(ordObj)yang telah dibuat sebelumnya dengan menggunakan fungsi jarak euclideanDistance(obj1,obj2) yang hasilnya kemudian ditampung di dictionary similarity #ambil data ordinal aliOrd = [ df . iloc [ 0 , 5 : 6 ] . values ] aniOrd = [ df . iloc [ 1 , 5 : 6 ] . values ] abiOrd = [ df . iloc [ 2 , 5 : 6 ] . values ] d_x [ 1 ][ 2 ] = euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas bisa kita tampilkan jarak dari ordinal ,sebagai berikut 0 1 2 3 Dani Risa Imam Dani 0 Risa 1 0 Imam 0.5 0.5 0 Menghitung Jarak Tipe Campuran \u00b6 Pada proses ini kita akan menghitung jarak dengan berbagai tipe . Untuk menghitungnya kita menjumlah jarak dari masing-masing tipe d_x [ 1 ][ 2 ] = euclidianDistance ( aniNum , aliNum ) + \\ distanceNom ( aniKat , aliKat ) + distanceSimetris ( aniBin , aliBin ) + \\ euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ( abiNum , aliNum ) + \\ distanceNom ( abiKat , aliKat ) + distanceSimetris ( abiBin , aliBin ) + \\ euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ( abiNum , aniNum ) + \\ distanceNom ( abiKat , aniKat ) + distanceSimetris ( abiBin , aniBin ) + \\ euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas bisa kita tampilkan jarak dari berbagai tipe data ,sebagai berikut 0 1 2 3 Dani Risa Imam Dani 0 Risa 5.83 0 Imam 2.91 3.91 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak Data/#mengukur-tipe-jarak-numerik","text":"Data Numerik adalah data yang merupakan hasil dari pengukuran . Maka hasil dari dari numerical data itu adalah data murni yang dihasilkan dari penelitian tiap orang untuk mengukur data-data yang nanti dihasilkan melalui data yang telah diperoleh dari pengukuran data keseluruhan. Dalam era modern ini kita sudah mengenali data base (data mining ) yang memiliki banyak tipe data yang digunakan untuk bisa mengukur jarak tiap data data tersebut .","title":"Mengukur Tipe Jarak Numerik"},{"location":"Mengukur Jarak Data/#cara-menghitung-setiap-ukuran-jarak-dengan-tipe-numerik-yang-dibedakan-dalam-berbagai-macamyaitu-diantaranya","text":"","title":"Cara Menghitung setiap ukuran jarak dengan Tipe Numerik yang dibedakan dalam berbagai macam,yaitu diantaranya :"},{"location":"Mengukur Jarak Data/#minkowski-distance","text":"Minkowski Distance adalah matriks dalam Ruang vektor normed yang dapat dianggap sebagai generalisasi dari Ecludian Distance dan Manhattan Distance. Dapat dirumuskan sebagai berikut : $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Dimana m adalah bilangan real positif Xi dan Yi adalah dua vektor yang dalam ruang dimensi n yang diemplementasikan untuk mengukur jarak Minkowski Distance pada model clustering data pada atribut yang telah diketahui untuk bisa melakukan normalisasi dan menghindari dominasi dari atribut yang memiliki data skala besar.","title":"Minkowski Distance"},{"location":"Mengukur Jarak Data/#manhattan-distance","text":"Manhattan Distance adalah kasus khusus dari jarak Minkowsi distance pada m = 1 . Seperti Minkowski Distance , Manhattan Distance sensitif terhadap outlier. Tetapi jika digunakan dalam algoritma clustering, bentuk cluster adalah hyper-rectangular. Ukuran inilah yang didefinisikan sebagai rumus berikut : $$ d_{\\operatorname{man}}= \\sum_{i=1}^{n} \\left | x_{i} - y {i}\\right| $$","title":"Manhattan Distance"},{"location":"Mengukur Jarak Data/#euclidean-distance","text":"Jarak yang sering digunakan dan dikenal untuk data tipe numerik adalah menggunakan jarak Euclidean ini. Jarak euclidean ini adalah kasus khusus dari Jarak Minkowski distance ketika m = 2. Euclidean distance berkinerja baik ketika digunakan untuk bisa mengumpulkan data cluster kompak atau terisolasi. Meskipun jarak Eucldian ini sangat umum dalam pengolompokan, ia memiliki kelemahan,yaitu : jka dua vektor data tidak memiliki nilai atribut yang sama, kemungkinan memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain jika menggunakan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatas kelemahan ini.","title":"Euclidean Distance"},{"location":"Mengukur Jarak Data/#average-distance","text":"Dengan jarak averange ini memiliki kekurangan dari Jarak Euclidean diatas, rata-rata jarak adalah versi modifikasi dari Jarak Euclidean untuk memperbaiki hasil. Untuk dua titik x, y dalam ruang dimensi n , rata-rata jarak didefinisikan sebagai berikut : $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Average Distance"},{"location":"Mengukur Jarak Data/#weighted-euclidean","text":"Ini adalah modifikasi lain dari Jarak Euclidean Distance yang memiliki tingkatan penting dari masing-masing atribut yang telah ditentukan. Dapat definisikan sebagai berikut dengan rumus : $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Dimana Wi tersebut adalah bobot yang diberikan pada atribut ke i.","title":"Weighted Euclidean"},{"location":"Mengukur Jarak Data/#chord-distance","text":"Chord Distance ini adalah suatu jarak yang telah dimodifikasi dari jarak Euclidean distance untuk bisa mengatasi kekurangan yang ada pada Euclidean distance. InI dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Serta jarak ini juga dapat dihitung dari data yang tidak dinormalisasi. Chord Distance ini dapat didefinisikan sebagai berikut : $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ Dimana : $$ Dimana \\quad ||x||_2 \\quad adalah \\quad L^2-norm||x||_2 =\\sqrt{ \\sum _ { i = 1 } ^ { n } x _ { i }^2} $$","title":"Chord Distance"},{"location":"Mengukur Jarak Data/#mahalanobis-distance","text":"Mahalanobis Distance berdasarkan data berbeda dengan kedua jarak yaitu Euclidean dan Manhattan distances, yang bebas antara data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal cluster. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi liner antara fitur dengan menerapkan suatu transformasi pemutihan ke data atau dengan menggunakan kudrat Jarak Mahalanobis. Mahalanobis dapat didefinisikan sebagai berikut : $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Dimana S sebagai matriks covariance data.","title":"Mahalanobis Distance"},{"location":"Mengukur Jarak Data/#cosine-measure","text":"Ukururan Cosine banyak kesamaan yang digunakan dalam similaritas dokumen dan dinyatakan dengan $$ Cosine(x,y)=\\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } $$","title":"Cosine Measure"},{"location":"Mengukur Jarak Data/#pearson-correlation","text":"Pearson correlation banyak digunakan dalam data ekspresi gen. Ukuran kesamaan ini menghitung similaritas antara dua bentuk pola ekspresi gen . Pearson correlation didefinisikan sebagai berikut : $$ Pearson ( x , y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\mu _ { x } ) ( y _ { i } - \\mu _ { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } } } $$ Pada Jarak Pearson correlation ini memiliki kelamahan adalah sensitif terhadap outlier.","title":"Pearson Correlation"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-data-binary","text":"Similirity dan desimilirty pada objek Biner ini dibedakan dalam 2 macam , yaitu atribut biner yang simetris dan asimetris. Atribut biner hanya memiliki 2 status 0 dan 1 contoh menggambar seseorang memiliki jenis kelamin dapat kita bedakan dengan pemisalan bahwa laki-laki menunjukkan 1 dan perempuan 0 karena dalam jenis kelamin hanya memiliki 2 status ,hal ini yang bisa kita sebut bahwa data ini disebut sebagai biner karena dengan membedakannya menggunakan 2 status tanpa adanya metode khusus untuk melakukan atribut biner sebagai atribut numerik. Oleh karena itu data biner dapat dibedakan dalam komputasi. Hal tersebut disebut simetris karena tanpa menghitung kembali ketidaksamaan berbeda dengan asimetris. Bila asimetris kita perlu untuk menghitung ketidaksamaan antara dua atribut biner contoh dalam penghitungan data kehamilan ataupun data terkena penyakit HIV itu perlu kita hitung kembali dikarenakan meskipun itu data biner ,dalam data pastinya ada perhitungan misal dalam terkena penyakit HIV itu data hari pertama menunjukkan positif di hari selanjutnya bisa berbeda ,begitu pula dengan data kehamilan pada data pertama menunjukkan negatif dan data berikutnya menunjukkan positif maka kita bisa anggap bahwa asimetris ini perlunya melibatkan perhitungan matriks antara ketidaksamaan dari data biner yang diberikan. Jika semua atribut iner dianggap memilki bobot yang sama, kita memiliki tabel kontingensi 2 x 2 dimana q adalah jumlah atribut yang = 1 untuk kedua objek i dan j,r adalah jumlah atribut yang sama dengan 1 untuk objek I tetapi 0 untuk objek j, s adalah jumlah atribut yang sama dengan 0 untuk objek i tetapi 1 untuk objek j, dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek i dan j. Jumlah total atribut adalah p, di mana p = q + r +s +t Atribut biner simetris, masing-masing memiliki nilai bobot yang sama. Dapat kita nyatakan sebagai Dissimilarity pada atribut yang disebut symmetric binary dissimilarity. Jika objek i dan j disebut sebagai atribut biner simetris pada rumus berikut : $$ d(i,j)=\\frac {r+s}{q+r+s+t} $$ Untuk atribut biner asimetris, dua atribut biner asimetris perlu dilakukan pencocokan agar keduanya dapat diketahui bahwa 1 (kecocokan positif) dari pada 0(kecocokan negatif). Ketidaksamaan berdasarkan atribut ini disebut asimetris biner dissimilarity. Dimana dapat kita hitung dengan rumus berikut : $$ d(i,j)=\\frac {r+s}{q+r+s} $$ Kita dapat mengukur perbedaan antara dua atribut biner berdasarkan pada disimilarity pada rumus berikut : $$ sim(i,j)=\\frac {q}{q+r+s}=1-d(i,j) $$ Persamaan similarity diatas disebut dengan Jaccard Coefficient","title":"Mengukur Jarak Tipe Data Binary"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-data-kategorical","text":"","title":"Mengukur Jarak Tipe Data Kategorical"},{"location":"Mengukur Jarak Data/#overlay-metric","text":"Saat semuat atribut adalah tipe data nominal, ukuran jarak yang paling sederhana dengan menggunakan Overlay Metric (OM) yang dinyatakan dengan $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\delta ( a _ { i } ( x ) , a _ { i } ( y ) ) $$ dimana n adalah banyaknya atribut, ai (x) dan ai(y) adalah atribut ke i yaitu Ai dari masing masing objek x dan y, Delta (ai(x)),ai(y)) adalah 0 jika ai(x)=ai(y) dan 1 jika sebaliknya. OM banyak digunakan oleh instance-based learning dan locally weighted learning. Ada kegagalan memanfaatkan tambahan informasi yang diberikan oleh nilai atribut nominal yang bisa membantu dalam generalisasi","title":"Overlay Metric"},{"location":"Mengukur Jarak Data/#value-difference-metric-vdm","text":"Versi sederhana dari VDM tanpa skema pembobotan dapat didefinisikan sebagai berikut : $$ d ( x , y ) = \\sum _ { i = 1 } ^ { n } \\sum _ { c = 1 } ^ { C } \\left| P ( c | a _ { i } ( x ) ) - P ( c | a _ { i } ( y ) ) \\right | $$","title":"Value Difference Metric (VDM)"},{"location":"Mengukur Jarak Data/#minimum-risk-metric-mrm","text":"Ukuran ini dipresentasikan oleh Blanzieri dan Ricci, berbeda dari SFM yaitu meminimumkan selisih antara kesalahan berhingga dan kesalahan asymtotic. MRM meminimumkan risk of misclassfication (kegagalan dalam kesalahan klasifikasi) didefinisikan sebagai berikut : $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } P ( c | x ) ( 1 - P ( c | y ) ) $$ Dimana C adalah banyaknya kelas VDM mengamsusikan bahwa dua nilai dari atribut adalah lebih dekat jika memiliki klasifikasi yang sama. Pendekatan lain berbasis probabilitias adalah SFM (Short and Fukunaga Metric) yang kemudian dikembangkan oleh Myles and Hand didefinisikan sebagai berikut : $$ d ( x , y ) = \\sum _ { c = 1 } ^ { C } \\left | P ( c | x ) - P ( c | y ) \\right| $$","title":"Minimum Risk Metric (MRM)"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-ordinal","text":"Nilai-nilai atribut ordinal dapat kita ketahui dari urutan atau peringkat, namum besarnya antara nilai-nilai berturut turut tidak diketahui. Contoh tingkatan tingkatan bawah, sedang, tinggi untuk atribut ukuran. Atribut ordinal juga dapat diperoleh dari diskritasi atribut numerik dengan membuat rentang nilai ke dalam sejumlah kategori tertentu. Kategori-kategori ini disusun dalam peringkat. Yaitu rentang atribut numerik dapat dipetakan ke atribut ordinal f yang Memiliki Mf state. M adalah jumlah keadaan yang dapat dilakukan oleh ordinal yang dimiliki atau bisa disebut sebagai banyaknya ordinal. rif sebagai ordinal yang akan dihitung. zif sebagai menormalisasikan data dengan menggantinya peringkatnya dengan rif,yaitu degan rumus berikut : $$ z _ { i f } = \\frac { r _ { i f } - 1 } { M _ { f } - 1 } $$ setalah menghitung rumus diatas dan mendapat nilai dari zif itu ,menghitung jaraknya dengan tipe numerik yang akan digunakan.","title":"Mengukur Jarak Tipe Ordinal"},{"location":"Mengukur Jarak Data/#mengukur-jarak-tipe-campuran","text":"Cara mengukurnya dengan menghitung ketidaksamaan antara objek dengan atribut tipe data campuran yang berupa nominal,biner simetris,biner asimetris,kategorical atau ordinal yang ada pada kebanyakan database dapat dinyatakan dalam semua tipe dengan melakukan proses normalisasi dengan tipe atribut secara bersamaan. Salah satunya dengan menggabungkan atribut yang berbeda dengan matriks ketidaksamaan tunggal dan menyatakannya dalam skala interval antar [0,0,1.0]. Misalkan data berisi atribut p tipe campuran. Disimilarity antara objek i dan j dinyatakan dengan : $$ d ( i , j ) = \\frac { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } d _ { i j } ^ { ( f ) } } { \\sum _ { f = 1 } ^ { p } \\delta _ { i j } ^ { ( f ) } } $$","title":"Mengukur Jarak Tipe Campuran"},{"location":"Mengukur Jarak Data/#cara-menghitung-jarak-dengan-tipe-data-campuran-menggunakan-python","text":"","title":"Cara Menghitung Jarak Dengan Tipe Data Campuran Menggunakan Python"},{"location":"Mengukur Jarak Data/#langkah-pertama","text":"Dengan menginstall pandas terlebih dahulu di cmd dengan pip install pandas. Bila telah di install masukkan kedalam code untuk bisa dibaca didalam program tersebut import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder","title":"Langkah Pertama"},{"location":"Mengukur Jarak Data/#langkah-kedua","text":"Memasukkan data csv. yang telah dibuat dalam tipe campuran dan dimasukkkan kedalam code untuk bisa dibaca didalam programnya data = pd . read_csv ( \"DataJrk1.csv\" , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () Maka akan tampil tabel yang ada didalam csv tersebut,sebagai berikut Nama Jenis Kelamin IPK Penghasilan Orangtua Alamat Prestasi Dani L 3.5 3000000 Sumenep Internasional Risa P 3.3 5000000 Sampang Regional Imam L 3.4 4000000 Bangkalan Nasional","title":"Langkah Kedua"},{"location":"Mengukur Jarak Data/#langkah-ketiga","text":"Langkah ini, menerapkan dari formula atau rumus yang telah dihitung untuk bisa menghitung jarak diatas yang menggunakan tipe data campuran, kita menghitungnya dalam bentuk fungsi python. Fungsi berikut digunakan untuk dinormalisasikan terbelebih dahulu untuk menjadi tipe data numerik. def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) Fungsi berikut untuk menghitung jarak tipe numerik dengan menerapkan rumus dari Euclidean Distance untuk menghitung jarak tersebut #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) Fungsi berikut untuk menghitung jarak pada tipe data binary simetris #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t )) Fungsi untuk menghitung jarak Tipe data Kategorikal #Menghitung Jarak tipe categorikal def distanceNom ( x , y ): p = len ( x ) or len ( y ) m = 0 for i in range ( len ( x )): if x [ i ][ 0 ] == y [ i ][ 0 ]: m += 1 return ( p - m ) / p Fungsi Selanjutnya dengan melakukan normalisasi pada tipe data ordinal #Menghitung Jarak tipe ordinal #inisialisasi x = { 'Internasional' : 3 , 'Nasional' : 2 , 'Regional' : 1 } def normalizedOrd ( y ): i_max = 0 for i in x : if x [ i ] > i_max : i_max = x [ i ] if y [ 0 ] == i : i_val = x [ i ] return ( i_val - 1 ) / ( i_max - 1 )","title":"Langkah Ketiga"},{"location":"Mengukur Jarak Data/#langkah-keempat","text":"Pada langkah ini kita membuat inisialisasi didictionary dengan disimilarity matrix: d_x = { 0 : [ '' , 'Dani' , 'Risa' , 'Imam' ], 1 : [ 'Dani' , 0 , '' , '' ], 2 : [ 'Risa' , '' , 0 , '' ], 3 : [ 'Imam' , '' , '' , 0 ] }","title":"Langkah Keempat"},{"location":"Mengukur Jarak Data/#langkah-kelima","text":"Untuk mempermudah menghitung jarak dari tipe data biner, kita bisa melakukan konversi nilai dari fitur tersebut dalam bentuk numerik atau angka 0 atau 1. Dalam pemprosesan konversi tersebut kita dapat menggunakan fungsi LabelEncode yang merupakan bawaan dari library sklearn X = data . iloc [:,:] . values labelEncode_X = LabelEncoder () X [:, 1 ] = labelEncode_X . fit_transform ( X [:, 1 ])","title":"Langkah Kelima"},{"location":"Mengukur Jarak Data/#langkah-keenam","text":"Pada Langkah ini kita menghitung jarak dari masing-masing tipe menggunaka fungsi yang telah dibuat sebelumnya","title":"Langkah Keenam"},{"location":"Mengukur Jarak Data/#menghitung-jarak-tipe-numerik","text":"Berikut proses menghitung jarak dengan tipe numerik . Pada proses berikut kita mengambil fitur-fitur numerik dari masing-masing objek ,yaitu Dani,Risa dan Imam. Dari data numerik tersebut kemudian dinormalisasi dan menghitungnya dengan menggunakan fungsi Euclidean Distance yang hasilnya ditampung pada dictionary disimilarity matrix #ambil data numerikal aliNum = df . iloc [ 0 , 2 : 4 ] . values aniNum = df . iloc [ 1 , 2 : 4 ] . values abiNum = df . iloc [ 2 , 2 : 4 ] . values #normalisasi data numerikal aliNum = [ normalisasi ( aliNum [ 0 ], data . columns [ 2 ]), normalisasi ( aliNum [ 1 ], data . columns [ 3 ])] aniNum = [ normalisasi ( aniNum [ 0 ], data . columns [ 2 ]), normalisasi ( aniNum [ 1 ], data . columns [ 3 ])] abiNum = [ normalisasi ( abiNum [ 0 ], data . columns [ 2 ]), normalisasi ( abiNum [ 1 ], data . columns [ 3 ])] d_x [ 1 ][ 2 ] = euclidianDistance ( aniNum , aliNum ) d_x [ 1 ][ 3 ] = euclidianDistance ( abiNum , aliNum ) d_x [ 2 ][ 3 ] = euclidianDistance ( abiNum , aniNum ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas, akan menampilkan jarak dalam bentuk disimilarity matrix. Apabila disimilarity matrix mendekati0 maka kedua objek tersebut makin sama. 0 1 2 3 Dani Risa Imam Dani 0 Risa 2.83 0 Imam 1.41 1.41 0","title":"Menghitung Jarak Tipe Numerik"},{"location":"Mengukur Jarak Data/#menghitung-jarak-tipe-kategorikal","text":"Proses berikut kita menghitung jarak dengan dengan tipe kategorikal/ nominal. Pada proses tersebut kita akan mengambil nilai dari fitur kategorikal dari masing-masing objek. Dalam kasus diatas kategorikal di atas adalah alamat yang beratas namakan tempat (Kabupaten). Selanjutnya nilai masing masing diambil dan dihitung menggunakan fungsi distanceNom(obj1,obj2) yang telah dibuat sebelumnya yaitu #ambil data kategorical aliKat = [ df . iloc [ 0 , 4 : 5 ] . values ] aniKat = [ df . iloc [ 1 , 4 : 5 ] . values ] abiKat = [ df . iloc [ 2 , 4 : 5 ] . values ] d_x [ 1 ][ 2 ] = distanceNom ( aniKat , aliKat ) d_x [ 1 ][ 3 ] = distanceNom ( abiKat , aliKat ) d_x [ 2 ][ 3 ] = distanceNom ( abiKat , aniKat ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () 0 1 2 3 Dani Risa Imam Dani 0 Risa 1 0 Imam 1 1 0","title":"Menghitung Jarak Tipe Kategorikal"},{"location":"Mengukur Jarak Data/#menghitung-jarak-tipe-binary","text":"Proses berikut menghitung jarak tipe biner dengan mengambil nilai dari masing masing objek .Dalam kasus tadi pada fitur binary adalah Jenis Kelamin. Pada fungsi ini kita menggunakan rumus di dalam fungsi distanceSimetris(obj1,obj2). Dari hasil perhitungan bisa kita tampung di dictionary disimilarity matrix #ambil data binary aliBin = X [ 0 , 1 : 2 ] aniBin = X [ 1 , 1 : 2 ] abiBin = X [ 2 , 1 : 2 ] d_x [ 1 ][ 2 ] = distanceSimetris ( aniBin , aliBin ) d_x [ 1 ][ 3 ] = distanceSimetris ( abiBin , aliBin ) d_x [ 2 ][ 3 ] = distanceSimetris ( abiBin , aniBin ) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas bisa kita tampilkan hasil running dari jarak yang telah dihitung 0 1 2 3 Dani Risa Imam Dani 0 Risa 1 0 Imam 0 1 0","title":"Menghitung Jarak Tipe Binary"},{"location":"Mengukur Jarak Data/#menghitung-jarak-tipe-ordinal","text":"Berikut kita menghitung jarak tipe ordinal dengan mengambil nilai dari masing masing objek diatas dari fitur Prestasi tingkatan objeknya. Nilai dari masing masing objek di normalisasi menggunakan fungsi normalizedOrd(ordObj)yang telah dibuat sebelumnya dengan menggunakan fungsi jarak euclideanDistance(obj1,obj2) yang hasilnya kemudian ditampung di dictionary similarity #ambil data ordinal aliOrd = [ df . iloc [ 0 , 5 : 6 ] . values ] aniOrd = [ df . iloc [ 1 , 5 : 6 ] . values ] abiOrd = [ df . iloc [ 2 , 5 : 6 ] . values ] d_x [ 1 ][ 2 ] = euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas bisa kita tampilkan jarak dari ordinal ,sebagai berikut 0 1 2 3 Dani Risa Imam Dani 0 Risa 1 0 Imam 0.5 0.5 0","title":"Menghitung Jarak Tipe Ordinal"},{"location":"Mengukur Jarak Data/#menghitung-jarak-tipe-campuran","text":"Pada proses ini kita akan menghitung jarak dengan berbagai tipe . Untuk menghitungnya kita menjumlah jarak dari masing-masing tipe d_x [ 1 ][ 2 ] = euclidianDistance ( aniNum , aliNum ) + \\ distanceNom ( aniKat , aliKat ) + distanceSimetris ( aniBin , aliBin ) + \\ euclidianDistance ([ normalizedOrd ( aniOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 1 ][ 3 ] = euclidianDistance ( abiNum , aliNum ) + \\ distanceNom ( abiKat , aliKat ) + distanceSimetris ( abiBin , aliBin ) + \\ euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aliOrd )]) d_x [ 2 ][ 3 ] = euclidianDistance ( abiNum , aniNum ) + \\ distanceNom ( abiKat , aniKat ) + distanceSimetris ( abiBin , aniBin ) + \\ euclidianDistance ([ normalizedOrd ( abiOrd )],[ normalizedOrd ( aniOrd )]) d_x = pd . DataFrame ( d_x ) d_x . style . hide_index () Dari proses diatas bisa kita tampilkan jarak dari berbagai tipe data ,sebagai berikut 0 1 2 3 Dani Risa Imam Dani 0 Risa 5.83 0 Imam 2.91 3.91 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Menghitung Jarak Tipe Campuran"},{"location":"Missing Values/","text":"Missing Values using KNN \u00b6 KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan. Kalibrasi Parameter KNN \u00b6 Jumlah tetangga yang harus dicari \u00b6 Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan. Metode agregasi untuk digunakan \u00b6 Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal Normalisasi data \u00b6 Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan. Atribut numerik jarak \u00b6 Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...). Atribut kategorikal jarak \u00b6 tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya. Contoh Datanya sebagai berikut \u00b6 Pertama import data terlebih dahulu di code untuk bisa ditampilkan.Pada Contoh Berikut saya mengambil data yang terkena HIV,saya mengambil data tersebut dari internet. import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder data = pd . read_csv ( 'HIVaids.csv' , delimiter = ';' , decimal = ',' ) #encode fitur tipe biner X = data . iloc [:,:] . values labelEncode_X = LabelEncoder () X [:, 4 ] = labelEncode_X . fit_transform ( X [:, 4 ]) df = pd . DataFrame ( data ) df . style . highlight_null ( null_color = 'red' ) . hide_index () Berikut Tampilan Data yang ada data kosong (null) missing values untuk bisa di knn no income days delay gender age hiv emergency 1 3358 30 4 Male 20.6708 87 89 2 3535 16 17 Male 55.2882 95 77 3 3547 40 1 Male 55.9151 95 116 4 3592 13 10 Male 61.6646 59 73 5 3728 19 6 Male 30.1273 67 73 6 3790 13 3 Male 57.0623 76 69 7 3807 37 5 Male 24.6762 74 77 8 3808 31 7 Male 28.2683 91 110 9 4253 40 3 Male 22.6037 115 110 10 4356 31 7 Male 21.399 86 83 11 4384 35 8 Male 36.3806 76 90 12 4542 22 11 Female 21.9576 71 89 13 4705 18 1 Female 21.6838 127 109 14 4744 15 25 Male 57.566 82 85 15 4802 36 0 Male 62.475 88 97 16 4941 46 4 Female 19.0144 69 88 17 4983 33 5 Male 38.3929 102 117 18 5129 26 1 Male 25.0459 77 89 19 5154 35 5 Male 22.1903 82 95 20 5162 33 1 Male 25.0185 118 101 21 5174 38 4 Female 37.2704 87 99 22 5208 31 8 Female 21.3771 97 90 23 5253 29 1 Male 33.1335 104 105 24 5298 30 3 Male 22.9569 87 86 25 5640 34 7 Male 25.9986 93 113 26 5668 27 7 Male 40.9227 72 79 27 5680 17 1 Male 27.7563 84 90 28 5699 26 1 Female 34.2231 95 108 29 5713 36 8 Male 16.2683 89 97 30 5736 18 9 Male 16.1478 89 86 31 5754 36 1 Male 16.3368 87 86 32 5776 26 8 Male 17.128 71 88 33 6122 29 1 Male 56.2108 95 103 34 6163 21 1 Male 19.3593 112 106 35 6179 22 2 Male 38.0123 89 95 36 6671 30 7 Female 27.8056 71 82 37 6859 27 1 Male 34.2122 74 79 38 6870 22 0 Male 42.4832 84 95 39 6914 43 0 Male 61.5222 85 90 40 6937 18 0 Female 21.191 94 81 41 6977 30 1 Male 36.2108 97 94 42 7120 39 0 Male 69.7057 84 86 43 7309 31 0 Female 50.6667 85 95 44 7321 23 0 Male 26.0041 84 83 45 7548 31 0 Male 24.3669 108 106 46 2364 41 14 Male 25.8097 84 94 47 2600 3333 9 Male 43.9398 86 80 48 2761 40 3 Female 24.3696 98 112 49 3237 65 9 Male 49.8508 67 67 50 3277 51 1 Male 37.4702 104 96 51 3346 44 18 Female 57.2758 79 85 52 3359 59 9 Female 56.8953 84 91 53 3373 39 28 Female 26.308 87 91 54 3544 32 14 Male 54.5298 81 98 55 3655 57 5 Female 21.9055 90 103 56 3762 48 6 Male 20.3559 85 93 57 3919 58 1 Male 30.3655 99 95 58 4094 50 2 Male 19.7262 79 93 59 4133 34 14 Male 20 70 88 60 4183 42 3 Male 26.2341 98 116 61 4189 69 4 Female 29.462 75 86 62 4315 63 0 Male 38.141 107 130 63 4482 58 14 Female 18.2341 86 103 64 4638 20 17 Male 20.512 82 72 65 4678 63 7 Male 46.6448 96 95 66 4696 54 4 Male 46.9569 101 112 67 4755 24 18 Male 27.5127 105 102 68 4837 42 10 Male 19.6906 83 88 69 4996 51 12 Male 43.0281 77 78 70 5009 50 7 Male 24.3806 61 104 71 5014 46 7 Female 23.7618 75 90 72 5192 60 1 Male 58.6283 87 97 73 5204 71 0 Male 59.0746 97 107 74 5238 44 3 Male 45.1006 99 103 75 5280 83 1 Male 48.6434 78 88 76 5289 52 1 Male 48.5722 84 85 77 5456 48 14 Male 41.1636 80 101 78 5458 44 14 Male 34.4778 84 95 79 5474 65 2 Female 28.6598 95 86 80 5568 64 1 Female 51.9918 75 79 81 5580 56 7 Male 17.7933 86 95 82 5581 65 2 Male 26.3053 85 95 83 5628 51 3 Female 30.2642 81 85 84 6154 43 5 Female 22.6064 74 80 85 6180 59 12 Male 20.7201 67 84 86 6314 58 3 Male 16.6927 80 99 87 6340 71 0 Male 19.3238 76 72 88 6564 69 0 Male 34.4997 67 74 89 6614 57 0 Male 45.1116 80 101 90 6686 44 14 Female 38.3491 90 100 91 6795 55 0 Male 30.7159 87 104 92 7080 64 5 Female 76.6598 76 106 93 7084 54 2 Male 36.5722 87 93 94 7271 55 0 Male 41.7659 100 95 95 7371 55 1 Male 56.7858 80 88 96 2569 49 35 Male 18.7159 50 101 97 3058 56 28 Male 22.2533 65 75 98 3645 43 45 Male 27.4935 72 90 99 3844 73 9 Male 26.1164 79 94 100 4725 124 10 Male 32.9172 93 97 101 4744 65 25 Male 57.566 105 119 102 4807 64 14 Female 47.7974 74 74 103 4892 62 21 Male 22.0397 76 88 104 4962 63 1 Female 25.1964 69 67 105 5125 78 12 Male 17.5387 94 118 106 5222 63 30 Male 22.5298 77 85 107 5253 86 1 Male 33.1335 106 128 108 5386 78 21 Male 20.8761 78 93 109 5534 87 14 Male 29.2621 75 82 110 5712 88 14 Male 22.2697 70 68 111 5837 82 1 Female 33.3087 82 110 112 5879 75 21 Male 25.8453 80 105 113 5893 71 21 Male 22.8118 65 90 114 5916 84 0 Female 26.8556 93 73 115 6410 80 14 Male 32.1725 85 98 116 7173 84 4 Male 24.9801 72 75 117 7221 98 0 Male 63.5044 74 79 118 2453 120 10 Male 37.2758 63 99 119 2653 97 28 Male 30.0068 93 112 120 4218 82 28 Male 25.9904 74 92 121 4542 121 11 Female 21.9576 86 114 122 4902 102 8 Male 16.1424 87 77 123 4933 134 0 Male 18.4559 69 83 124 4941 131 4 Female 19.0144 96 96 125 5085 117 2 Male 49.0267 67 71 126 5111 107 7 Male 21.6947 71 80 127 5154 120 5 Male 22.1903 89 109 128 5222 93 30 Male 22.5298 77 91 129 5298 107 3 Male 22.9569 117 112 130 5339 119 7 Male 21.8152 87 82 131 5387 109 12 Male 21.7988 85 112 132 5414 105 10 Female 40.2765 93 104 133 5494 111 7 Male 54.6913 86 86 134 5896 126 4 Female 26.8775 50 74 135 5901 115 7 Male 22.1739 112 116 136 6135 96 18 Male 26.5626 66 105 137 6173 125 4 Male 35.3046 94 97 138 6214 112 0 Male 60.3176 65 74 139 6253 128 0 Female 46.4038 104 112 140 6433 120 4 Male 23.8604 100 103 141 6665 119 3 Female 23.0171 106 94 142 6834 123 0 Male 30.7488 72 75 143 1176 146 17 Female 19.729 65 98 144 2849 151 0 Male 20.0876 51 86 145 2882 141 18 Male 19.2334 84 85 146 3051 131 13 Male 37.2403 68 79 147 3728 151 6 Male 30.1273 96 105 148 3913 96 42 Female 23.9233 56 80 149 4133 133 14 Male 20 82 94 150 4661 135 17 Female 30.8419 84 93 151 4678 143 7 Male 46.6448 98 107 152 4696 150 4 Male 46.9569 120 120 153 4705 146 1 Female 21.6838 133 111 154 4802 142 0 Male 62.475 101 117 155 4807 139 14 Female 47.7974 80 78 156 4983 146 5 Male 38.3929 107 123 157 5014 151 7 Female 23.7618 97 110 158 5162 144 1 Male 25.0185 130 118 159 5238 150 3 Male 45.1006 117 126 160 5642 162 0 Male 65.87 89 103 161 5699 138 1 Female 34.2231 110 107 162 5713 144 8 Male 16.2683 100 99 163 5804 159 2 Female 28.8515 102 107 164 5818 125 14 Male 34.9268 72 91 165 6314 140 3 Male 16.6927 87 96 166 6664 164 2 Male 24.7337 66 73 167 1048 85 94 Male 20.115 63 82 168 1085 159 11 Male 30.7105 103 97 169 3237 189 9 Male 49.8508 79 82 170 3358 175 4 Male 20.6708 97 97 171 3808 165 7 Male 28.2683 94 111 172 4094 177 2 Male 19.7262 89 102 173 4253 175 3 Male 22.6037 114 118 174 4638 140 17 Male 20.512 89 78 175 4755 128 18 Male 27.5127 105 109 176 4865 142 35 Male 58.3354 84 103 177 4892 148 21 Male 22.0397 106 110 178 5009 174 7 Male 24.3806 77 103 179 5111 177 7 Male 21.6947 72 81 180 5125 173 12 Male 17.5387 106 119 181 5192 179 1 Male 58.6283 93 105 182 5505 171 1 Male 65.4784 95 93 183 5581 176 2 Male 26.3053 96 110 184 5599 148 21 Male 18.7488 72 81 185 5680 184 1 Male 27.7563 84 90 186 5782 108 68 Female 19.6715 69 85 187 6180 177 12 Male 20.7201 81 94 188 6671 184 7 Female 27.8056 91 92 189 2124 173 30 Male 30.7625 76 106 190 2646 187 14 Male 22.9158 97 97 191 2790 211 0 Male 48.8049 89 99 192 4189 202 4 Female 29.462 81 90 193 4775 180 28 Male 53.5551 70 86 194 4933 226 0 Male 18.4559 79 86 195 4962 210 1 Female 25.1964 71 70 196 5208 193 8 Female 21.3771 133 111 197 5456 193 14 Male 41.1636 87 110 198 5668 219 7 Male 40.9227 76 90 199 5712 192 14 Male 22.2697 87 85 200 5893 200 21 Male 22.8118 65 89 201 5916 205 0 Female 26.8556 92 76 202 6122 212 1 Male 56.2108 109 117 203 6136 216 1 Male 32.7912 92 89 204 6175 278 1 Male 51.1704 99 98 205 6228 174 3 Female 31.5537 114 108 206 7173 210 4 Male 24.9801 79 78 207 1176 216 17 Female 19.729 74 100 208 3467 186 42 Male 25.3936 53 69 209 4744 217 25 Male 57.566 108 118 210 5386 241 21 Male 20.8761 80 94 211 5837 242 1 Female 33.3087 93 105 212 6247 228 13 Male 42.3162 77 80 213 1892 276 2 Male 21.7796 87 107 214 2882 262 18 Male 19.2334 94 90 215 3058 236 28 Male 22.2533 85 88 216 4342 263 1 Male 44.063 79 91 217 4865 240 35 Male 58.3354 93 105 218 5085 269 2 Male 49.0267 65 77 219 5222 247 30 Male 22.5298 88 85 220 5339 271 7 Male 21.8152 94 89 221 5474 280 2 Female 28.6598 99 91 222 5600 232 0 Male 48.7885 75 81 223 2826 290 14 Male 23.2334 94 108 224 4725 286 10 Male 32.9172 105 94 225 5204 299 0 Male 59.0746 99 105 226 6498 270 28 Male 24.0767 82 101 227 2081 185 43 Male 17.6975 77 97 228 4678 340 7 Male 46.6448 108 119 229 5397 328 0 Female 62.7981 121 108 230 6214 318 0 Male 60.3176 78 82 231 7034 280 60 Male 23.1376 78 80 232 1493 453 60 Male 17.8042 59 81 233 1836 375 1 Male 47.0554 101 108 234 1939 295 130 Male 28.2738 67 117 235 2646 438 14 Male 22.9158 98 94 236 2653 352 28 Male 30.0068 105 126 237 3226 444 0 Male 27.4552 76 64 238 3467 333 42 Male 25.3936 68 74 239 4342 432 1 Male 44.063 92 107 240 4542 431 11 Female 21.9576 98 114 241 4661 374 17 Female 30.8419 93 95 242 4902 397 8 Male 16.1424 92 86 243 4983 398 5 Male 38.3929 121 132 244 5111 442 7 Male 21.6947 77 86 245 5125 510 12 Male 17.5387 112 125 246 5289 417 1 Male 48.5722 83 83 247 5386 436 21 Male 20.8761 90 103 248 5387 480 12 Male 21.7988 94 116 249 5505 527 1 Male 65.4784 104 87 250 5580 369 7 Male 17.7933 96 107 251 5581 378 2 Male 26.3053 95 95 252 5599 443 21 Male 18.7488 78 80 253 5668 390 7 Male 40.9227 92 92 254 5680 403 1 Male 27.7563 94 93 255 5712 365 14 Male 22.2697 98 86 256 5772 412 35 Male 26.2587 102 104 257 5804 354 2 Female nan 122 105 258 5811 431 25 Male 80.0328 78 80 259 5841 415 8 Male 27.2279 82 83 260 6226 438 0 Male 36.8022 84 92 261 6247 389 13 Male 42.3162 82 80 262 6468 513 60 Male 43.4798 99 94 263 6614 362 0 Male 45.1116 88 106 264 6665 368 3 Female 23.0171 100 92 265 781 714 15 Male 29.8699 85 85 266 1048 576 94 Male 20.115 91 96 267 1157 810 23 Male 17.3881 97 84 268 1493 684 60 Male 17.8042 66 75 269 1611 511 60 Male 23.2799 69 107 270 1624 604 1 Male 19.5619 97 85 271 1939 562 130 Male 28.2738 85 111 272 2498 615 0 Female 17.4292 86 113 273 2826 636 14 Male 23.2334 111 101 274 2849 642 0 Male 20.0876 76 98 275 3032 525 20 Male 16.9391 79 87 276 3226 683 0 Male 27.4552 89 78 277 4218 814 28 Male 25.9904 99 96 278 4807 532 14 Female 47.7974 84 82 279 5014 637 7 Female 23.7618 101 114 280 5222 690 30 Male 22.5298 81 90 281 5253 591 1 Male 33.1335 114 124 282 5628 609 3 Female 30.2642 89 78 283 6059 794 1 Female 16.9801 71 76 284 6228 662 3 Female 31.5537 128 111 285 6247 616 13 Male 42.3162 85 82 286 405 986 0 Male 21.4702 66 116 287 626 870 55 Male 19.7536 80 85 288 1075 907 42 Female 27.2772 63 64 289 2849 1040 0 Male 20.0876 91 103 290 3032 884 20 Male 16.9391 87 93 291 3226 1123 0 Male 27.4552 88 81 292 4864 936 0 Female 53.9767 119 131 293 5474 1100 2 Female 28.6598 94 88 294 5568 1114 1 Female 51.9918 81 82 295 5580 1087 7 Male 17.7933 106 98 296 5581 1113 2 Male 26.3053 99 96 297 5617 1113 17 Male 19.7864 78 87 298 5642 1143 0 Male 65.87 104 109 299 5713 1016 8 Male 16.2683 126 106 300 5837 962 1 Female 33.3087 109 110 301 6140 1077 44 Female 21.4209 65 88 302 7061 923 0 Male 36.8816 74 81 303 651 1491 21 Male 22.0068 71 94 304 2527 1294 0 Male 16.9172 93 104 305 2638 1093 255 Male 16.5613 78 84 306 4865 1363 35 Male 58.3354 88 104 307 5009 1537 7 Male 24.3806 76 112 308 5014 1523 7 Female 23.7618 105 114 309 5085 1512 2 Male 49.0267 75 75 310 1939 1926 130 Male 28.2738 95 108 311 2662 1569 180 Male 28.0821 90 101 312 2826 1809 14 Male 23.2334 104 108 313 2882 1716 18 Male 19.2334 100 103 314 3768 1916 0 Male 19.1102 69 80 315 4356 2000 7 Male 21.399 104 91 316 4638 1779 17 Male 20.512 92 76 317 4696 1769 4 Male 46.9569 105 124 318 4744 1743 25 Male 57.566 97 118 319 6140 1742 44 Female 21.4209 67 87 320 1075 2259 42 Female 27.2772 78 79 321 1939 3111 130 Male 28.2738 88 111 322 2653 2191 28 Male 30.0068 117 129 323 3592 2569 10 Male 61.6646 76 93 324 3808 2434 7 Male 28.2683 105 111 325 651 3412 21 Male 22.0068 68 92 326 1939 3864 130 Male 28.2738 88 105 327 2600 3337 9 Male 43.9398 101 84 328 3835 4933 14 Male 25.9932 91 88 329 2773 7631 42 Male 6.51335 88 103 330 5142 11628 57 Male 16.4326 101 95 331 5964 11038 0 Male 12.8363 71 73 Selanjutnya kita mengimplementasikan rumus jarak ke dalam bentuk fungsi python. yaitu: eulidianDistance() dengan fungsi jarak tipe binary distanceSimetris (). def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t )) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) Kemudian dari dataset tersebut, kita lakukan pengecekan dengan mencari baris yang missing values,. c_j = 0 for j in df [ 'age' ] . isna (): if j == True : col_missing = c_j c_j += 1 Pada langkah berikut, kita lakukan perhitungan jarak pada data yang missing dengan seluruh tetangganya dan menampungnya pada dapat dictionary yang ada. missing_data = df . iloc [ col_missing , [ 2 , 3 , 6 , 7 ]] . values missing_normal = [ normalisasi ( missing_data [ 0 ], data . columns [ 2 ]), normalisasi ( missing_data [ 1 ], data . columns [ 3 ]), normalisasi ( missing_data [ 2 ], data . columns [ 6 ]), normalisasi ( missing_data [ 3 ], data . columns [ 7 ])] for i in range ( len ( data [ data . columns [ 0 ]])): if i == col_missing : continue ; select_data = df . iloc [ i , [ 2 , 3 , 6 , 7 ]] . values normal_data = [ normalisasi ( select_data [ 0 ], data . columns [ 2 ]), normalisasi ( select_data [ 1 ], data . columns [ 3 ]), normalisasi ( select_data [ 2 ], data . columns [ 6 ]), normalisasi ( select_data [ 3 ], data . columns [ 7 ])] data . loc [ i , 'jarak' ] = euclidianDistance ( missing_normal , normal_data ) + distanceSimetris ([ X [ col_missing , 4 ]],[ X [ i , 4 ]]) Kemudian kita urutkan data tersebut berdasarkan jarak dari yang terkecil sampai ke terbesar. Selanjutnya kita mengisi data yang hilang dengan mengambil rata-rata dari 2 tetangga terdekat. df = pd . DataFrame ( data ) df . sort_values ( by = 'jarak' , axis = 0 , ascending = True , inplace = True ) df . iloc [ - 1 , [ 5 ]] = round ( df . iloc [ 0 : 2 , 5 ] . mean (), 2 ) df . style . hide_index () Berikut merupakan tampilan dari data yang telah di urutkan. pada baris terakhir telihat bahwa kolom age sudah terisi dengan angka sebagai berikut no income days delay gender age hiv emergency jarak 229 5397 328 0 Female 62.7981 121 108 0.25 13 4705 18 1 Female 21.6838 127 109 0.53 205 6228 174 3 Female 31.5537 114 108 0.6 284 6228 662 3 Female 31.5537 128 111 0.64 161 5699 138 1 Female 34.2231 110 107 0.84 153 4705 146 1 Female 21.6838 133 111 0.86 196 5208 193 8 Female 21.3771 133 111 0.88 300 5837 962 1 Female 33.3087 109 110 1.07 139 6253 128 0 Female 46.4038 104 112 1.31 141 6665 119 3 Female 23.0171 106 94 1.33 163 5804 159 2 Female 28.8515 102 107 1.35 20 5162 33 1 Male 25.0185 118 101 1.48 279 5014 637 7 Female 23.7618 101 114 1.56 129 5298 107 3 Male 22.9569 117 112 1.64 9 4253 40 3 Male 22.6037 115 110 1.66 308 5014 1523 7 Female 23.7618 105 114 1.67 299 5713 1016 8 Male 16.2683 126 106 1.68 48 2761 40 3 Female 24.3696 98 112 1.69 157 5014 151 7 Female 23.7618 97 110 1.72 264 6665 368 3 Female 23.0171 100 92 1.73 34 6163 21 1 Male 19.3593 112 106 1.73 240 4542 431 11 Female 21.9576 98 114 1.75 221 5474 280 2 Female 28.6598 99 91 1.82 28 5699 26 1 Female 34.2231 95 108 1.83 124 4941 131 4 Female 19.0144 96 96 1.85 211 5837 242 1 Female 33.3087 93 105 1.92 292 4864 936 0 Female 53.9767 119 131 1.93 273 2826 636 14 Male 23.2334 111 101 1.94 132 5414 105 10 Female 40.2765 93 104 1.96 45 7548 31 0 Male 24.3669 108 106 1.98 22 5208 31 8 Female 21.3771 97 90 2 135 5901 115 7 Male 22.1739 112 116 2.07 173 4253 175 3 Male 22.6037 114 118 2.08 158 5162 144 1 Male 25.0185 130 118 2.09 152 4696 150 4 Male 46.9569 120 120 2.1 241 4661 374 17 Female 30.8419 93 95 2.13 55 3655 57 5 Female 21.9055 90 103 2.14 202 6122 212 1 Male 56.2108 109 117 2.22 90 6686 44 14 Female 38.3491 90 100 2.22 23 5253 29 1 Male 33.1335 104 105 2.23 79 5474 65 2 Female 28.6598 95 86 2.26 188 6671 184 7 Female 27.8056 91 92 2.26 293 5474 1100 2 Female 28.6598 94 88 2.3 67 4755 24 18 Male 27.5127 105 102 2.33 175 4755 128 18 Male 27.5127 105 109 2.33 177 4892 148 21 Male 22.0397 106 110 2.35 295 5580 1087 7 Male 17.7933 106 98 2.35 50 3277 51 1 Male 37.4702 104 96 2.38 21 5174 38 4 Female 37.2704 87 99 2.38 228 4678 340 7 Male 46.6448 108 119 2.38 298 5642 1143 0 Male 65.87 104 109 2.41 224 4725 286 10 Male 32.9172 105 94 2.41 233 1836 375 1 Male 47.0554 101 108 2.41 168 1085 159 11 Male 30.7105 103 97 2.43 63 4482 58 14 Female 18.2341 86 103 2.44 272 2498 615 0 Female 17.4292 86 113 2.46 140 6433 120 4 Male 23.8604 100 103 2.48 281 5253 591 1 Male 33.1335 114 124 2.48 121 4542 121 11 Female 21.9576 86 114 2.5 66 4696 54 4 Male 46.9569 101 112 2.5 180 5125 173 12 Male 17.5387 106 119 2.51 225 5204 299 0 Male 59.0746 99 105 2.52 40 6937 18 0 Female 21.191 94 81 2.53 162 5713 144 8 Male 16.2683 100 99 2.55 159 5238 150 3 Male 45.1006 117 126 2.55 74 5238 44 3 Male 45.1006 99 103 2.55 43 7309 31 0 Female 50.6667 85 95 2.57 209 4744 217 25 Male 57.566 108 118 2.59 204 6175 278 1 Male 51.1704 99 98 2.6 17 4983 33 5 Male 38.3929 102 117 2.61 151 4678 143 7 Male 46.6448 98 107 2.62 245 5125 510 12 Male 17.5387 112 125 2.63 156 4983 146 5 Male 38.3929 107 123 2.64 94 7271 55 0 Male 41.7659 100 95 2.65 154 4802 142 0 Male 62.475 101 117 2.65 111 5837 82 1 Female 33.3087 82 110 2.69 73 5204 71 0 Male 59.0746 97 107 2.69 57 3919 58 1 Male 30.3655 99 95 2.7 150 4661 135 17 Female 30.8419 84 93 2.72 52 3359 59 9 Female 56.8953 84 91 2.72 53 3373 39 28 Female 26.308 87 91 2.73 250 5580 369 7 Male 17.7933 96 107 2.74 147 3728 151 6 Male 30.1273 96 105 2.74 249 5505 527 1 Male 65.4784 104 87 2.75 183 5581 176 2 Male 26.3053 96 110 2.76 170 3358 175 4 Male 20.6708 97 97 2.76 101 4744 65 25 Male 57.566 105 119 2.77 296 5581 1113 2 Male 26.3053 99 96 2.78 60 4183 42 3 Male 26.2341 98 116 2.8 312 2826 1809 14 Male 23.2334 104 108 2.82 190 2646 187 14 Male 22.9158 97 97 2.82 33 6122 29 1 Male 56.2108 95 103 2.82 235 2646 438 14 Male 22.9158 98 94 2.83 256 5772 412 35 Male 26.2587 102 104 2.84 41 6977 30 1 Male 36.2108 97 94 2.86 201 5916 205 0 Female 26.8556 92 76 2.87 65 4678 63 7 Male 46.6448 96 95 2.89 282 5628 609 3 Female 30.2642 89 78 2.91 192 4189 202 4 Female 29.462 81 90 2.91 223 2826 290 14 Male 23.2334 94 108 2.92 171 3808 165 7 Male 28.2683 94 111 2.92 251 5581 378 2 Male 26.3053 95 95 2.93 243 4983 398 5 Male 38.3929 121 132 2.93 181 5192 179 1 Male 58.6283 93 105 2.93 137 6173 125 4 Male 35.3046 94 97 2.95 107 5253 86 1 Male 33.1335 106 128 2.97 277 4218 814 28 Male 25.9904 99 96 2.97 114 5916 84 0 Female 26.8556 93 73 2.98 3 3547 40 1 Male 55.9151 95 116 2.98 313 2882 1716 18 Male 19.2334 100 103 2.99 182 5505 171 1 Male 65.4784 95 93 2.99 239 4342 432 1 Male 44.063 92 107 3 278 4807 532 14 Female 47.7974 84 82 3.03 25 5640 34 7 Male 25.9986 93 113 3.03 100 4725 124 10 Male 32.9172 93 97 3.04 254 5680 403 1 Male 27.7563 94 93 3.04 248 5387 480 12 Male 21.7988 94 116 3.05 62 4315 63 0 Male 38.141 107 130 3.05 92 7080 64 5 Female 76.6598 76 106 3.05 83 5628 51 3 Female 30.2642 81 85 3.07 304 2527 1294 0 Male 16.9172 93 104 3.09 8 3808 31 7 Male 28.2683 91 110 3.11 105 5125 78 12 Male 17.5387 94 118 3.12 236 2653 352 28 Male 30.0068 105 126 3.13 315 4356 2000 7 Male 21.399 104 91 3.13 289 2849 1040 0 Male 20.0876 91 103 3.14 255 5712 365 14 Male 22.2697 98 86 3.14 317 4696 1769 4 Male 46.9569 105 124 3.16 220 5339 271 7 Male 21.8152 94 89 3.18 160 5642 162 0 Male 65.87 89 103 3.19 324 3808 2434 7 Male 28.2683 105 111 3.2 172 4094 177 2 Male 19.7262 89 102 3.2 270 1624 604 1 Male 19.5619 97 85 3.2 253 5668 390 7 Male 40.9227 92 92 3.2 127 5154 120 5 Male 22.1903 89 109 3.21 214 2882 262 18 Male 19.2334 94 90 3.22 119 2653 97 28 Male 30.0068 93 112 3.23 191 2790 211 0 Male 48.8049 89 99 3.23 294 5568 1114 1 Female 51.9918 81 82 3.23 51 3346 44 18 Female 57.2758 79 85 3.25 263 6614 362 0 Male 45.1116 88 106 3.25 207 1176 216 17 Female 19.729 74 100 3.25 247 5386 436 21 Male 20.8761 90 103 3.25 29 5713 36 8 Male 16.2683 89 97 3.28 203 6136 216 1 Male 32.7912 92 89 3.29 217 4865 240 35 Male 58.3354 93 105 3.3 71 5014 46 7 Female 23.7618 75 90 3.3 35 6179 22 2 Male 38.0123 89 95 3.31 213 1892 276 2 Male 21.7796 87 107 3.33 91 6795 55 0 Male 30.7159 87 104 3.34 15 4802 36 0 Male 62.475 88 97 3.34 72 5192 60 1 Male 58.6283 87 97 3.4 61 4189 69 4 Female 29.462 75 86 3.4 267 1157 810 23 Male 17.3881 97 84 3.4 197 5456 193 14 Male 41.1636 87 110 3.4 165 6314 140 3 Male 16.6927 87 96 3.41 242 4902 397 8 Male 16.1424 92 86 3.42 155 4807 139 14 Female 47.7974 80 78 3.42 318 4744 1743 25 Male 57.566 97 118 3.43 93 7084 54 2 Male 36.5722 87 93 3.49 81 5580 56 7 Male 17.7933 86 95 3.5 131 5387 109 12 Male 21.7988 85 112 3.54 115 6410 80 14 Male 32.1725 85 98 3.55 82 5581 65 2 Male 26.3053 85 95 3.56 322 2653 2191 28 Male 30.0068 117 129 3.57 12 4542 22 11 Female 21.9576 71 89 3.58 1 3358 30 4 Male 20.6708 87 89 3.6 30 5736 18 9 Male 16.1478 89 86 3.6 290 3032 884 20 Male 16.9391 87 93 3.61 56 3762 48 6 Male 20.3559 85 93 3.61 80 5568 64 1 Female 51.9918 75 79 3.63 38 6870 22 0 Male 42.4832 84 95 3.63 84 6154 43 5 Female 22.6064 74 80 3.65 78 5458 44 14 Male 34.4778 84 95 3.66 46 2364 41 14 Male 25.8097 84 94 3.68 260 6226 438 0 Male 36.8022 84 92 3.68 39 6914 43 0 Male 61.5222 85 90 3.69 24 5298 30 3 Male 22.9569 87 86 3.7 31 5754 36 1 Male 16.3368 87 86 3.7 16 4941 46 4 Female 19.0144 69 88 3.72 185 5680 184 1 Male 27.7563 84 90 3.73 306 4865 1363 35 Male 58.3354 88 104 3.73 27 5680 17 1 Male 27.7563 84 90 3.74 2 3535 16 17 Male 55.2882 95 77 3.75 133 5494 111 7 Male 54.6913 86 86 3.75 36 6671 30 7 Female 27.8056 71 82 3.76 19 5154 35 5 Male 22.1903 82 95 3.76 199 5712 192 14 Male 22.2697 87 85 3.76 262 6468 513 60 Male 43.4798 99 94 3.81 149 4133 133 14 Male 20 82 94 3.81 89 6614 57 0 Male 45.1116 80 101 3.81 54 3544 32 14 Male 54.5298 81 98 3.81 176 4865 142 35 Male 58.3354 84 103 3.82 86 6314 58 3 Male 16.6927 80 99 3.82 77 5456 48 14 Male 41.1636 80 101 3.84 130 5339 119 7 Male 21.8152 87 82 3.85 187 6180 177 12 Male 20.7201 81 94 3.85 226 6498 270 28 Male 24.0767 82 101 3.85 143 1176 146 17 Female 19.729 65 98 3.85 42 7120 39 0 Male 69.7057 84 86 3.86 219 5222 247 30 Male 22.5298 88 85 3.87 10 4356 31 7 Male 21.399 86 83 3.87 68 4837 42 10 Male 19.6906 83 88 3.88 112 5879 75 21 Male 25.8453 80 105 3.89 265 781 714 15 Male 29.8699 85 85 3.89 76 5289 52 1 Male 48.5722 84 85 3.9 102 4807 64 14 Female 47.7974 74 74 3.9 291 3226 1123 0 Male 27.4552 88 81 3.9 215 3058 236 28 Male 22.2533 85 88 3.91 276 3226 683 0 Male 27.4552 89 78 3.92 145 2882 141 18 Male 19.2334 84 85 3.95 44 7321 23 0 Male 26.0041 84 83 3.97 283 6059 794 1 Female 16.9801 71 76 3.97 174 4638 140 17 Male 20.512 89 78 3.97 210 5386 241 21 Male 20.8761 80 94 3.98 285 6247 616 13 Male 42.3162 85 82 3.98 99 3844 73 9 Male 26.1164 79 94 3.98 178 5009 174 7 Male 24.3806 77 103 3.99 58 4094 50 2 Male 19.7262 79 93 3.99 246 5289 417 1 Male 48.5722 83 83 4.02 216 4342 263 1 Male 44.063 79 91 4.02 95 7371 55 1 Male 56.7858 80 88 4.04 122 4902 102 8 Male 16.1424 87 77 4.07 259 5841 415 8 Male 27.2279 82 83 4.08 274 2849 642 0 Male 20.0876 76 98 4.09 280 5222 690 30 Male 22.5298 81 90 4.11 320 1075 2259 42 Female 27.2772 78 79 4.13 108 5386 78 21 Male 20.8761 78 93 4.13 14 4744 15 25 Male 57.566 82 85 4.15 75 5280 83 1 Male 48.6434 78 88 4.16 194 4933 226 0 Male 18.4559 79 86 4.16 316 4638 1779 17 Male 20.512 92 76 4.18 195 4962 210 1 Female 25.1964 71 70 4.19 18 5129 26 1 Male 25.0459 77 89 4.2 275 3032 525 20 Male 16.9391 79 87 4.2 261 6247 389 13 Male 42.3162 82 80 4.21 189 2124 173 30 Male 30.7625 76 106 4.23 198 5668 219 7 Male 40.9227 76 90 4.23 11 4384 35 8 Male 36.3806 76 90 4.24 307 5009 1537 7 Male 24.3806 76 112 4.26 244 5111 442 7 Male 21.6947 77 86 4.28 169 3237 189 9 Male 49.8508 79 82 4.3 297 5617 1113 17 Male 19.7864 78 87 4.3 301 6140 1077 44 Female 21.4209 65 88 4.32 327 2600 3337 9 Male 43.9398 101 84 4.33 128 5222 93 30 Male 22.5298 77 91 4.33 230 6214 318 0 Male 60.3176 78 82 4.34 103 4892 62 21 Male 22.0397 76 88 4.36 319 6140 1742 44 Female 21.4209 67 87 4.36 227 2081 185 43 Male 17.6975 77 97 4.42 206 7173 210 4 Male 24.9801 79 78 4.44 104 4962 63 1 Female 25.1964 69 67 4.44 120 4218 82 28 Male 25.9904 74 92 4.47 252 5599 443 21 Male 18.7488 78 80 4.48 106 5222 63 30 Male 22.5298 77 85 4.48 164 5818 125 14 Male 34.9268 72 91 4.49 212 6247 228 13 Male 42.3162 77 80 4.49 258 5811 431 25 Male 80.0328 78 80 4.52 222 5600 232 0 Male 48.7885 75 81 4.55 109 5534 87 14 Male 29.2621 75 82 4.55 186 5782 108 68 Female 19.6715 69 85 4.56 69 4996 51 12 Male 43.0281 77 78 4.58 64 4638 20 17 Male 20.512 82 72 4.59 32 5776 26 8 Male 17.128 71 88 4.6 302 7061 923 0 Male 36.8816 74 81 4.64 303 651 1491 21 Male 22.0068 71 94 4.67 59 4133 34 14 Male 20 70 88 4.69 117 7221 98 0 Male 63.5044 74 79 4.69 37 6859 27 1 Male 34.2122 74 79 4.69 323 3592 2569 10 Male 61.6646 76 93 4.72 179 5111 177 7 Male 21.6947 72 81 4.73 287 626 870 55 Male 19.7536 80 85 4.75 7 3807 37 5 Male 24.6762 74 77 4.76 136 6135 96 18 Male 26.5626 66 105 4.76 184 5599 148 21 Male 18.7488 72 81 4.8 26 5668 27 7 Male 40.9227 72 79 4.81 286 405 986 0 Male 21.4702 66 116 4.82 126 5111 107 7 Male 21.6947 71 80 4.82 193 4775 180 28 Male 53.5551 70 86 4.83 123 4933 134 0 Male 18.4559 69 83 4.85 87 6340 71 0 Male 19.3238 76 72 4.85 98 3645 43 45 Male 27.4935 72 90 4.86 309 5085 1512 2 Male 49.0267 75 75 4.91 118 2453 120 10 Male 37.2758 63 99 4.94 116 7173 84 4 Male 24.9801 72 75 4.94 142 6834 123 0 Male 30.7488 72 75 4.94 85 6180 59 12 Male 20.7201 67 84 4.96 148 3913 96 42 Female 23.9233 56 80 4.96 47 2600 3333 9 Male 43.9398 86 80 4.97 6 3790 13 3 Male 57.0623 76 69 4.99 113 5893 71 21 Male 22.8118 65 90 4.99 200 5893 200 21 Male 22.8118 65 89 5.01 70 5009 50 7 Male 24.3806 61 104 5.05 146 3051 131 13 Male 37.2403 68 79 5.05 231 7034 280 60 Male 23.1376 78 80 5.07 288 1075 907 42 Female 27.2772 63 64 5.12 266 1048 576 94 Male 20.115 91 96 5.14 269 1611 511 60 Male 23.2799 69 107 5.16 314 3768 1916 0 Male 19.1102 69 80 5.16 237 3226 444 0 Male 27.4552 76 64 5.21 134 5896 126 4 Female 26.8775 50 74 5.25 88 6564 69 0 Male 34.4997 67 74 5.26 218 5085 269 2 Male 49.0267 65 77 5.26 5 3728 19 6 Male 30.1273 67 73 5.3 166 6664 164 2 Male 24.7337 66 73 5.34 110 5712 88 14 Male 22.2697 70 68 5.36 138 6214 112 0 Male 60.3176 65 74 5.37 125 5085 117 2 Male 49.0267 67 71 5.38 97 3058 56 28 Male 22.2533 65 75 5.45 238 3467 333 42 Male 25.3936 68 74 5.46 49 3237 65 9 Male 49.8508 67 67 5.55 325 651 3412 21 Male 22.0068 68 92 5.62 328 3835 4933 14 Male 25.9932 91 88 5.69 4 3592 13 10 Male 61.6646 59 73 5.77 268 1493 684 60 Male 17.8042 66 75 5.83 144 2849 151 0 Male 20.0876 51 86 5.89 96 2569 49 35 Male 18.7159 50 101 5.94 232 1493 453 60 Male 17.8042 59 81 6.03 310 1939 1926 130 Male 28.2738 95 108 6.41 208 3467 186 42 Male 25.3936 53 69 6.45 167 1048 85 94 Male 20.115 63 82 6.51 271 1939 562 130 Male 28.2738 85 111 6.51 321 1939 3111 130 Male 28.2738 88 111 6.93 234 1939 295 130 Male 28.2738 67 117 7.17 326 1939 3864 130 Male 28.2738 88 105 7.22 329 2773 7631 42 Male 6.51335 88 103 7.96 311 2662 1569 180 Male 28.0821 90 101 8.24 331 5964 11038 0 Male 12.8363 71 73 11.23 330 5142 11628 57 Male 16.4326 101 95 11.25 305 2638 1093 255 Male 16.5613 78 84 11.27 257 5804 354 2 Female 42.24 122 105 nan","title":"Missing Values"},{"location":"Missing Values/#missing-values-using-knn","text":"KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan.","title":"Missing Values using KNN"},{"location":"Missing Values/#kalibrasi-parameter-knn","text":"","title":"Kalibrasi Parameter KNN"},{"location":"Missing Values/#jumlah-tetangga-yang-harus-dicari","text":"Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan.","title":"Jumlah tetangga yang harus dicari"},{"location":"Missing Values/#metode-agregasi-untuk-digunakan","text":"Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal","title":"Metode agregasi untuk digunakan"},{"location":"Missing Values/#normalisasi-data","text":"Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan.","title":"Normalisasi data"},{"location":"Missing Values/#atribut-numerik-jarak","text":"Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...).","title":"Atribut numerik jarak"},{"location":"Missing Values/#atribut-kategorikal-jarak","text":"tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya.","title":"Atribut kategorikal jarak"},{"location":"Missing Values/#contoh-datanya-sebagai-berikut","text":"Pertama import data terlebih dahulu di code untuk bisa ditampilkan.Pada Contoh Berikut saya mengambil data yang terkena HIV,saya mengambil data tersebut dari internet. import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder data = pd . read_csv ( 'HIVaids.csv' , delimiter = ';' , decimal = ',' ) #encode fitur tipe biner X = data . iloc [:,:] . values labelEncode_X = LabelEncoder () X [:, 4 ] = labelEncode_X . fit_transform ( X [:, 4 ]) df = pd . DataFrame ( data ) df . style . highlight_null ( null_color = 'red' ) . hide_index () Berikut Tampilan Data yang ada data kosong (null) missing values untuk bisa di knn no income days delay gender age hiv emergency 1 3358 30 4 Male 20.6708 87 89 2 3535 16 17 Male 55.2882 95 77 3 3547 40 1 Male 55.9151 95 116 4 3592 13 10 Male 61.6646 59 73 5 3728 19 6 Male 30.1273 67 73 6 3790 13 3 Male 57.0623 76 69 7 3807 37 5 Male 24.6762 74 77 8 3808 31 7 Male 28.2683 91 110 9 4253 40 3 Male 22.6037 115 110 10 4356 31 7 Male 21.399 86 83 11 4384 35 8 Male 36.3806 76 90 12 4542 22 11 Female 21.9576 71 89 13 4705 18 1 Female 21.6838 127 109 14 4744 15 25 Male 57.566 82 85 15 4802 36 0 Male 62.475 88 97 16 4941 46 4 Female 19.0144 69 88 17 4983 33 5 Male 38.3929 102 117 18 5129 26 1 Male 25.0459 77 89 19 5154 35 5 Male 22.1903 82 95 20 5162 33 1 Male 25.0185 118 101 21 5174 38 4 Female 37.2704 87 99 22 5208 31 8 Female 21.3771 97 90 23 5253 29 1 Male 33.1335 104 105 24 5298 30 3 Male 22.9569 87 86 25 5640 34 7 Male 25.9986 93 113 26 5668 27 7 Male 40.9227 72 79 27 5680 17 1 Male 27.7563 84 90 28 5699 26 1 Female 34.2231 95 108 29 5713 36 8 Male 16.2683 89 97 30 5736 18 9 Male 16.1478 89 86 31 5754 36 1 Male 16.3368 87 86 32 5776 26 8 Male 17.128 71 88 33 6122 29 1 Male 56.2108 95 103 34 6163 21 1 Male 19.3593 112 106 35 6179 22 2 Male 38.0123 89 95 36 6671 30 7 Female 27.8056 71 82 37 6859 27 1 Male 34.2122 74 79 38 6870 22 0 Male 42.4832 84 95 39 6914 43 0 Male 61.5222 85 90 40 6937 18 0 Female 21.191 94 81 41 6977 30 1 Male 36.2108 97 94 42 7120 39 0 Male 69.7057 84 86 43 7309 31 0 Female 50.6667 85 95 44 7321 23 0 Male 26.0041 84 83 45 7548 31 0 Male 24.3669 108 106 46 2364 41 14 Male 25.8097 84 94 47 2600 3333 9 Male 43.9398 86 80 48 2761 40 3 Female 24.3696 98 112 49 3237 65 9 Male 49.8508 67 67 50 3277 51 1 Male 37.4702 104 96 51 3346 44 18 Female 57.2758 79 85 52 3359 59 9 Female 56.8953 84 91 53 3373 39 28 Female 26.308 87 91 54 3544 32 14 Male 54.5298 81 98 55 3655 57 5 Female 21.9055 90 103 56 3762 48 6 Male 20.3559 85 93 57 3919 58 1 Male 30.3655 99 95 58 4094 50 2 Male 19.7262 79 93 59 4133 34 14 Male 20 70 88 60 4183 42 3 Male 26.2341 98 116 61 4189 69 4 Female 29.462 75 86 62 4315 63 0 Male 38.141 107 130 63 4482 58 14 Female 18.2341 86 103 64 4638 20 17 Male 20.512 82 72 65 4678 63 7 Male 46.6448 96 95 66 4696 54 4 Male 46.9569 101 112 67 4755 24 18 Male 27.5127 105 102 68 4837 42 10 Male 19.6906 83 88 69 4996 51 12 Male 43.0281 77 78 70 5009 50 7 Male 24.3806 61 104 71 5014 46 7 Female 23.7618 75 90 72 5192 60 1 Male 58.6283 87 97 73 5204 71 0 Male 59.0746 97 107 74 5238 44 3 Male 45.1006 99 103 75 5280 83 1 Male 48.6434 78 88 76 5289 52 1 Male 48.5722 84 85 77 5456 48 14 Male 41.1636 80 101 78 5458 44 14 Male 34.4778 84 95 79 5474 65 2 Female 28.6598 95 86 80 5568 64 1 Female 51.9918 75 79 81 5580 56 7 Male 17.7933 86 95 82 5581 65 2 Male 26.3053 85 95 83 5628 51 3 Female 30.2642 81 85 84 6154 43 5 Female 22.6064 74 80 85 6180 59 12 Male 20.7201 67 84 86 6314 58 3 Male 16.6927 80 99 87 6340 71 0 Male 19.3238 76 72 88 6564 69 0 Male 34.4997 67 74 89 6614 57 0 Male 45.1116 80 101 90 6686 44 14 Female 38.3491 90 100 91 6795 55 0 Male 30.7159 87 104 92 7080 64 5 Female 76.6598 76 106 93 7084 54 2 Male 36.5722 87 93 94 7271 55 0 Male 41.7659 100 95 95 7371 55 1 Male 56.7858 80 88 96 2569 49 35 Male 18.7159 50 101 97 3058 56 28 Male 22.2533 65 75 98 3645 43 45 Male 27.4935 72 90 99 3844 73 9 Male 26.1164 79 94 100 4725 124 10 Male 32.9172 93 97 101 4744 65 25 Male 57.566 105 119 102 4807 64 14 Female 47.7974 74 74 103 4892 62 21 Male 22.0397 76 88 104 4962 63 1 Female 25.1964 69 67 105 5125 78 12 Male 17.5387 94 118 106 5222 63 30 Male 22.5298 77 85 107 5253 86 1 Male 33.1335 106 128 108 5386 78 21 Male 20.8761 78 93 109 5534 87 14 Male 29.2621 75 82 110 5712 88 14 Male 22.2697 70 68 111 5837 82 1 Female 33.3087 82 110 112 5879 75 21 Male 25.8453 80 105 113 5893 71 21 Male 22.8118 65 90 114 5916 84 0 Female 26.8556 93 73 115 6410 80 14 Male 32.1725 85 98 116 7173 84 4 Male 24.9801 72 75 117 7221 98 0 Male 63.5044 74 79 118 2453 120 10 Male 37.2758 63 99 119 2653 97 28 Male 30.0068 93 112 120 4218 82 28 Male 25.9904 74 92 121 4542 121 11 Female 21.9576 86 114 122 4902 102 8 Male 16.1424 87 77 123 4933 134 0 Male 18.4559 69 83 124 4941 131 4 Female 19.0144 96 96 125 5085 117 2 Male 49.0267 67 71 126 5111 107 7 Male 21.6947 71 80 127 5154 120 5 Male 22.1903 89 109 128 5222 93 30 Male 22.5298 77 91 129 5298 107 3 Male 22.9569 117 112 130 5339 119 7 Male 21.8152 87 82 131 5387 109 12 Male 21.7988 85 112 132 5414 105 10 Female 40.2765 93 104 133 5494 111 7 Male 54.6913 86 86 134 5896 126 4 Female 26.8775 50 74 135 5901 115 7 Male 22.1739 112 116 136 6135 96 18 Male 26.5626 66 105 137 6173 125 4 Male 35.3046 94 97 138 6214 112 0 Male 60.3176 65 74 139 6253 128 0 Female 46.4038 104 112 140 6433 120 4 Male 23.8604 100 103 141 6665 119 3 Female 23.0171 106 94 142 6834 123 0 Male 30.7488 72 75 143 1176 146 17 Female 19.729 65 98 144 2849 151 0 Male 20.0876 51 86 145 2882 141 18 Male 19.2334 84 85 146 3051 131 13 Male 37.2403 68 79 147 3728 151 6 Male 30.1273 96 105 148 3913 96 42 Female 23.9233 56 80 149 4133 133 14 Male 20 82 94 150 4661 135 17 Female 30.8419 84 93 151 4678 143 7 Male 46.6448 98 107 152 4696 150 4 Male 46.9569 120 120 153 4705 146 1 Female 21.6838 133 111 154 4802 142 0 Male 62.475 101 117 155 4807 139 14 Female 47.7974 80 78 156 4983 146 5 Male 38.3929 107 123 157 5014 151 7 Female 23.7618 97 110 158 5162 144 1 Male 25.0185 130 118 159 5238 150 3 Male 45.1006 117 126 160 5642 162 0 Male 65.87 89 103 161 5699 138 1 Female 34.2231 110 107 162 5713 144 8 Male 16.2683 100 99 163 5804 159 2 Female 28.8515 102 107 164 5818 125 14 Male 34.9268 72 91 165 6314 140 3 Male 16.6927 87 96 166 6664 164 2 Male 24.7337 66 73 167 1048 85 94 Male 20.115 63 82 168 1085 159 11 Male 30.7105 103 97 169 3237 189 9 Male 49.8508 79 82 170 3358 175 4 Male 20.6708 97 97 171 3808 165 7 Male 28.2683 94 111 172 4094 177 2 Male 19.7262 89 102 173 4253 175 3 Male 22.6037 114 118 174 4638 140 17 Male 20.512 89 78 175 4755 128 18 Male 27.5127 105 109 176 4865 142 35 Male 58.3354 84 103 177 4892 148 21 Male 22.0397 106 110 178 5009 174 7 Male 24.3806 77 103 179 5111 177 7 Male 21.6947 72 81 180 5125 173 12 Male 17.5387 106 119 181 5192 179 1 Male 58.6283 93 105 182 5505 171 1 Male 65.4784 95 93 183 5581 176 2 Male 26.3053 96 110 184 5599 148 21 Male 18.7488 72 81 185 5680 184 1 Male 27.7563 84 90 186 5782 108 68 Female 19.6715 69 85 187 6180 177 12 Male 20.7201 81 94 188 6671 184 7 Female 27.8056 91 92 189 2124 173 30 Male 30.7625 76 106 190 2646 187 14 Male 22.9158 97 97 191 2790 211 0 Male 48.8049 89 99 192 4189 202 4 Female 29.462 81 90 193 4775 180 28 Male 53.5551 70 86 194 4933 226 0 Male 18.4559 79 86 195 4962 210 1 Female 25.1964 71 70 196 5208 193 8 Female 21.3771 133 111 197 5456 193 14 Male 41.1636 87 110 198 5668 219 7 Male 40.9227 76 90 199 5712 192 14 Male 22.2697 87 85 200 5893 200 21 Male 22.8118 65 89 201 5916 205 0 Female 26.8556 92 76 202 6122 212 1 Male 56.2108 109 117 203 6136 216 1 Male 32.7912 92 89 204 6175 278 1 Male 51.1704 99 98 205 6228 174 3 Female 31.5537 114 108 206 7173 210 4 Male 24.9801 79 78 207 1176 216 17 Female 19.729 74 100 208 3467 186 42 Male 25.3936 53 69 209 4744 217 25 Male 57.566 108 118 210 5386 241 21 Male 20.8761 80 94 211 5837 242 1 Female 33.3087 93 105 212 6247 228 13 Male 42.3162 77 80 213 1892 276 2 Male 21.7796 87 107 214 2882 262 18 Male 19.2334 94 90 215 3058 236 28 Male 22.2533 85 88 216 4342 263 1 Male 44.063 79 91 217 4865 240 35 Male 58.3354 93 105 218 5085 269 2 Male 49.0267 65 77 219 5222 247 30 Male 22.5298 88 85 220 5339 271 7 Male 21.8152 94 89 221 5474 280 2 Female 28.6598 99 91 222 5600 232 0 Male 48.7885 75 81 223 2826 290 14 Male 23.2334 94 108 224 4725 286 10 Male 32.9172 105 94 225 5204 299 0 Male 59.0746 99 105 226 6498 270 28 Male 24.0767 82 101 227 2081 185 43 Male 17.6975 77 97 228 4678 340 7 Male 46.6448 108 119 229 5397 328 0 Female 62.7981 121 108 230 6214 318 0 Male 60.3176 78 82 231 7034 280 60 Male 23.1376 78 80 232 1493 453 60 Male 17.8042 59 81 233 1836 375 1 Male 47.0554 101 108 234 1939 295 130 Male 28.2738 67 117 235 2646 438 14 Male 22.9158 98 94 236 2653 352 28 Male 30.0068 105 126 237 3226 444 0 Male 27.4552 76 64 238 3467 333 42 Male 25.3936 68 74 239 4342 432 1 Male 44.063 92 107 240 4542 431 11 Female 21.9576 98 114 241 4661 374 17 Female 30.8419 93 95 242 4902 397 8 Male 16.1424 92 86 243 4983 398 5 Male 38.3929 121 132 244 5111 442 7 Male 21.6947 77 86 245 5125 510 12 Male 17.5387 112 125 246 5289 417 1 Male 48.5722 83 83 247 5386 436 21 Male 20.8761 90 103 248 5387 480 12 Male 21.7988 94 116 249 5505 527 1 Male 65.4784 104 87 250 5580 369 7 Male 17.7933 96 107 251 5581 378 2 Male 26.3053 95 95 252 5599 443 21 Male 18.7488 78 80 253 5668 390 7 Male 40.9227 92 92 254 5680 403 1 Male 27.7563 94 93 255 5712 365 14 Male 22.2697 98 86 256 5772 412 35 Male 26.2587 102 104 257 5804 354 2 Female nan 122 105 258 5811 431 25 Male 80.0328 78 80 259 5841 415 8 Male 27.2279 82 83 260 6226 438 0 Male 36.8022 84 92 261 6247 389 13 Male 42.3162 82 80 262 6468 513 60 Male 43.4798 99 94 263 6614 362 0 Male 45.1116 88 106 264 6665 368 3 Female 23.0171 100 92 265 781 714 15 Male 29.8699 85 85 266 1048 576 94 Male 20.115 91 96 267 1157 810 23 Male 17.3881 97 84 268 1493 684 60 Male 17.8042 66 75 269 1611 511 60 Male 23.2799 69 107 270 1624 604 1 Male 19.5619 97 85 271 1939 562 130 Male 28.2738 85 111 272 2498 615 0 Female 17.4292 86 113 273 2826 636 14 Male 23.2334 111 101 274 2849 642 0 Male 20.0876 76 98 275 3032 525 20 Male 16.9391 79 87 276 3226 683 0 Male 27.4552 89 78 277 4218 814 28 Male 25.9904 99 96 278 4807 532 14 Female 47.7974 84 82 279 5014 637 7 Female 23.7618 101 114 280 5222 690 30 Male 22.5298 81 90 281 5253 591 1 Male 33.1335 114 124 282 5628 609 3 Female 30.2642 89 78 283 6059 794 1 Female 16.9801 71 76 284 6228 662 3 Female 31.5537 128 111 285 6247 616 13 Male 42.3162 85 82 286 405 986 0 Male 21.4702 66 116 287 626 870 55 Male 19.7536 80 85 288 1075 907 42 Female 27.2772 63 64 289 2849 1040 0 Male 20.0876 91 103 290 3032 884 20 Male 16.9391 87 93 291 3226 1123 0 Male 27.4552 88 81 292 4864 936 0 Female 53.9767 119 131 293 5474 1100 2 Female 28.6598 94 88 294 5568 1114 1 Female 51.9918 81 82 295 5580 1087 7 Male 17.7933 106 98 296 5581 1113 2 Male 26.3053 99 96 297 5617 1113 17 Male 19.7864 78 87 298 5642 1143 0 Male 65.87 104 109 299 5713 1016 8 Male 16.2683 126 106 300 5837 962 1 Female 33.3087 109 110 301 6140 1077 44 Female 21.4209 65 88 302 7061 923 0 Male 36.8816 74 81 303 651 1491 21 Male 22.0068 71 94 304 2527 1294 0 Male 16.9172 93 104 305 2638 1093 255 Male 16.5613 78 84 306 4865 1363 35 Male 58.3354 88 104 307 5009 1537 7 Male 24.3806 76 112 308 5014 1523 7 Female 23.7618 105 114 309 5085 1512 2 Male 49.0267 75 75 310 1939 1926 130 Male 28.2738 95 108 311 2662 1569 180 Male 28.0821 90 101 312 2826 1809 14 Male 23.2334 104 108 313 2882 1716 18 Male 19.2334 100 103 314 3768 1916 0 Male 19.1102 69 80 315 4356 2000 7 Male 21.399 104 91 316 4638 1779 17 Male 20.512 92 76 317 4696 1769 4 Male 46.9569 105 124 318 4744 1743 25 Male 57.566 97 118 319 6140 1742 44 Female 21.4209 67 87 320 1075 2259 42 Female 27.2772 78 79 321 1939 3111 130 Male 28.2738 88 111 322 2653 2191 28 Male 30.0068 117 129 323 3592 2569 10 Male 61.6646 76 93 324 3808 2434 7 Male 28.2683 105 111 325 651 3412 21 Male 22.0068 68 92 326 1939 3864 130 Male 28.2738 88 105 327 2600 3337 9 Male 43.9398 101 84 328 3835 4933 14 Male 25.9932 91 88 329 2773 7631 42 Male 6.51335 88 103 330 5142 11628 57 Male 16.4326 101 95 331 5964 11038 0 Male 12.8363 71 73 Selanjutnya kita mengimplementasikan rumus jarak ke dalam bentuk fungsi python. yaitu: eulidianDistance() dengan fungsi jarak tipe binary distanceSimetris (). def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t )) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) Kemudian dari dataset tersebut, kita lakukan pengecekan dengan mencari baris yang missing values,. c_j = 0 for j in df [ 'age' ] . isna (): if j == True : col_missing = c_j c_j += 1 Pada langkah berikut, kita lakukan perhitungan jarak pada data yang missing dengan seluruh tetangganya dan menampungnya pada dapat dictionary yang ada. missing_data = df . iloc [ col_missing , [ 2 , 3 , 6 , 7 ]] . values missing_normal = [ normalisasi ( missing_data [ 0 ], data . columns [ 2 ]), normalisasi ( missing_data [ 1 ], data . columns [ 3 ]), normalisasi ( missing_data [ 2 ], data . columns [ 6 ]), normalisasi ( missing_data [ 3 ], data . columns [ 7 ])] for i in range ( len ( data [ data . columns [ 0 ]])): if i == col_missing : continue ; select_data = df . iloc [ i , [ 2 , 3 , 6 , 7 ]] . values normal_data = [ normalisasi ( select_data [ 0 ], data . columns [ 2 ]), normalisasi ( select_data [ 1 ], data . columns [ 3 ]), normalisasi ( select_data [ 2 ], data . columns [ 6 ]), normalisasi ( select_data [ 3 ], data . columns [ 7 ])] data . loc [ i , 'jarak' ] = euclidianDistance ( missing_normal , normal_data ) + distanceSimetris ([ X [ col_missing , 4 ]],[ X [ i , 4 ]]) Kemudian kita urutkan data tersebut berdasarkan jarak dari yang terkecil sampai ke terbesar. Selanjutnya kita mengisi data yang hilang dengan mengambil rata-rata dari 2 tetangga terdekat. df = pd . DataFrame ( data ) df . sort_values ( by = 'jarak' , axis = 0 , ascending = True , inplace = True ) df . iloc [ - 1 , [ 5 ]] = round ( df . iloc [ 0 : 2 , 5 ] . mean (), 2 ) df . style . hide_index () Berikut merupakan tampilan dari data yang telah di urutkan. pada baris terakhir telihat bahwa kolom age sudah terisi dengan angka sebagai berikut no income days delay gender age hiv emergency jarak 229 5397 328 0 Female 62.7981 121 108 0.25 13 4705 18 1 Female 21.6838 127 109 0.53 205 6228 174 3 Female 31.5537 114 108 0.6 284 6228 662 3 Female 31.5537 128 111 0.64 161 5699 138 1 Female 34.2231 110 107 0.84 153 4705 146 1 Female 21.6838 133 111 0.86 196 5208 193 8 Female 21.3771 133 111 0.88 300 5837 962 1 Female 33.3087 109 110 1.07 139 6253 128 0 Female 46.4038 104 112 1.31 141 6665 119 3 Female 23.0171 106 94 1.33 163 5804 159 2 Female 28.8515 102 107 1.35 20 5162 33 1 Male 25.0185 118 101 1.48 279 5014 637 7 Female 23.7618 101 114 1.56 129 5298 107 3 Male 22.9569 117 112 1.64 9 4253 40 3 Male 22.6037 115 110 1.66 308 5014 1523 7 Female 23.7618 105 114 1.67 299 5713 1016 8 Male 16.2683 126 106 1.68 48 2761 40 3 Female 24.3696 98 112 1.69 157 5014 151 7 Female 23.7618 97 110 1.72 264 6665 368 3 Female 23.0171 100 92 1.73 34 6163 21 1 Male 19.3593 112 106 1.73 240 4542 431 11 Female 21.9576 98 114 1.75 221 5474 280 2 Female 28.6598 99 91 1.82 28 5699 26 1 Female 34.2231 95 108 1.83 124 4941 131 4 Female 19.0144 96 96 1.85 211 5837 242 1 Female 33.3087 93 105 1.92 292 4864 936 0 Female 53.9767 119 131 1.93 273 2826 636 14 Male 23.2334 111 101 1.94 132 5414 105 10 Female 40.2765 93 104 1.96 45 7548 31 0 Male 24.3669 108 106 1.98 22 5208 31 8 Female 21.3771 97 90 2 135 5901 115 7 Male 22.1739 112 116 2.07 173 4253 175 3 Male 22.6037 114 118 2.08 158 5162 144 1 Male 25.0185 130 118 2.09 152 4696 150 4 Male 46.9569 120 120 2.1 241 4661 374 17 Female 30.8419 93 95 2.13 55 3655 57 5 Female 21.9055 90 103 2.14 202 6122 212 1 Male 56.2108 109 117 2.22 90 6686 44 14 Female 38.3491 90 100 2.22 23 5253 29 1 Male 33.1335 104 105 2.23 79 5474 65 2 Female 28.6598 95 86 2.26 188 6671 184 7 Female 27.8056 91 92 2.26 293 5474 1100 2 Female 28.6598 94 88 2.3 67 4755 24 18 Male 27.5127 105 102 2.33 175 4755 128 18 Male 27.5127 105 109 2.33 177 4892 148 21 Male 22.0397 106 110 2.35 295 5580 1087 7 Male 17.7933 106 98 2.35 50 3277 51 1 Male 37.4702 104 96 2.38 21 5174 38 4 Female 37.2704 87 99 2.38 228 4678 340 7 Male 46.6448 108 119 2.38 298 5642 1143 0 Male 65.87 104 109 2.41 224 4725 286 10 Male 32.9172 105 94 2.41 233 1836 375 1 Male 47.0554 101 108 2.41 168 1085 159 11 Male 30.7105 103 97 2.43 63 4482 58 14 Female 18.2341 86 103 2.44 272 2498 615 0 Female 17.4292 86 113 2.46 140 6433 120 4 Male 23.8604 100 103 2.48 281 5253 591 1 Male 33.1335 114 124 2.48 121 4542 121 11 Female 21.9576 86 114 2.5 66 4696 54 4 Male 46.9569 101 112 2.5 180 5125 173 12 Male 17.5387 106 119 2.51 225 5204 299 0 Male 59.0746 99 105 2.52 40 6937 18 0 Female 21.191 94 81 2.53 162 5713 144 8 Male 16.2683 100 99 2.55 159 5238 150 3 Male 45.1006 117 126 2.55 74 5238 44 3 Male 45.1006 99 103 2.55 43 7309 31 0 Female 50.6667 85 95 2.57 209 4744 217 25 Male 57.566 108 118 2.59 204 6175 278 1 Male 51.1704 99 98 2.6 17 4983 33 5 Male 38.3929 102 117 2.61 151 4678 143 7 Male 46.6448 98 107 2.62 245 5125 510 12 Male 17.5387 112 125 2.63 156 4983 146 5 Male 38.3929 107 123 2.64 94 7271 55 0 Male 41.7659 100 95 2.65 154 4802 142 0 Male 62.475 101 117 2.65 111 5837 82 1 Female 33.3087 82 110 2.69 73 5204 71 0 Male 59.0746 97 107 2.69 57 3919 58 1 Male 30.3655 99 95 2.7 150 4661 135 17 Female 30.8419 84 93 2.72 52 3359 59 9 Female 56.8953 84 91 2.72 53 3373 39 28 Female 26.308 87 91 2.73 250 5580 369 7 Male 17.7933 96 107 2.74 147 3728 151 6 Male 30.1273 96 105 2.74 249 5505 527 1 Male 65.4784 104 87 2.75 183 5581 176 2 Male 26.3053 96 110 2.76 170 3358 175 4 Male 20.6708 97 97 2.76 101 4744 65 25 Male 57.566 105 119 2.77 296 5581 1113 2 Male 26.3053 99 96 2.78 60 4183 42 3 Male 26.2341 98 116 2.8 312 2826 1809 14 Male 23.2334 104 108 2.82 190 2646 187 14 Male 22.9158 97 97 2.82 33 6122 29 1 Male 56.2108 95 103 2.82 235 2646 438 14 Male 22.9158 98 94 2.83 256 5772 412 35 Male 26.2587 102 104 2.84 41 6977 30 1 Male 36.2108 97 94 2.86 201 5916 205 0 Female 26.8556 92 76 2.87 65 4678 63 7 Male 46.6448 96 95 2.89 282 5628 609 3 Female 30.2642 89 78 2.91 192 4189 202 4 Female 29.462 81 90 2.91 223 2826 290 14 Male 23.2334 94 108 2.92 171 3808 165 7 Male 28.2683 94 111 2.92 251 5581 378 2 Male 26.3053 95 95 2.93 243 4983 398 5 Male 38.3929 121 132 2.93 181 5192 179 1 Male 58.6283 93 105 2.93 137 6173 125 4 Male 35.3046 94 97 2.95 107 5253 86 1 Male 33.1335 106 128 2.97 277 4218 814 28 Male 25.9904 99 96 2.97 114 5916 84 0 Female 26.8556 93 73 2.98 3 3547 40 1 Male 55.9151 95 116 2.98 313 2882 1716 18 Male 19.2334 100 103 2.99 182 5505 171 1 Male 65.4784 95 93 2.99 239 4342 432 1 Male 44.063 92 107 3 278 4807 532 14 Female 47.7974 84 82 3.03 25 5640 34 7 Male 25.9986 93 113 3.03 100 4725 124 10 Male 32.9172 93 97 3.04 254 5680 403 1 Male 27.7563 94 93 3.04 248 5387 480 12 Male 21.7988 94 116 3.05 62 4315 63 0 Male 38.141 107 130 3.05 92 7080 64 5 Female 76.6598 76 106 3.05 83 5628 51 3 Female 30.2642 81 85 3.07 304 2527 1294 0 Male 16.9172 93 104 3.09 8 3808 31 7 Male 28.2683 91 110 3.11 105 5125 78 12 Male 17.5387 94 118 3.12 236 2653 352 28 Male 30.0068 105 126 3.13 315 4356 2000 7 Male 21.399 104 91 3.13 289 2849 1040 0 Male 20.0876 91 103 3.14 255 5712 365 14 Male 22.2697 98 86 3.14 317 4696 1769 4 Male 46.9569 105 124 3.16 220 5339 271 7 Male 21.8152 94 89 3.18 160 5642 162 0 Male 65.87 89 103 3.19 324 3808 2434 7 Male 28.2683 105 111 3.2 172 4094 177 2 Male 19.7262 89 102 3.2 270 1624 604 1 Male 19.5619 97 85 3.2 253 5668 390 7 Male 40.9227 92 92 3.2 127 5154 120 5 Male 22.1903 89 109 3.21 214 2882 262 18 Male 19.2334 94 90 3.22 119 2653 97 28 Male 30.0068 93 112 3.23 191 2790 211 0 Male 48.8049 89 99 3.23 294 5568 1114 1 Female 51.9918 81 82 3.23 51 3346 44 18 Female 57.2758 79 85 3.25 263 6614 362 0 Male 45.1116 88 106 3.25 207 1176 216 17 Female 19.729 74 100 3.25 247 5386 436 21 Male 20.8761 90 103 3.25 29 5713 36 8 Male 16.2683 89 97 3.28 203 6136 216 1 Male 32.7912 92 89 3.29 217 4865 240 35 Male 58.3354 93 105 3.3 71 5014 46 7 Female 23.7618 75 90 3.3 35 6179 22 2 Male 38.0123 89 95 3.31 213 1892 276 2 Male 21.7796 87 107 3.33 91 6795 55 0 Male 30.7159 87 104 3.34 15 4802 36 0 Male 62.475 88 97 3.34 72 5192 60 1 Male 58.6283 87 97 3.4 61 4189 69 4 Female 29.462 75 86 3.4 267 1157 810 23 Male 17.3881 97 84 3.4 197 5456 193 14 Male 41.1636 87 110 3.4 165 6314 140 3 Male 16.6927 87 96 3.41 242 4902 397 8 Male 16.1424 92 86 3.42 155 4807 139 14 Female 47.7974 80 78 3.42 318 4744 1743 25 Male 57.566 97 118 3.43 93 7084 54 2 Male 36.5722 87 93 3.49 81 5580 56 7 Male 17.7933 86 95 3.5 131 5387 109 12 Male 21.7988 85 112 3.54 115 6410 80 14 Male 32.1725 85 98 3.55 82 5581 65 2 Male 26.3053 85 95 3.56 322 2653 2191 28 Male 30.0068 117 129 3.57 12 4542 22 11 Female 21.9576 71 89 3.58 1 3358 30 4 Male 20.6708 87 89 3.6 30 5736 18 9 Male 16.1478 89 86 3.6 290 3032 884 20 Male 16.9391 87 93 3.61 56 3762 48 6 Male 20.3559 85 93 3.61 80 5568 64 1 Female 51.9918 75 79 3.63 38 6870 22 0 Male 42.4832 84 95 3.63 84 6154 43 5 Female 22.6064 74 80 3.65 78 5458 44 14 Male 34.4778 84 95 3.66 46 2364 41 14 Male 25.8097 84 94 3.68 260 6226 438 0 Male 36.8022 84 92 3.68 39 6914 43 0 Male 61.5222 85 90 3.69 24 5298 30 3 Male 22.9569 87 86 3.7 31 5754 36 1 Male 16.3368 87 86 3.7 16 4941 46 4 Female 19.0144 69 88 3.72 185 5680 184 1 Male 27.7563 84 90 3.73 306 4865 1363 35 Male 58.3354 88 104 3.73 27 5680 17 1 Male 27.7563 84 90 3.74 2 3535 16 17 Male 55.2882 95 77 3.75 133 5494 111 7 Male 54.6913 86 86 3.75 36 6671 30 7 Female 27.8056 71 82 3.76 19 5154 35 5 Male 22.1903 82 95 3.76 199 5712 192 14 Male 22.2697 87 85 3.76 262 6468 513 60 Male 43.4798 99 94 3.81 149 4133 133 14 Male 20 82 94 3.81 89 6614 57 0 Male 45.1116 80 101 3.81 54 3544 32 14 Male 54.5298 81 98 3.81 176 4865 142 35 Male 58.3354 84 103 3.82 86 6314 58 3 Male 16.6927 80 99 3.82 77 5456 48 14 Male 41.1636 80 101 3.84 130 5339 119 7 Male 21.8152 87 82 3.85 187 6180 177 12 Male 20.7201 81 94 3.85 226 6498 270 28 Male 24.0767 82 101 3.85 143 1176 146 17 Female 19.729 65 98 3.85 42 7120 39 0 Male 69.7057 84 86 3.86 219 5222 247 30 Male 22.5298 88 85 3.87 10 4356 31 7 Male 21.399 86 83 3.87 68 4837 42 10 Male 19.6906 83 88 3.88 112 5879 75 21 Male 25.8453 80 105 3.89 265 781 714 15 Male 29.8699 85 85 3.89 76 5289 52 1 Male 48.5722 84 85 3.9 102 4807 64 14 Female 47.7974 74 74 3.9 291 3226 1123 0 Male 27.4552 88 81 3.9 215 3058 236 28 Male 22.2533 85 88 3.91 276 3226 683 0 Male 27.4552 89 78 3.92 145 2882 141 18 Male 19.2334 84 85 3.95 44 7321 23 0 Male 26.0041 84 83 3.97 283 6059 794 1 Female 16.9801 71 76 3.97 174 4638 140 17 Male 20.512 89 78 3.97 210 5386 241 21 Male 20.8761 80 94 3.98 285 6247 616 13 Male 42.3162 85 82 3.98 99 3844 73 9 Male 26.1164 79 94 3.98 178 5009 174 7 Male 24.3806 77 103 3.99 58 4094 50 2 Male 19.7262 79 93 3.99 246 5289 417 1 Male 48.5722 83 83 4.02 216 4342 263 1 Male 44.063 79 91 4.02 95 7371 55 1 Male 56.7858 80 88 4.04 122 4902 102 8 Male 16.1424 87 77 4.07 259 5841 415 8 Male 27.2279 82 83 4.08 274 2849 642 0 Male 20.0876 76 98 4.09 280 5222 690 30 Male 22.5298 81 90 4.11 320 1075 2259 42 Female 27.2772 78 79 4.13 108 5386 78 21 Male 20.8761 78 93 4.13 14 4744 15 25 Male 57.566 82 85 4.15 75 5280 83 1 Male 48.6434 78 88 4.16 194 4933 226 0 Male 18.4559 79 86 4.16 316 4638 1779 17 Male 20.512 92 76 4.18 195 4962 210 1 Female 25.1964 71 70 4.19 18 5129 26 1 Male 25.0459 77 89 4.2 275 3032 525 20 Male 16.9391 79 87 4.2 261 6247 389 13 Male 42.3162 82 80 4.21 189 2124 173 30 Male 30.7625 76 106 4.23 198 5668 219 7 Male 40.9227 76 90 4.23 11 4384 35 8 Male 36.3806 76 90 4.24 307 5009 1537 7 Male 24.3806 76 112 4.26 244 5111 442 7 Male 21.6947 77 86 4.28 169 3237 189 9 Male 49.8508 79 82 4.3 297 5617 1113 17 Male 19.7864 78 87 4.3 301 6140 1077 44 Female 21.4209 65 88 4.32 327 2600 3337 9 Male 43.9398 101 84 4.33 128 5222 93 30 Male 22.5298 77 91 4.33 230 6214 318 0 Male 60.3176 78 82 4.34 103 4892 62 21 Male 22.0397 76 88 4.36 319 6140 1742 44 Female 21.4209 67 87 4.36 227 2081 185 43 Male 17.6975 77 97 4.42 206 7173 210 4 Male 24.9801 79 78 4.44 104 4962 63 1 Female 25.1964 69 67 4.44 120 4218 82 28 Male 25.9904 74 92 4.47 252 5599 443 21 Male 18.7488 78 80 4.48 106 5222 63 30 Male 22.5298 77 85 4.48 164 5818 125 14 Male 34.9268 72 91 4.49 212 6247 228 13 Male 42.3162 77 80 4.49 258 5811 431 25 Male 80.0328 78 80 4.52 222 5600 232 0 Male 48.7885 75 81 4.55 109 5534 87 14 Male 29.2621 75 82 4.55 186 5782 108 68 Female 19.6715 69 85 4.56 69 4996 51 12 Male 43.0281 77 78 4.58 64 4638 20 17 Male 20.512 82 72 4.59 32 5776 26 8 Male 17.128 71 88 4.6 302 7061 923 0 Male 36.8816 74 81 4.64 303 651 1491 21 Male 22.0068 71 94 4.67 59 4133 34 14 Male 20 70 88 4.69 117 7221 98 0 Male 63.5044 74 79 4.69 37 6859 27 1 Male 34.2122 74 79 4.69 323 3592 2569 10 Male 61.6646 76 93 4.72 179 5111 177 7 Male 21.6947 72 81 4.73 287 626 870 55 Male 19.7536 80 85 4.75 7 3807 37 5 Male 24.6762 74 77 4.76 136 6135 96 18 Male 26.5626 66 105 4.76 184 5599 148 21 Male 18.7488 72 81 4.8 26 5668 27 7 Male 40.9227 72 79 4.81 286 405 986 0 Male 21.4702 66 116 4.82 126 5111 107 7 Male 21.6947 71 80 4.82 193 4775 180 28 Male 53.5551 70 86 4.83 123 4933 134 0 Male 18.4559 69 83 4.85 87 6340 71 0 Male 19.3238 76 72 4.85 98 3645 43 45 Male 27.4935 72 90 4.86 309 5085 1512 2 Male 49.0267 75 75 4.91 118 2453 120 10 Male 37.2758 63 99 4.94 116 7173 84 4 Male 24.9801 72 75 4.94 142 6834 123 0 Male 30.7488 72 75 4.94 85 6180 59 12 Male 20.7201 67 84 4.96 148 3913 96 42 Female 23.9233 56 80 4.96 47 2600 3333 9 Male 43.9398 86 80 4.97 6 3790 13 3 Male 57.0623 76 69 4.99 113 5893 71 21 Male 22.8118 65 90 4.99 200 5893 200 21 Male 22.8118 65 89 5.01 70 5009 50 7 Male 24.3806 61 104 5.05 146 3051 131 13 Male 37.2403 68 79 5.05 231 7034 280 60 Male 23.1376 78 80 5.07 288 1075 907 42 Female 27.2772 63 64 5.12 266 1048 576 94 Male 20.115 91 96 5.14 269 1611 511 60 Male 23.2799 69 107 5.16 314 3768 1916 0 Male 19.1102 69 80 5.16 237 3226 444 0 Male 27.4552 76 64 5.21 134 5896 126 4 Female 26.8775 50 74 5.25 88 6564 69 0 Male 34.4997 67 74 5.26 218 5085 269 2 Male 49.0267 65 77 5.26 5 3728 19 6 Male 30.1273 67 73 5.3 166 6664 164 2 Male 24.7337 66 73 5.34 110 5712 88 14 Male 22.2697 70 68 5.36 138 6214 112 0 Male 60.3176 65 74 5.37 125 5085 117 2 Male 49.0267 67 71 5.38 97 3058 56 28 Male 22.2533 65 75 5.45 238 3467 333 42 Male 25.3936 68 74 5.46 49 3237 65 9 Male 49.8508 67 67 5.55 325 651 3412 21 Male 22.0068 68 92 5.62 328 3835 4933 14 Male 25.9932 91 88 5.69 4 3592 13 10 Male 61.6646 59 73 5.77 268 1493 684 60 Male 17.8042 66 75 5.83 144 2849 151 0 Male 20.0876 51 86 5.89 96 2569 49 35 Male 18.7159 50 101 5.94 232 1493 453 60 Male 17.8042 59 81 6.03 310 1939 1926 130 Male 28.2738 95 108 6.41 208 3467 186 42 Male 25.3936 53 69 6.45 167 1048 85 94 Male 20.115 63 82 6.51 271 1939 562 130 Male 28.2738 85 111 6.51 321 1939 3111 130 Male 28.2738 88 111 6.93 234 1939 295 130 Male 28.2738 67 117 7.17 326 1939 3864 130 Male 28.2738 88 105 7.22 329 2773 7631 42 Male 6.51335 88 103 7.96 311 2662 1569 180 Male 28.0821 90 101 8.24 331 5964 11038 0 Male 12.8363 71 73 11.23 330 5142 11628 57 Male 16.4326 101 95 11.25 305 2638 1093 255 Male 16.5613 78 84 11.27 257 5804 354 2 Female 42.24 122 105 nan","title":"Contoh Datanya sebagai berikut"},{"location":"Regresi Linear Berganda/","text":"Regresi Linear Berganda \u00b6 Pengertian \u00b6 Regresi Berganda adalah model regresi atau prediksi yang melibatkan lebih dari satu variabel bebas. Istilah regresi berganda dapat disebut sebagai istilah multiple regression. Kata multiple berarti jamak atau lebih dari satu variabel Analisis regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Kegunaan Analisis Regresi Linear Berganda \u00b6 Analisis Regresi Linear Berganda digunakan untuk mengukur pengaruh antara lebih dari satu variabel prediktor (variabel bebas) terhadap variabel terikat. Rumus $$ Y = a + b_1X_1+b_2X_2+....+b_nX_n $$ Keterangan : Y = variabel terikat a = konstanta $b_1b_2$ = koefisien regresi $X_1X_2$ = variabel bebas Untuk menghitung Rumus Regresi Linear Berganda $b_1,b_2, $dan a \u00b6 $$ b_1 = \\frac{(\\sum x_2^2).(\\sum x_1y)- (\\sum x_1x_2).(\\sum x_2y)}{(\\sum x_1^2).(\\sum x_2^2)- (\\sum x_1x_2)^2} $$ $$ b_2 = \\frac{(\\sum x_1^2).(\\sum x_1y)- (\\sum x_1x_2).(\\sum x_1y)}{(\\sum x_1^2).(\\sum x_2^2)- (\\sum x_1x_2)^2} $$ $$ a = \\frac {\\sum Y}{n}- b_1 . \\frac{(\\sum X_1)}{n} -b_2.\\frac{(\\sum X_2)}{n} $$ Contoh Kasus Untuk menganalisis Regresi Linear berganda dengan 2 Fitur \u00b6 Dalam Kasus berikut ,menganalisa regresi linear berganda dengan 2 fitur sebagai berikut menggunakan Excel dengan cara Manual X1 X2 Y X1^2 X2^2 X1 x X2 X1 x Y X2 x Y 3 3 8 9 9 9 24 24 2 4 8 4 16 8 16 32 3 1 7 9 1 3 21 7 4 5 14 16 25 20 56 70 Total$(\\sum)$ 12 13 37 38 51 40 117 133 Untuk mengurangi Sistem eror bisa Melakukan Normalisasi terlebih dahulu Pada Total $\\sum X_1^2/4$ dan $\\sum X_2^2/4$ Implementasi Kasus Regresi Linear Berganda 2 Fitur ke Code Python \u00b6 Pertama dengan meng-import numpy dan from sklearn.linear_model import LinearRegression. Setelah itu bisa memasukkan data terlebih dahulu pada fitur X dan Y nya import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 3 ],[ 2 , 4 ],[ 3 , 1 ],[ 4 , 5 ]]) y = np . array ([ 8 , 8 , 7 , 14 ]) X array ([[ 3 , 3 ], [ 2 , 4 ], [ 3 , 1 ], [ 4 , 5 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 0.9600886917960089 a = reg . intercept_ a - 1.8181818181818183 b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 6 x2 = 2 reg . coef_ array ([ 2.40909091 , 1.18181818 ]) y = ( b1 * x1 ) + ( b2 * x2 ) + a y 15.000000000000002 Sudah didapatkan Y yang dicari menggunakan program dari python Sistem yang digunakan tersebut sudah termasuk normalisasi dan sudah paling sedikit sistem erornya Pada Contoh ke 2 menggunakan 2 fitur juga, berikut import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 2 , 3 ],[ 4 , 2 ],[ 5 , 3 ],[ 7 , 1 ]]) y = np . array ([ 10 , 12 , 16 , 16 ]) X array ([[ 2 , 3 ], [ 4 , 2 ], [ 5 , 3 ], [ 7 , 1 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a 7.105427357601002e-15 b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 6 x2 = 2 reg . coef_ array ([ 2. , 2. ]) y = ( b1 * x1 ) + ( b2 * x2 ) + a y 16.0 Implementasi Kasus Regresi Linear Berganda 3 Fitur ke Code Python \u00b6 Menggunakan import numpy as np dan from sklearn.linear_model import LinearRegression import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 3 , 2 ],[ 2 , 4 , 1 ],[ 3 , 1 , 2 ],[ 4 , 5 , 3 ]]) y = np . array ([ 16 , 14 , 12 , 24 ]) X array ([[ 3 , 3 , 2 ], [ 2 , 4 , 1 ], [ 3 , 1 , 2 ], [ 4 , 5 , 3 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a 3.552713678800501e-15 b3 = reg . coef_ [ 2 ] b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 7 x2 = 5 x3 = 4 reg . coef_ array ([ 3. , 3. , 3. ]) y = ( b1 * x1 ) + ( b2 * x2 ) + ( b3 * x3 ) + a y 48.0 Y yang didapatkan dari program di atas adalah 48 Dengan 3 Fitur Contoh 3 Fitur yang lain import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 2 , 1 ],[ 2 , 3 , 2 ],[ 3 , 3 , 2 ],[ 4 , 6 , 3 ]]) y = np . array ([ 12 , 15 , 19 , 22 ]) X array ([[ 3 , 2 , 1 ], [ 2 , 3 , 2 ], [ 3 , 3 , 2 ], [ 4 , 6 , 3 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a - 3.000000000000007 b3 = reg . coef_ [ 2 ] b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 7 x2 = 5 x3 = 4 reg . coef_ array ([ 4. , - 4. , 11. ]) y = ( b1 * x1 ) + ( b2 * x2 ) + ( b3 * x3 ) + a y 49.0 Y yang sudah didapat adalah 49 dengan 3 Fitur Implementasi Kasus Regresi Linear Berganda 4 Fitur ke Code Python \u00b6 Menggunakan import numpy as np dan from sklearn.linear_model import LinearRegression import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 2 , 1 , 2 ],[ 2 , 3 , 2 , 1 ],[ 3 , 3 , 2 , 4 ],[ 4 , 6 , 3 , 2 ],[ 3 , 2 , 5 , 1 ]]) y = np . array ([ 14 , 17 , 19 , 24 , 27 ]) X array ([[ 3 , 2 , 1 , 2 ], [ 2 , 3 , 2 , 1 ], [ 3 , 3 , 2 , 4 ], [ 4 , 6 , 3 , 2 ], [ 3 , 2 , 5 , 1 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a 6.6304347826087024 b4 = reg . coef_ [ 3 ] b3 = reg . coef_ [ 2 ] b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 7 x2 = 5 x3 = 4 x4 = 1 reg . coef_ array ([ 0.56521739 , 0.67391304 , 3.36956522 , 0.47826087 ]) y = ( b1 * x1 ) + ( b2 * x2 ) + ( b3 * x3 ) + ( b4 * x4 ) + a y 27.913043478260857 Y yang sudah didapat adalah 27,913 dengan 4 Fitur MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Regresi Linear Berganda"},{"location":"Regresi Linear Berganda/#regresi-linear-berganda","text":"","title":"Regresi Linear Berganda"},{"location":"Regresi Linear Berganda/#pengertian","text":"Regresi Berganda adalah model regresi atau prediksi yang melibatkan lebih dari satu variabel bebas. Istilah regresi berganda dapat disebut sebagai istilah multiple regression. Kata multiple berarti jamak atau lebih dari satu variabel Analisis regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio.","title":"Pengertian"},{"location":"Regresi Linear Berganda/#kegunaan-analisis-regresi-linear-berganda","text":"Analisis Regresi Linear Berganda digunakan untuk mengukur pengaruh antara lebih dari satu variabel prediktor (variabel bebas) terhadap variabel terikat. Rumus $$ Y = a + b_1X_1+b_2X_2+....+b_nX_n $$ Keterangan : Y = variabel terikat a = konstanta $b_1b_2$ = koefisien regresi $X_1X_2$ = variabel bebas","title":"Kegunaan Analisis Regresi Linear Berganda"},{"location":"Regresi Linear Berganda/#untuk-menghitung-rumus-regresi-linear-berganda-b_1b_2-dan-a","text":"$$ b_1 = \\frac{(\\sum x_2^2).(\\sum x_1y)- (\\sum x_1x_2).(\\sum x_2y)}{(\\sum x_1^2).(\\sum x_2^2)- (\\sum x_1x_2)^2} $$ $$ b_2 = \\frac{(\\sum x_1^2).(\\sum x_1y)- (\\sum x_1x_2).(\\sum x_1y)}{(\\sum x_1^2).(\\sum x_2^2)- (\\sum x_1x_2)^2} $$ $$ a = \\frac {\\sum Y}{n}- b_1 . \\frac{(\\sum X_1)}{n} -b_2.\\frac{(\\sum X_2)}{n} $$","title":"Untuk menghitung Rumus Regresi Linear Berganda $b_1,b_2, $dan a"},{"location":"Regresi Linear Berganda/#contoh-kasus-untuk-menganalisis-regresi-linear-berganda-dengan-2-fitur","text":"Dalam Kasus berikut ,menganalisa regresi linear berganda dengan 2 fitur sebagai berikut menggunakan Excel dengan cara Manual X1 X2 Y X1^2 X2^2 X1 x X2 X1 x Y X2 x Y 3 3 8 9 9 9 24 24 2 4 8 4 16 8 16 32 3 1 7 9 1 3 21 7 4 5 14 16 25 20 56 70 Total$(\\sum)$ 12 13 37 38 51 40 117 133 Untuk mengurangi Sistem eror bisa Melakukan Normalisasi terlebih dahulu Pada Total $\\sum X_1^2/4$ dan $\\sum X_2^2/4$","title":"Contoh Kasus Untuk menganalisis Regresi Linear berganda dengan 2 Fitur"},{"location":"Regresi Linear Berganda/#implementasi-kasus-regresi-linear-berganda-2-fitur-ke-code-python","text":"Pertama dengan meng-import numpy dan from sklearn.linear_model import LinearRegression. Setelah itu bisa memasukkan data terlebih dahulu pada fitur X dan Y nya import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 3 ],[ 2 , 4 ],[ 3 , 1 ],[ 4 , 5 ]]) y = np . array ([ 8 , 8 , 7 , 14 ]) X array ([[ 3 , 3 ], [ 2 , 4 ], [ 3 , 1 ], [ 4 , 5 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 0.9600886917960089 a = reg . intercept_ a - 1.8181818181818183 b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 6 x2 = 2 reg . coef_ array ([ 2.40909091 , 1.18181818 ]) y = ( b1 * x1 ) + ( b2 * x2 ) + a y 15.000000000000002 Sudah didapatkan Y yang dicari menggunakan program dari python Sistem yang digunakan tersebut sudah termasuk normalisasi dan sudah paling sedikit sistem erornya Pada Contoh ke 2 menggunakan 2 fitur juga, berikut import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 2 , 3 ],[ 4 , 2 ],[ 5 , 3 ],[ 7 , 1 ]]) y = np . array ([ 10 , 12 , 16 , 16 ]) X array ([[ 2 , 3 ], [ 4 , 2 ], [ 5 , 3 ], [ 7 , 1 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a 7.105427357601002e-15 b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 6 x2 = 2 reg . coef_ array ([ 2. , 2. ]) y = ( b1 * x1 ) + ( b2 * x2 ) + a y 16.0","title":"Implementasi Kasus Regresi Linear Berganda 2 Fitur ke Code Python"},{"location":"Regresi Linear Berganda/#implementasi-kasus-regresi-linear-berganda-3-fitur-ke-code-python","text":"Menggunakan import numpy as np dan from sklearn.linear_model import LinearRegression import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 3 , 2 ],[ 2 , 4 , 1 ],[ 3 , 1 , 2 ],[ 4 , 5 , 3 ]]) y = np . array ([ 16 , 14 , 12 , 24 ]) X array ([[ 3 , 3 , 2 ], [ 2 , 4 , 1 ], [ 3 , 1 , 2 ], [ 4 , 5 , 3 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a 3.552713678800501e-15 b3 = reg . coef_ [ 2 ] b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 7 x2 = 5 x3 = 4 reg . coef_ array ([ 3. , 3. , 3. ]) y = ( b1 * x1 ) + ( b2 * x2 ) + ( b3 * x3 ) + a y 48.0 Y yang didapatkan dari program di atas adalah 48 Dengan 3 Fitur Contoh 3 Fitur yang lain import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 2 , 1 ],[ 2 , 3 , 2 ],[ 3 , 3 , 2 ],[ 4 , 6 , 3 ]]) y = np . array ([ 12 , 15 , 19 , 22 ]) X array ([[ 3 , 2 , 1 ], [ 2 , 3 , 2 ], [ 3 , 3 , 2 ], [ 4 , 6 , 3 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a - 3.000000000000007 b3 = reg . coef_ [ 2 ] b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 7 x2 = 5 x3 = 4 reg . coef_ array ([ 4. , - 4. , 11. ]) y = ( b1 * x1 ) + ( b2 * x2 ) + ( b3 * x3 ) + a y 49.0 Y yang sudah didapat adalah 49 dengan 3 Fitur","title":"Implementasi Kasus Regresi Linear Berganda 3 Fitur ke Code Python"},{"location":"Regresi Linear Berganda/#implementasi-kasus-regresi-linear-berganda-4-fitur-ke-code-python","text":"Menggunakan import numpy as np dan from sklearn.linear_model import LinearRegression import numpy as np from sklearn.linear_model import LinearRegression X = np . array ([[ 3 , 2 , 1 , 2 ],[ 2 , 3 , 2 , 1 ],[ 3 , 3 , 2 , 4 ],[ 4 , 6 , 3 , 2 ],[ 3 , 2 , 5 , 1 ]]) y = np . array ([ 14 , 17 , 19 , 24 , 27 ]) X array ([[ 3 , 2 , 1 , 2 ], [ 2 , 3 , 2 , 1 ], [ 3 , 3 , 2 , 4 ], [ 4 , 6 , 3 , 2 ], [ 3 , 2 , 5 , 1 ]]) reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) 1.0 a = reg . intercept_ a 6.6304347826087024 b4 = reg . coef_ [ 3 ] b3 = reg . coef_ [ 2 ] b2 = reg . coef_ [ 1 ] b1 = reg . coef_ [ 0 ] x1 = 7 x2 = 5 x3 = 4 x4 = 1 reg . coef_ array ([ 0.56521739 , 0.67391304 , 3.36956522 , 0.47826087 ]) y = ( b1 * x1 ) + ( b2 * x2 ) + ( b3 * x3 ) + ( b4 * x4 ) + a y 27.913043478260857 Y yang sudah didapat adalah 27,913 dengan 4 Fitur MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$'],['$','$']]} });","title":"Implementasi Kasus Regresi Linear Berganda 4 Fitur ke Code Python"},{"location":"index_komnum/","text":"Selamat Datang Di Halaman Tugas Komputasi Numerik (Computation Numeric) \u00b6 Profile \u00b6 Name : Wahyu Zainur Putra NIM : 180411100128 Kelas : Komputasi Numerik 4-B Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab, S.Si., M.Kom Alamat : Perum Griya Abadi AL 10 Socah Bangkalan Terima Kasih telah mengunjungi Halaman ini \u00b6 Kumpulan Tugas Tugas Komputasi Numerik","title":"Halaman Profile"},{"location":"index_komnum/#selamat-datang-di-halaman-tugas-komputasi-numerik-computation-numeric","text":"","title":"Selamat Datang Di Halaman Tugas Komputasi Numerik (Computation Numeric)"},{"location":"index_komnum/#profile","text":"Name : Wahyu Zainur Putra NIM : 180411100128 Kelas : Komputasi Numerik 4-B Jurusan : Teknik Informatika Angkatan : 2018 Dosen Pengampu : Mula'ab, S.Si., M.Kom Alamat : Perum Griya Abadi AL 10 Socah Bangkalan","title":"Profile"},{"location":"index_komnum/#terima-kasih-telah-mengunjungi-halaman-ini","text":"Kumpulan Tugas Tugas Komputasi Numerik","title":"Terima Kasih telah mengunjungi Halaman ini"},{"location":"statistik deskriptif/","text":"Statistik Deskriptif \u00b6 Pengertian \u00b6 Statistika Deskriptif adalah metode-metode yang berkaitan dengan pengumpulan data dan penyajian suatu gugus data sehingga memberikan informasi yang berguna Statistika Deskriptif juga merupakan metode yang sangat sederhana. Metode ini hanya medeskripsikan kondisi dari data yang sudah anda miliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang disajikan dalam uraian-uraian singkat dan juga terbatas. Dengan Statistika deskriptif, kumpulan data yang diperoleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada sehingga dapat meyakinkan pengguna menggunakan data-data yang tersaji dengan ringkas tersebut. Tipe Statistik Deskriptif \u00b6 Mean(rata-rata) \u00b6 Mean adalah \"nilai rata-rata\" dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data.Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan jenis data normal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka data tersebut dapat kita tuliskan sebagai berikut di bawah ini: $$ \\bar x={\\sum \\limits_{i=1}^{n} x_i\\over N} = {x_1 + x_2 + x_3 + . . . . . + x_n \\over N} $$ Dimana : X bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data Median \u00b6 Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga dikatakan sebagai nilai pertengahan dari sekelompok data yang telah diurutkan menurut besarnya. Jika banyaknya datanya ganjil, maka rumus mediannya : $$ Me=Q_2 =\\left ( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right),jika\\quad n\\quad Ganjil $$ Dan jika banyaknya datanya genap maka mediannya adalah : $$ Me=Q_2 =\\left ( \\begin{matrix} {xn \\over 2}{xn+1\\over2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad Genap $$ Keterangan : Me = Median dari kelompok data n : banyaknya data Modus \u00b6 Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi , jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik digunakan untuk data yang memili skala kategorik yaitu nominal atau ordinal. Modus bisa dihitung menggunakan rumus sebagai berikut : $$ M_o = Tb + p{b_1 \\over b_1 +b_2} $$ Dimana : Mo = banyaknya nilai yang sama / sering muncul Tb = Tepi bawah yang memiliki frekuensi tertenggi (kelas modus) b1= Interval kelas b1 (Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya) b2= Interval kelas b2 (Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya). p = panjang interval Standar Deviasi \u00b6 Standar Deviasi dan Varians adalah salah satu teknik statistik yang digunakan untuk menjelaskan hormogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians (Simpangan baku) merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama. Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Dimana : x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data Varians \u00b6 Varians merupakan rata-rata dari selisih kuadrat tersebut yang merupakan suatu ukuran penyimpangan dari observasi. Simbol varians pada ukuran populasi zigma kuadrat pada ukuran sample S2. Akar dari varians dinamakan standar deviasi atau simpangan baku. Varians bisa didapat menggunakan rumus sebagai berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari anggota data Skewness \u00b6 Skewness (kemencengan) atau bisa disebut sebagai penyimpangan dari kesimetrian dari suatu distribusi adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kana (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n-1) \\sigma^3} $$ Dimana : Xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile \u00b6 Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian yang sama besar. Nilai-nilai itu dilambangkan sebagai Q1,Q2, dan Q3. Lambang tersebut mempunyai sifat bahwa 25% data jatuh dibawah adalah Q1, 50% data jatuh dibawah adalah Q2, dan 75% data jatuh didibawah adalah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut : $$ Q_1 = {1\\over 4}(n +1 ),Quartil\\quad 1\\quad dalam\\quad 0.25 $$ $$ Q_2 = {1\\over 2}(n + 1),Quartil\\quad 2\\quad dalam\\quad 0.50 $$ $$ Q_3 = {3\\over 4} (n + 1),Quartil\\quad 3\\quad dalam\\quad 0.75 $$ Dimana : n = sebagai jumlah datanya Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Alat dan Bahan \u00b6 Pada Penerapan ini kita harus menggunakan data random 500 dengan jangkauan yang telah di tentukan yang disimpan dalam bentuk .csv dan untuk mempermudah penerapan ini kita harus menginstall dan mendownload hal-hal yang diperlukan untuk bisa mempermudah penerapan statistik deskriptif ini menggunakan python. Dalam jendala cmd kita harus menginstall yaitu: 1. Pandas, digunakan untuk data manajemen dan data analysis 2. Scipy, digunakan untuk libary yang berisikan kumpulan algoritma dan fungsi matematika Pada Langkah Pertama \u00b6 Kita harus memasukkan libary yang telah disiapkan sebelumnya import pandas as pd from scipy import stats Langkah Kedua ini \u00b6 Kita memasukkan data csv tersebut dan disiapkan df = pd . read_csv ( 'Book1.csv' , sep = ';' ) Data yang ada di csv berikut ini: X1 X2 X3 X4 0 63 44 123 109 1 52 32 112 75 2 53 30 105 83 3 56 31 125 88 4 66 32 102 76 5 69 22 129 95 6 51 42 111 102 7 80 45 126 109 8 73 43 114 74 9 65 46 106 80 10 55 33 114 77 11 80 37 100 92 12 55 38 110 84 13 60 46 119 86 14 54 36 113 104 15 68 50 118 104 16 55 36 118 84 17 56 20 100 100 18 55 41 113 75 19 56 22 100 93 20 61 34 105 97 21 66 36 101 82 22 70 33 124 92 23 63 35 113 81 24 80 44 100 88 25 67 50 120 86 26 58 28 109 72 27 73 22 128 103 28 70 36 102 74 29 55 40 113 105 ... ... ... ... ... 470 78 20 110 79 471 63 28 108 92 472 64 20 115 86 473 69 37 127 78 474 74 28 126 73 475 79 28 116 100 476 69 48 103 80 477 60 21 120 77 478 73 35 112 75 479 71 41 128 92 480 67 41 110 84 481 63 27 129 72 482 75 46 107 76 483 55 32 113 109 484 50 32 114 81 485 62 32 103 80 486 77 34 109 77 487 58 46 111 97 488 64 46 121 101 489 73 39 114 105 490 51 32 100 75 491 50 32 103 96 492 67 41 129 72 493 57 31 113 106 494 77 38 107 75 495 55 50 117 87 496 58 23 106 95 497 58 36 112 78 498 55 49 102 87 499 50 34 112 106 500 rows \u00d7 4 columns Langkah Ketiga \u00b6 Kita harus membuat Data Penyimpanan (Dictionary) yang digunakan untuk bisa menampung nilai yang akan ditampilkan. Selanjutnya mengambil dari dari kolom-kolom data yang ada di dalam csv dengan cara diiterasi serta dihitung dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. Kemudian hasil tersebut disimpan pada penyimpanan yang telah disiapkan. data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.50 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] Pada Langkah Keempat ini \u00b6 Kita menvisualisasikan hasil tersebut dalam bentuk dataframe tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Berikut Hasil Gabungan dari code yang telah di buat untuk menampilkan program tabel dibawah ini import pandas as pd from scipy import stats df = pd . read_csv ( 'Book1.csv' , sep = ';' ) data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.50 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Hasil Program yang telah divisualiasikan Stats X1 X2 X3 X4 0 Min 50.00 20.000 100.000 70.000 1 Max 80.00 50.000 130.000 110.000 2 Mean 64.89 35.384 114.094 89.784 3 Standar Deviasi 8.97 9.100 8.960 11.620 4 Variasi 80.49 82.860 80.250 135.110 5 Skewnes -0.04 -0.090 0.110 -0.010 6 Quantile 1 57.00 28.000 106.000 80.000 7 Quantile 2 66.00 36.000 113.000 90.000 8 Quantile 3 73.00 43.000 121.000 99.000 9 Median 66.00 36.000 113.000 90.000 10 Modus 73.00 20.000 102.000 97.000 Referensi \u00b6 1. https://id.wikipedia.org/wiki/Statistika_deskriptif 2. https://rumus.co.id/mean-median-modus-data-kelompok/ 3. http://emerer.com/cara-menghitung-median-modus-mode-kuartil-dan-desil/ 4. https://carasiiumi.com/cara-menghitung-standar-deviasi/ 5. http://muhammadsurindra.blogspot.com/2015/11/tugas2-pengantar-statistika-kaliini.html MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik Deskriptif"},{"location":"statistik deskriptif/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"statistik deskriptif/#pengertian","text":"Statistika Deskriptif adalah metode-metode yang berkaitan dengan pengumpulan data dan penyajian suatu gugus data sehingga memberikan informasi yang berguna Statistika Deskriptif juga merupakan metode yang sangat sederhana. Metode ini hanya medeskripsikan kondisi dari data yang sudah anda miliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang disajikan dalam uraian-uraian singkat dan juga terbatas. Dengan Statistika deskriptif, kumpulan data yang diperoleh akan tersaji dengan ringkas dan rapi serta dapat memberikan informasi inti dari kumpulan data yang ada sehingga dapat meyakinkan pengguna menggunakan data-data yang tersaji dengan ringkas tersebut.","title":"Pengertian"},{"location":"statistik deskriptif/#tipe-statistik-deskriptif","text":"","title":"Tipe Statistik Deskriptif"},{"location":"statistik deskriptif/#meanrata-rata","text":"Mean adalah \"nilai rata-rata\" dari beberapa buah data. Nilai mean dapat ditentukan dengan membagi jumlah data dengan banyaknya data. Mean (rata-rata) merupakan suatu ukuran pemusatan data.Mean suatu data juga merupakan statistik karena mampu menggambarkan bahwa data tersebut berada pada kisaran mean data tersebut. Mean tidak dapat digunakan sebagai ukuran pemusatan jenis data normal dan ordinal. Berdasarkan definisi dari mean adalah jumlah seluruh data dibagi dengan banyaknya data. Dengan kata lain jika kita memiliki N data sebagai berikut maka data tersebut dapat kita tuliskan sebagai berikut di bawah ini: $$ \\bar x={\\sum \\limits_{i=1}^{n} x_i\\over N} = {x_1 + x_2 + x_3 + . . . . . + x_n \\over N} $$ Dimana : X bar = x rata-rata = nilai rata-rata sampel x = data ke n n = banyaknya data","title":"Mean(rata-rata)"},{"location":"statistik deskriptif/#median","text":"Median menentukan letak tengah data setelah data disusun menurut urutan nilainya. Bisa juga dikatakan sebagai nilai pertengahan dari sekelompok data yang telah diurutkan menurut besarnya. Jika banyaknya datanya ganjil, maka rumus mediannya : $$ Me=Q_2 =\\left ( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right),jika\\quad n\\quad Ganjil $$ Dan jika banyaknya datanya genap maka mediannya adalah : $$ Me=Q_2 =\\left ( \\begin{matrix} {xn \\over 2}{xn+1\\over2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad Genap $$ Keterangan : Me = Median dari kelompok data n : banyaknya data","title":"Median"},{"location":"statistik deskriptif/#modus","text":"Modus adalah nilai yang sering muncul. Jika kita tertarik pada data frekuensi , jumlah dari suatu nilai dari kumpulan data, maka kita menggunakan modus. Modus sangat baik digunakan untuk data yang memili skala kategorik yaitu nominal atau ordinal. Modus bisa dihitung menggunakan rumus sebagai berikut : $$ M_o = Tb + p{b_1 \\over b_1 +b_2} $$ Dimana : Mo = banyaknya nilai yang sama / sering muncul Tb = Tepi bawah yang memiliki frekuensi tertenggi (kelas modus) b1= Interval kelas b1 (Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sebelumnya) b2= Interval kelas b2 (Frekuensi kelas modus dikurangi frekuensi kelas interval terdekat sesudahnya). p = panjang interval","title":"Modus"},{"location":"statistik deskriptif/#standar-deviasi","text":"Standar Deviasi dan Varians adalah salah satu teknik statistik yang digunakan untuk menjelaskan hormogenitas kelompok. Varians merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar Deviasi dan Varians (Simpangan baku) merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama. Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. Standar Deviasi bisa didapat menggunakan rumus sebagai berikut : $$ \\sigma^ = \\sqrt {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Dimana : x = data ke n x bar = x rata-rata = nilai rata-rata sampel n = banyaknya data","title":"Standar Deviasi"},{"location":"statistik deskriptif/#varians","text":"Varians merupakan rata-rata dari selisih kuadrat tersebut yang merupakan suatu ukuran penyimpangan dari observasi. Simbol varians pada ukuran populasi zigma kuadrat pada ukuran sample S2. Akar dari varians dinamakan standar deviasi atau simpangan baku. Varians bisa didapat menggunakan rumus sebagai berikut : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Dimana : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari anggota data","title":"Varians"},{"location":"statistik deskriptif/#skewness","text":"Skewness (kemencengan) atau bisa disebut sebagai penyimpangan dari kesimetrian dari suatu distribusi adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kana (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n-1) \\sigma^3} $$ Dimana : Xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi","title":"Skewness"},{"location":"statistik deskriptif/#quartile","text":"Quartile adalah nilai-nilai yang membagi segugus pengamatan menjadi empat bagian yang sama besar. Nilai-nilai itu dilambangkan sebagai Q1,Q2, dan Q3. Lambang tersebut mempunyai sifat bahwa 25% data jatuh dibawah adalah Q1, 50% data jatuh dibawah adalah Q2, dan 75% data jatuh didibawah adalah Q3. Quartile bisa dihitung menggunakan rumus sebagai berikut : $$ Q_1 = {1\\over 4}(n +1 ),Quartil\\quad 1\\quad dalam\\quad 0.25 $$ $$ Q_2 = {1\\over 2}(n + 1),Quartil\\quad 2\\quad dalam\\quad 0.50 $$ $$ Q_3 = {3\\over 4} (n + 1),Quartil\\quad 3\\quad dalam\\quad 0.75 $$ Dimana : n = sebagai jumlah datanya","title":"Quartile"},{"location":"statistik deskriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"statistik deskriptif/#alat-dan-bahan","text":"Pada Penerapan ini kita harus menggunakan data random 500 dengan jangkauan yang telah di tentukan yang disimpan dalam bentuk .csv dan untuk mempermudah penerapan ini kita harus menginstall dan mendownload hal-hal yang diperlukan untuk bisa mempermudah penerapan statistik deskriptif ini menggunakan python. Dalam jendala cmd kita harus menginstall yaitu: 1. Pandas, digunakan untuk data manajemen dan data analysis 2. Scipy, digunakan untuk libary yang berisikan kumpulan algoritma dan fungsi matematika","title":"Alat dan Bahan"},{"location":"statistik deskriptif/#pada-langkah-pertama","text":"Kita harus memasukkan libary yang telah disiapkan sebelumnya import pandas as pd from scipy import stats","title":"Pada Langkah Pertama"},{"location":"statistik deskriptif/#langkah-kedua-ini","text":"Kita memasukkan data csv tersebut dan disiapkan df = pd . read_csv ( 'Book1.csv' , sep = ';' ) Data yang ada di csv berikut ini: X1 X2 X3 X4 0 63 44 123 109 1 52 32 112 75 2 53 30 105 83 3 56 31 125 88 4 66 32 102 76 5 69 22 129 95 6 51 42 111 102 7 80 45 126 109 8 73 43 114 74 9 65 46 106 80 10 55 33 114 77 11 80 37 100 92 12 55 38 110 84 13 60 46 119 86 14 54 36 113 104 15 68 50 118 104 16 55 36 118 84 17 56 20 100 100 18 55 41 113 75 19 56 22 100 93 20 61 34 105 97 21 66 36 101 82 22 70 33 124 92 23 63 35 113 81 24 80 44 100 88 25 67 50 120 86 26 58 28 109 72 27 73 22 128 103 28 70 36 102 74 29 55 40 113 105 ... ... ... ... ... 470 78 20 110 79 471 63 28 108 92 472 64 20 115 86 473 69 37 127 78 474 74 28 126 73 475 79 28 116 100 476 69 48 103 80 477 60 21 120 77 478 73 35 112 75 479 71 41 128 92 480 67 41 110 84 481 63 27 129 72 482 75 46 107 76 483 55 32 113 109 484 50 32 114 81 485 62 32 103 80 486 77 34 109 77 487 58 46 111 97 488 64 46 121 101 489 73 39 114 105 490 51 32 100 75 491 50 32 103 96 492 67 41 129 72 493 57 31 113 106 494 77 38 107 75 495 55 50 117 87 496 58 23 106 95 497 58 36 112 78 498 55 49 102 87 499 50 34 112 106 500 rows \u00d7 4 columns","title":"Langkah Kedua ini"},{"location":"statistik deskriptif/#langkah-ketiga","text":"Kita harus membuat Data Penyimpanan (Dictionary) yang digunakan untuk bisa menampung nilai yang akan ditampilkan. Selanjutnya mengambil dari dari kolom-kolom data yang ada di dalam csv dengan cara diiterasi serta dihitung dengan berbagai metode yang telah disiapkan oleh pandas itu sendiri. Kemudian hasil tersebut disimpan pada penyimpanan yang telah disiapkan. data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.50 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]]","title":"Langkah Ketiga"},{"location":"statistik deskriptif/#pada-langkah-keempat-ini","text":"Kita menvisualisasikan hasil tersebut dalam bentuk dataframe tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Berikut Hasil Gabungan dari code yang telah di buat untuk menampilkan program tabel dibawah ini import pandas as pd from scipy import stats df = pd . read_csv ( 'Book1.csv' , sep = ';' ) data = { \"Stats\" : [ 'Min' , 'Max' , 'Mean' , 'Standar Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.50 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data , columns = [ 'Stats' ] + [ x for x in df . columns ]) tes Hasil Program yang telah divisualiasikan Stats X1 X2 X3 X4 0 Min 50.00 20.000 100.000 70.000 1 Max 80.00 50.000 130.000 110.000 2 Mean 64.89 35.384 114.094 89.784 3 Standar Deviasi 8.97 9.100 8.960 11.620 4 Variasi 80.49 82.860 80.250 135.110 5 Skewnes -0.04 -0.090 0.110 -0.010 6 Quantile 1 57.00 28.000 106.000 80.000 7 Quantile 2 66.00 36.000 113.000 90.000 8 Quantile 3 73.00 43.000 121.000 99.000 9 Median 66.00 36.000 113.000 90.000 10 Modus 73.00 20.000 102.000 97.000","title":"Pada Langkah Keempat ini"},{"location":"statistik deskriptif/#referensi","text":"1. https://id.wikipedia.org/wiki/Statistika_deskriptif 2. https://rumus.co.id/mean-median-modus-data-kelompok/ 3. http://emerer.com/cara-menghitung-median-modus-mode-kuartil-dan-desil/ 4. https://carasiiumi.com/cara-menghitung-standar-deviasi/ 5. http://muhammadsurindra.blogspot.com/2015/11/tugas2-pengantar-statistika-kaliini.html MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Referensi"}]}